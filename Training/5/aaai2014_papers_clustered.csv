title,authors,groups,keywords,topics,abstract,Text,Clean_Text,Cluster,TSNE-1,TSNE-2
Kernelized Bayesian Transfer Learning,Mehmet Gönen and Adam A. Margolin,Novel Machine Learning Algorithms (NMLA),"cross-domain learning
domain adaptation
kernel methods
transfer learning
variational approximation","APP: Biomedical / Bioinformatics
NMLA: Bayesian Learning
NMLA: Kernel Methods
NMLA: Transfer, Adaptation, Multitask Learning
VIS: Object Recognition","Transfer learning considers related but distinct tasks defined on heterogenous domains and tries to transfer knowledge between these tasks to improve generalization performance. It is particularly useful when we do not have sufficient amount of labeled training data in some tasks, which may be very costly, laborious, or even infeasible to obtain. Instead, learning the tasks jointly enables us to effectively increase the amount of labeled training data. In this paper, we formulate a kernelized Bayesian transfer learning framework that is a principled combination of kernel-based dimensionality reduction models with task-specific projection matrices to find a shared subspace and a coupled classification model for all of the tasks in this subspace. Our two main contributions are: (i) two novel probabilistic models for binary and multiclass classification, and (ii) very efficient variational approximation procedures for these models. We illustrate the generalization performance of our algorithms on two different applications. In computer vision experiments, our method outperforms the state-of-the-art algorithms on nine out of 12 benchmark supervised domain adaptation experiments defined on two object recognition data sets. In cancer biology experiments, we use our algorithm to predict mutation status of important cancer genes from gene expression profiles using two distinct cancer populations, namely, patient-derived primary tumor data and in-vitro-derived cancer cell line data. We show that we can increase our generalization performance on primary tumors using cell lines as an auxiliary data source.","Kernelized Bayesian Transfer Learning Transfer learning considers related but distinct tasks defined on heterogenous domains and tries to transfer knowledge between these tasks to improve generalization performance. It is particularly useful when we do not have sufficient amount of labeled training data in some tasks, which may be very costly, laborious, or even infeasible to obtain. Instead, learning the tasks jointly enables us to effectively increase the amount of labeled training data. In this paper, we formulate a kernelized Bayesian transfer learning framework that is a principled combination of kernel-based dimensionality reduction models with task-specific projection matrices to find a shared subspace and a coupled classification model for all of the tasks in this subspace. Our two main contributions are: (i) two novel probabilistic models for binary and multiclass classification, and (ii) very efficient variational approximation procedures for these models. We illustrate the generalization performance of our algorithms on two different applications. In computer vision experiments, our method outperforms the state-of-the-art algorithms on nine out of 12 benchmark supervised domain adaptation experiments defined on two object recognition data sets. In cancer biology experiments, we use our algorithm to predict mutation status of important cancer genes from gene expression profiles using two distinct cancer populations, namely, patient-derived primary tumor data and in-vitro-derived cancer cell line data. We show that we can increase our generalization performance on primary tumors using cell lines as an auxiliary data source. cross-domain learning
domain adaptation
kernel methods
transfer learning
variational approximation",kernel bayesian transfer learn transfer learn consid relat distinct task defin heterogen domain tri transfer knowledg task improv general perform particular use suffici amount label train data task may cost labori even infeas obtain instead learn task joint enabl us effect increas amount label train data paper formul kernel bayesian transfer learn framework principl combin kernelbas dimension reduct model taskspecif project matric find share subspac coupl classif model task subspac two main contribut two novel probabilist model binari multiclass classif ii effici variat approxim procedur model illustr general perform algorithm two differ applic comput vision experi method outperform stateoftheart algorithm nine 12 benchmark supervis domain adapt experi defin two object recognit data set cancer biolog experi use algorithm predict mutat status import cancer gene gene express profil use two distinct cancer popul name patientderiv primari tumor data invitroderiv cancer cell line data show increas general perform primari tumor use cell line auxiliari data sourc crossdomain learn domain adapt kernel method transfer learn variat approxim,6,-14.5256605,-9.908694
"""Source Free"" Transfer Learning for Text Classification","Zhongqi Lu, Yin Zhu, Sinno Pan, Evan Xiang, Yujing Wang and Qiang Yang","AI and the Web (AIW)
Novel Machine Learning Algorithms (NMLA)","Transfer Learning
Auxiliary Data Retrieval
Text Classification","AIW: Knowledge acquisition from the web
AIW: Machine learning and the web
NMLA: Transfer, Adaptation, Multitask Learning","Transfer learning uses relevant auxiliary data to help the learning task in a target domain where labeled data are usually insufficient to train an accurate model. Given appropriate auxiliary data, researchers have proposed many transfer learning models. How to find such auxiliary data, however, is of little research in the past. In this paper, we focus on this auxiliary data retrieval problem, and propose a transfer learning framework that effectively selects helpful auxiliary data from an open knowledge space (e.g. the World Wide Web). Because there is no need of manually selecting auxiliary data for different target domain tasks, we call our framework Source Free Transfer Learning (SFTL). For each target domain task, SFTL framework iteratively queries for the helpful auxiliary data based on the learned model and then updates the model using the retrieved auxiliary data. We highlight the automatic constructions of queries and the robustness of the SFTL framework. Our experiments on the 20 NewsGroup dataset and the Google search snippets dataset suggest that the new framework is capable to have the comparable performance to those state-of-the-art methods with dedicated selections of auxiliary data.","""Source Free"" Transfer Learning for Text Classification Transfer learning uses relevant auxiliary data to help the learning task in a target domain where labeled data are usually insufficient to train an accurate model. Given appropriate auxiliary data, researchers have proposed many transfer learning models. How to find such auxiliary data, however, is of little research in the past. In this paper, we focus on this auxiliary data retrieval problem, and propose a transfer learning framework that effectively selects helpful auxiliary data from an open knowledge space (e.g. the World Wide Web). Because there is no need of manually selecting auxiliary data for different target domain tasks, we call our framework Source Free Transfer Learning (SFTL). For each target domain task, SFTL framework iteratively queries for the helpful auxiliary data based on the learned model and then updates the model using the retrieved auxiliary data. We highlight the automatic constructions of queries and the robustness of the SFTL framework. Our experiments on the 20 NewsGroup dataset and the Google search snippets dataset suggest that the new framework is capable to have the comparable performance to those state-of-the-art methods with dedicated selections of auxiliary data. Transfer Learning
Auxiliary Data Retrieval
Text Classification",sourc free transfer learn text classif transfer learn use relev auxiliari data help learn task target domain label data usual insuffici train accur model given appropri auxiliari data research propos mani transfer learn model find auxiliari data howev littl research past paper focus auxiliari data retriev problem propos transfer learn framework effect select help auxiliari data open knowledg space eg world wide web need manual select auxiliari data differ target domain task call framework sourc free transfer learn sftl target domain task sftl framework iter queri help auxiliari data base learn model updat model use retriev auxiliari data highlight automat construct queri robust sftl framework experi 20 newsgroup dataset googl search snippet dataset suggest new framework capabl compar perform stateoftheart method dedic select auxiliari data transfer learn auxiliari data retriev text classif,6,-14.467796,-9.890941
A Generalization of Probabilistic Serial to Randomized Social Choice,Haris Aziz and Paul Stursberg,Game Theory and Economic Paradigms (GTEP),"social choice theory
voting
fair division
social decision schemes","GTEP: Game Theory
GTEP: Social Choice / Voting","The probabilistic serial (PS) rule is one of the most well-established and desirable rules for the random assignment problem. We present the egalitarian simultaneous reservation (ESR) social decision scheme — an extension of PS to the more general setting of randomized social choice. ESR also generalizes an egalitarian rule from the literature which is defined only for dichotomous preferences. We consider various desirable fairness, efficiency, and strategic properties of ESR and show that it compares favourably against other social decision schemes. Finally, we define a more general class of social decision schemes called Simultaneous Reservation (SR), that contains ESR as well as the serial dictatorship rules. We show that outcomes of SR characterize efficiency with respect to a natural refinement of stochastic dominance.","A Generalization of Probabilistic Serial to Randomized Social Choice The probabilistic serial (PS) rule is one of the most well-established and desirable rules for the random assignment problem. We present the egalitarian simultaneous reservation (ESR) social decision scheme — an extension of PS to the more general setting of randomized social choice. ESR also generalizes an egalitarian rule from the literature which is defined only for dichotomous preferences. We consider various desirable fairness, efficiency, and strategic properties of ESR and show that it compares favourably against other social decision schemes. Finally, we define a more general class of social decision schemes called Simultaneous Reservation (SR), that contains ESR as well as the serial dictatorship rules. We show that outcomes of SR characterize efficiency with respect to a natural refinement of stochastic dominance. social choice theory
voting
fair division
social decision schemes",general probabilist serial random social choic probabilist serial ps rule one wellestablish desir rule random assign problem present egalitarian simultan reserv esr social decis scheme — extens ps general set random social choic esr also general egalitarian rule literatur defin dichotom prefer consid various desir fair effici strateg properti esr show compar favour social decis scheme final defin general class social decis scheme call simultan reserv sr contain esr well serial dictatorship rule show outcom sr character effici respect natur refin stochast domin social choic theori vote fair divis social decis scheme,9,17.450542,1.2323031
Lifetime Lexical Variation in Social Media,"Liao Lizi, Jing Jiang, Ying Ding, Heyan Huang and Ee-Peng Lim",NLP and Text Mining (NLPTM),"Generative model
Social Networks
Age Prediction","AIW: Web personalization and user modeling
NLPTM: Information Extraction
NLPTM: Natural Language Processing (General/Other)","As the rapid growth of online social media attracts a large number of Internet users, the large volume of content generated by these users also provides us with an opportunity to study the lexical variations of people of different age. In this paper, we present a latent variable model that jointly models the lexical content of tweets and Twitter users' age. Our model inherently assumes that a topic has not only a word distribution but also an age distribution. We propose a Gibbs-EM algorithm to perform inference on our model. Empirical evaluation shows that our model can generate meaningful age-specific topics such as ""school"" for teenagers and ""health"" for older people. Our model also performs age prediction better than a number of baseline methods.","Lifetime Lexical Variation in Social Media As the rapid growth of online social media attracts a large number of Internet users, the large volume of content generated by these users also provides us with an opportunity to study the lexical variations of people of different age. In this paper, we present a latent variable model that jointly models the lexical content of tweets and Twitter users' age. Our model inherently assumes that a topic has not only a word distribution but also an age distribution. We propose a Gibbs-EM algorithm to perform inference on our model. Empirical evaluation shows that our model can generate meaningful age-specific topics such as ""school"" for teenagers and ""health"" for older people. Our model also performs age prediction better than a number of baseline methods. Generative model
Social Networks
Age Prediction",lifetim lexic variat social media rapid growth onlin social media attract larg number internet user larg volum content generat user also provid us opportun studi lexic variat peopl differ age paper present latent variabl model joint model lexic content tweet twitter user age model inher assum topic word distribut also age distribut propos gibbsem algorithm perform infer model empir evalu show model generat meaning agespecif topic school teenag health older peopl model also perform age predict better number baselin method generat model social network age predict,0,8.226215,0.5923469
Hybrid Singular Value Thresholding for Tensor Completion,"Xiaoqin Zhang, Zhengyuan Zhou, Di Wang and Yi Ma","Knowledge Representation and Reasoning (KRR)
Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)
Vision (VIS)","tensor completion
low-rank recovery
hybrid singular value thresholding","KRR: Knowledge Representation (General/Other)
MLA: Machine Learning Applications (General/other)
NMLA: Data Mining and Knowledge Discovery
NMLA: Dimension Reduction/Feature Selection
VIS: Statistical Methods and Learning","In this paper, we study the low-rank tensor completion problem, where a high-order tensor with missing entries is given and the goal is to complete the tensor. We propose to minimize a new convex objective function, based on log sum of exponentials of nuclear norms, that promotes the low-rankness of unfolding matrices of the completed tensor. We show for the first time that the proximal operator to this objective function is readily computable through a hybrid singular value thresholding scheme. This leads to a new solution to high-order (low-rank) tensor completion via convex relaxation. We show that this convex relaxation and the resulting solution are much more effective than existing tensor completion methods
(including those also based on minimizing ranks of unfolding matrices). The hybrid singular value thresholding scheme can be applied to any problem where the goal is
to minimize the maximum rank of a set of low-rank matrices.","Hybrid Singular Value Thresholding for Tensor Completion In this paper, we study the low-rank tensor completion problem, where a high-order tensor with missing entries is given and the goal is to complete the tensor. We propose to minimize a new convex objective function, based on log sum of exponentials of nuclear norms, that promotes the low-rankness of unfolding matrices of the completed tensor. We show for the first time that the proximal operator to this objective function is readily computable through a hybrid singular value thresholding scheme. This leads to a new solution to high-order (low-rank) tensor completion via convex relaxation. We show that this convex relaxation and the resulting solution are much more effective than existing tensor completion methods
(including those also based on minimizing ranks of unfolding matrices). The hybrid singular value thresholding scheme can be applied to any problem where the goal is
to minimize the maximum rank of a set of low-rank matrices. tensor completion
low-rank recovery
hybrid singular value thresholding",hybrid singular valu threshold tensor complet paper studi lowrank tensor complet problem highord tensor miss entri given goal complet tensor propos minim new convex object function base log sum exponenti nuclear norm promot lowrank unfold matric complet tensor show first time proxim oper object function readili comput hybrid singular valu threshold scheme lead new solut highord lowrank tensor complet via convex relax show convex relax result solut much effect exist tensor complet method includ also base minim rank unfold matric hybrid singular valu threshold scheme appli problem goal minim maximum rank set lowrank matric tensor complet lowrank recoveri hybrid singular valu threshold,7,-22.718435,2.2447379
Locality Preserving Hashing,"Kang Zhao, Hongtao Lu and Jincheng Mei",Vision (VIS),"Similarity Search
Approximate Nearest Neighbor Search
Binary Codes
Locality Preserving Hashing",VIS: Image and Video Retrieval,"Hashing has recently attracted considerable attention for large scale similarity search. However, learning compact codes with good performance is still a challenge. In many cases, the real-world data lies on a low-dimensional manifold embedded in high-dimensional ambient space. To capture meaningful neighbors, a compact hashing representation should uncover the intrinsic geometric structure of the manifold, e.g., the neighborhood relationships between subregions. Most existing hashing methods only consider this issue during mapping data points into certain projected dimensions. When getting the binary codes, they either directly quantize the projected values with a threshold, or use an orthogonal matrix to refine the initial projection matrix, which both consider projection and quantization separately, and it will not well preserve the locality structure in the whole learning process. In this paper, we propose a novel hashing algorithm called Locality Preserving Hashing to effectively solve the above problems. Specifically, we learn a set of locality preserving projections with a joint optimization framework, which minimizes the average projection distance and quantization loss simultaneously. Experimental comparisons with other state-of-the-art methods on two large scale databases demonstrate the effectiveness and efficiency of our method.","Locality Preserving Hashing Hashing has recently attracted considerable attention for large scale similarity search. However, learning compact codes with good performance is still a challenge. In many cases, the real-world data lies on a low-dimensional manifold embedded in high-dimensional ambient space. To capture meaningful neighbors, a compact hashing representation should uncover the intrinsic geometric structure of the manifold, e.g., the neighborhood relationships between subregions. Most existing hashing methods only consider this issue during mapping data points into certain projected dimensions. When getting the binary codes, they either directly quantize the projected values with a threshold, or use an orthogonal matrix to refine the initial projection matrix, which both consider projection and quantization separately, and it will not well preserve the locality structure in the whole learning process. In this paper, we propose a novel hashing algorithm called Locality Preserving Hashing to effectively solve the above problems. Specifically, we learn a set of locality preserving projections with a joint optimization framework, which minimizes the average projection distance and quantization loss simultaneously. Experimental comparisons with other state-of-the-art methods on two large scale databases demonstrate the effectiveness and efficiency of our method. Similarity Search
Approximate Nearest Neighbor Search
Binary Codes
Locality Preserving Hashing",local preserv hash hash recent attract consider attent larg scale similar search howev learn compact code good perform still challeng mani case realworld data lie lowdimension manifold embed highdimension ambient space captur meaning neighbor compact hash represent uncov intrins geometr structur manifold eg neighborhood relationship subregion exist hash method consid issu map data point certain project dimens get binari code either direct quantiz project valu threshold use orthogon matrix refin initi project matrix consid project quantize separ well preserv local structur whole learn process paper propos novel hash algorithm call local preserv hash effect solv problem specif learn set local preserv project joint optim framework minim averag project distanc quantize loss simultan experiment comparison stateoftheart method two larg scale databas demonstr effect effici method similar search approxim nearest neighbor search binari code local preserv hash,1,7.2896495,-11.89428
Discovering Better AAAI Keywords via Clustering with Crowd-sourced Constraints,"Kelly Moran, Byron Wallace and Carla Brodley",Machine Learning Applications (MLA),"constraint-based clustering
machine learning
crowdsourcing",MLA: Applications of Unsupervised Learning,"Selecting good conference keywords is important because they often determine the composition of review committees and hence which papers are reviewed by whom. But presently conference keywords are generated in an ad-hoc manner by a small set of conference organizers. This approach is plainly not ideal. There is no guarantee, for example, that the generated keyword set aligns with what the community is actually working on and submitting to the conference in a given year. This is especially true in fast moving fields such as AI. The problem is exacerbated by the tendency of organizers to draw heavily on preceding years' keyword lists when generating a new set. Rather than a select few ordaining a keyword set that that represents AI at large, it would be preferable to generate these keywords more directly from the data, with input from research community members. To this end, we solicited feedback from seven AAAI PC members regarding a previously existing keyword set and used these 'crowd-sourced constraints' to inform a clustering over the abstracts of all submissions to AAAI 2013. We show that the keywords discovered via this data-driven, human-in-the-loop method are at least as preferred (by AAAI PC members) as 2013's manually generated set, and that they include categories previously overlooked by organizers. Many of the discovered terms were used for this year's conference.","Discovering Better AAAI Keywords via Clustering with Crowd-sourced Constraints Selecting good conference keywords is important because they often determine the composition of review committees and hence which papers are reviewed by whom. But presently conference keywords are generated in an ad-hoc manner by a small set of conference organizers. This approach is plainly not ideal. There is no guarantee, for example, that the generated keyword set aligns with what the community is actually working on and submitting to the conference in a given year. This is especially true in fast moving fields such as AI. The problem is exacerbated by the tendency of organizers to draw heavily on preceding years' keyword lists when generating a new set. Rather than a select few ordaining a keyword set that that represents AI at large, it would be preferable to generate these keywords more directly from the data, with input from research community members. To this end, we solicited feedback from seven AAAI PC members regarding a previously existing keyword set and used these 'crowd-sourced constraints' to inform a clustering over the abstracts of all submissions to AAAI 2013. We show that the keywords discovered via this data-driven, human-in-the-loop method are at least as preferred (by AAAI PC members) as 2013's manually generated set, and that they include categories previously overlooked by organizers. Many of the discovered terms were used for this year's conference. constraint-based clustering
machine learning
crowdsourcing",discov better aaai keyword via cluster crowdsourc constraint select good confer keyword import often determin composit review committe henc paper review present confer keyword generat adhoc manner small set confer organ approach plain ideal guarante exampl generat keyword set align communiti actual work submit confer given year especi true fast move field ai problem exacerb tendenc organ draw heavili preced year keyword list generat new set rather select ordain keyword set repres ai larg would prefer generat keyword direct data input research communiti member end solicit feedback seven aaai pc member regard previous exist keyword set use crowdsourc constraint inform cluster abstract submiss aaai 2013 show keyword discov via datadriven humanintheloop method least prefer aaai pc member 2013s manual generat set includ categori previous overlook organ mani discov term use year confer constraintbas cluster machin learn crowdsourc,7,-0.20211127,2.911401
Online Classification Using a Voted RDA Method,"Tianbing Xu, Jianfeng Gao, Lin Xiao and Amelia Regan","Machine Learning Applications (MLA)
NLP and Machine Learning (NLPML)
Novel Machine Learning Algorithms (NMLA)","Online Classification
Voted Dual Averaging Method
Natural Language Processing
Parsing Reranking
Sparse Regularization","MLA: Machine Learning Applications (General/other)
NLPML: Natural Language Processing (General/Other)
NMLA: Big Data / Scalability
NMLA: Classification
NMLA: Online Learning","We propose a voted dual averaging method for online
classification problems with explicit regularization.
This method employs the update rule of the regularized
dual averaging (RDA) method proposed by Xiao, but
only on the subsequence of training examples where a
classification error is made. We derive a bound on the
number of mistakes made by this method on the training
set, as well as its generalization error rate.We also introduce
the concept of relative strength of regularization,
and show how it affects the mistake bound and generalization
performance. We examine the method using
ℓ1-regularization on a large-scale natural language processing
task, and obtained state-of-the-art classification
performance with fairly sparse models.","Online Classification Using a Voted RDA Method We propose a voted dual averaging method for online
classification problems with explicit regularization.
This method employs the update rule of the regularized
dual averaging (RDA) method proposed by Xiao, but
only on the subsequence of training examples where a
classification error is made. We derive a bound on the
number of mistakes made by this method on the training
set, as well as its generalization error rate.We also introduce
the concept of relative strength of regularization,
and show how it affects the mistake bound and generalization
performance. We examine the method using
ℓ1-regularization on a large-scale natural language processing
task, and obtained state-of-the-art classification
performance with fairly sparse models. Online Classification
Voted Dual Averaging Method
Natural Language Processing
Parsing Reranking
Sparse Regularization",onlin classif use vote rda method propos vote dual averag method onlin classif problem explicit regular method employ updat rule regular dual averag rda method propos xiao subsequ train exampl classif error made deriv bound number mistak made method train set well general error ratew also introduc concept relat strength regular show affect mistak bound general perform examin method use ℓ1regular largescal natur languag process task obtain stateoftheart classif perform fair spars model onlin classif vote dual averag method natur languag process pars rerank spars regular,4,-5.003317,-6.2128344
Fraudulent Support Telephone Number Identification Based on Co-occurrence Information on the Web,"Xin Li, Yiqun Liu, Min Zhang and Shaoping Ma",AI and the Web (AIW),"Fraudulent Support Telephone Number
Co-occurrence Graph
Propagation Algorithm","AIW: Enhancing web search and information retrieval
AIW: Recognizing web spam such as link farms and splogs","""Fraudulent support phones"" refers to the misleading telephone numbers placed on Web pages or other media that claim to provide services with which they are not associated. Most fraudulent support phone information is found on search engine result pages (SERPs), and such information substantially degrades the search engine user experience. In this paper, we propose an approach to identify fraudulent support telephone numbers on the Web based on the co-occurrence relations between telephone numbers that appear on SERPs. We start from a small set of seed official support phone numbers and seed fraudulent numbers. Then, we construct a co-occurrence graph according to the co-occurrence relationships of the telephone numbers that appear on Web pages. Additionally, we take the page layout information into consideration on the assumption that telephone numbers that appear in nearby page blocks should be regarded as more closely related. Finally, we develop a propagation algorithm to diffuse the trust scores of seed official support phone numbers and the distrust scores of the seed fraudulent numbers on the co-occurrence graph to detect additional fraudulent numbers. Experimental results based on over 1.5 million SERPs produced by a popular Chinese commercial search engine indicate that our approach outperforms TrustRank, Anti-TrustRank and Good-Bad Rank algorithms by achieving an AUC value of over 0.90.","Fraudulent Support Telephone Number Identification Based on Co-occurrence Information on the Web ""Fraudulent support phones"" refers to the misleading telephone numbers placed on Web pages or other media that claim to provide services with which they are not associated. Most fraudulent support phone information is found on search engine result pages (SERPs), and such information substantially degrades the search engine user experience. In this paper, we propose an approach to identify fraudulent support telephone numbers on the Web based on the co-occurrence relations between telephone numbers that appear on SERPs. We start from a small set of seed official support phone numbers and seed fraudulent numbers. Then, we construct a co-occurrence graph according to the co-occurrence relationships of the telephone numbers that appear on Web pages. Additionally, we take the page layout information into consideration on the assumption that telephone numbers that appear in nearby page blocks should be regarded as more closely related. Finally, we develop a propagation algorithm to diffuse the trust scores of seed official support phone numbers and the distrust scores of the seed fraudulent numbers on the co-occurrence graph to detect additional fraudulent numbers. Experimental results based on over 1.5 million SERPs produced by a popular Chinese commercial search engine indicate that our approach outperforms TrustRank, Anti-TrustRank and Good-Bad Rank algorithms by achieving an AUC value of over 0.90. Fraudulent Support Telephone Number
Co-occurrence Graph
Propagation Algorithm",fraudul support telephon number identif base cooccurr inform web fraudul support phone refer mislead telephon number place web page media claim provid servic associ fraudul support phone inform found search engin result page serp inform substanti degrad search engin user experi paper propos approach identifi fraudul support telephon number web base cooccurr relat telephon number appear serp start small set seed offici support phone number seed fraudul number construct cooccurr graph accord cooccurr relationship telephon number appear web page addit take page layout inform consider assumpt telephon number appear nearbi page block regard close relat final develop propag algorithm diffus trust score seed offici support phone number distrust score seed fraudul number cooccurr graph detect addit fraudul number experiment result base 15 million serp produc popular chines commerci search engin indic approach outperform trustrank antitrustrank goodbad rank algorithm achiev auc valu 090 fraudul support telephon number cooccurr graph propag algorithm,0,-1.1012974,1.3626969
Supervised Hashing for Image Retrieval via Image Representation Learning,"Rongkai Xia, Yan Pan, Hanjiang Lai, Cong Liu and Shuicheng Yan","Novel Machine Learning Algorithms (NMLA)
Vision (VIS)","supervised hashing
approximate near neighbor search
representation learning
convolutional neural networks
coordinate descent","NMLA: Neural Networks/Deep Learning
VIS: Image and Video Retrieval","Hashing is a popular approximate nearest neighbor search approach in large-scale image retrieval. Supervised hashing, which incorporates similarity/dissimilarity information on entity pairs to improve the quality of hashing function learning, has recently received increasing attention. However, in the existing supervised hashing methods for images, an input image is usually encoded by a vector of hand-crafted visual features. Such hand-crafted feature vectors do not necessary preserve the accurate semantic similarities of images pairs, which may often degrade the performance of hashing function learning. In this paper, we propose a supervised hashing method for image search, in which we automatically learn a good image representation tailored to hashing as well as a set of hash functions. The proposed method has two stages. In the first stage, given the pairwise similarity matrix $S$ on pairs of training images, we propose a scalable coordinate descent method to decompose $S$ into a product of $HH^T$ where $H$ is a matrix with each of its row being the approximate hash code associated to a training image. In the second stage, we propose to simultaneously learn a good feature representation for the input images as well as a set of hash functions, via a deep convolutional network tailored to the learned hash codes in $H$ or the discrete class labels of the images. Extensive empirical evaluations on three benchmark datasets with different kinds of images show that the proposed method has superior performance gains over several state-of-the-art supervised and unsupervised hashing methods.","Supervised Hashing for Image Retrieval via Image Representation Learning Hashing is a popular approximate nearest neighbor search approach in large-scale image retrieval. Supervised hashing, which incorporates similarity/dissimilarity information on entity pairs to improve the quality of hashing function learning, has recently received increasing attention. However, in the existing supervised hashing methods for images, an input image is usually encoded by a vector of hand-crafted visual features. Such hand-crafted feature vectors do not necessary preserve the accurate semantic similarities of images pairs, which may often degrade the performance of hashing function learning. In this paper, we propose a supervised hashing method for image search, in which we automatically learn a good image representation tailored to hashing as well as a set of hash functions. The proposed method has two stages. In the first stage, given the pairwise similarity matrix $S$ on pairs of training images, we propose a scalable coordinate descent method to decompose $S$ into a product of $HH^T$ where $H$ is a matrix with each of its row being the approximate hash code associated to a training image. In the second stage, we propose to simultaneously learn a good feature representation for the input images as well as a set of hash functions, via a deep convolutional network tailored to the learned hash codes in $H$ or the discrete class labels of the images. Extensive empirical evaluations on three benchmark datasets with different kinds of images show that the proposed method has superior performance gains over several state-of-the-art supervised and unsupervised hashing methods. supervised hashing
approximate near neighbor search
representation learning
convolutional neural networks
coordinate descent",supervis hash imag retriev via imag represent learn hash popular approxim nearest neighbor search approach largescal imag retriev supervis hash incorpor similaritydissimilar inform entiti pair improv qualiti hash function learn recent receiv increas attent howev exist supervis hash method imag input imag usual encod vector handcraft visual featur handcraft featur vector necessari preserv accur semant similar imag pair may often degrad perform hash function learn paper propos supervis hash method imag search automat learn good imag represent tailor hash well set hash function propos method two stage first stage given pairwis similar matrix pair train imag propos scalabl coordin descent method decompos product hht h matrix row approxim hash code associ train imag second stage propos simultan learn good featur represent input imag well set hash function via deep convolut network tailor learn hash code h discret class label imag extens empir evalu three benchmark dataset differ kind imag show propos method superior perform gain sever stateoftheart supervis unsupervis hash method supervis hash approxim near neighbor search represent learn convolut neural network coordin descent,1,-21.796694,-7.428891
Tailoring Local Search for Partial MaxSAT,"Shaowei Cai, Chuan Luo, Kaile Su and John Thornton","Heuristic Search and Optimization (HSO)
Search and Constraint Satisfaction (SCS)","Partial MaxSAT
Local Search
Heuristics","HSO: Heuristic Search
HSO: Optimization
SCS: Constraint Optimization","Partial MaxSAT (PMS) is a generalization to SAT and MaxSAT. Many real world problems can be encoded into PMS in a more natural and compact way than SAT and MaxSAT. In this paper, we propose new ideas for local search for PMS, which mainly rely on the distinction between hard and soft clauses. We then use these ideas to develop a local search PMS algorithm called Dist. Experimental results on PMS benchmarks from MaxSAT Evaluation 2013 show that Dist significantly outperforms state-of-the-art PMS algorithms, including both local search algorithms and complete ones, on random and crafted benchmarks. For the industrial benchmark, Dist dramatically outperforms previous local search algorithms and is comparable and complementary to complete algorithms.","Tailoring Local Search for Partial MaxSAT Partial MaxSAT (PMS) is a generalization to SAT and MaxSAT. Many real world problems can be encoded into PMS in a more natural and compact way than SAT and MaxSAT. In this paper, we propose new ideas for local search for PMS, which mainly rely on the distinction between hard and soft clauses. We then use these ideas to develop a local search PMS algorithm called Dist. Experimental results on PMS benchmarks from MaxSAT Evaluation 2013 show that Dist significantly outperforms state-of-the-art PMS algorithms, including both local search algorithms and complete ones, on random and crafted benchmarks. For the industrial benchmark, Dist dramatically outperforms previous local search algorithms and is comparable and complementary to complete algorithms. Partial MaxSAT
Local Search
Heuristics",tailor local search partial maxsat partial maxsat pms general sat maxsat mani real world problem encod pms natur compact way sat maxsat paper propos new idea local search pms main reli distinct hard soft claus use idea develop local search pms algorithm call dist experiment result pms benchmark maxsat evalu 2013 show dist signific outperform stateoftheart pms algorithm includ local search algorithm complet one random craft benchmark industri benchmark dist dramat outperform previous local search algorithm compar complementari complet algorithm partial maxsat local search heurist,4,-11.095279,15.849639
R2: An Efficient MCMC Sampler for Probabilistic Programs,"Aditya Nori, Chung-Kil Hur, Sriram Rajamani and Selva Samuel","Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)","Probabilistic programming
Program analysis
Sampling","MLA: Machine Learning Applications (General/other)
NMLA: Bayesian Learning
RU: Bayesian Networks
RU: Graphical Models (Other)
RU: Probabilistic Inference","We present a new Markov Chain Monte Carlo (MCMC) sampling algorithm for probabilistic programs. Our approach and tool, called R2, has the unique feature of employing program analysis in order to improve the efficiency of MCMC sampling. Given an input program P, R2 propagates observations in P backwards to obtain a semantically equivalent program P' in which every probabilistic assignment is immediately followed by an observe statement. Inference is performed by a suitably modified version of the Metropolis-Hastings algorithm that exploits the structure of the program P0. This has the overall effect of preventing rejections due to program executions that fail to satisfy observations in P. We formalize the semantics of probabilistic programs and rigorously prove the correctness of R2.We also empirically demonstrate the effectiveness of R2—--in particular, we show that R2 is able to produce results of similar quality as the Church and Stan probabilistic programming tools with much shorter execution time.","R2: An Efficient MCMC Sampler for Probabilistic Programs We present a new Markov Chain Monte Carlo (MCMC) sampling algorithm for probabilistic programs. Our approach and tool, called R2, has the unique feature of employing program analysis in order to improve the efficiency of MCMC sampling. Given an input program P, R2 propagates observations in P backwards to obtain a semantically equivalent program P' in which every probabilistic assignment is immediately followed by an observe statement. Inference is performed by a suitably modified version of the Metropolis-Hastings algorithm that exploits the structure of the program P0. This has the overall effect of preventing rejections due to program executions that fail to satisfy observations in P. We formalize the semantics of probabilistic programs and rigorously prove the correctness of R2.We also empirically demonstrate the effectiveness of R2—--in particular, we show that R2 is able to produce results of similar quality as the Church and Stan probabilistic programming tools with much shorter execution time. Probabilistic programming
Program analysis
Sampling",r2 effici mcmc sampler probabilist program present new markov chain mont carlo mcmc sampl algorithm probabilist program approach tool call r2 uniqu featur employ program analysi order improv effici mcmc sampl given input program p r2 propag observ p backward obtain semant equival program p everi probabilist assign immedi follow observ statement infer perform suitabl modifi version metropolishast algorithm exploit structur program p0 overal effect prevent reject due program execut fail satisfi observ p formal semant probabilist program rigor prove correct r2we also empir demonstr effect r2—in particular show r2 abl produc result similar qualiti church stan probabilist program tool much shorter execut time probabilist program program analysi sampl,3,-13.968264,1.5689132
Reconsidering Mutual Information Based Feature Selection: A Statistical Significance View,"Vinh Nguyen, Jeffrey Chan and James Bailey",Novel Machine Learning Algorithms (NMLA),"feature selection
mutual information
global optimization","NMLA: Data Mining and Knowledge Discovery
NMLA: Dimension Reduction/Feature Selection","Mutual information (MI) based approaches are an important feature selection paradigm. Although the stated goal of MI-based feature selection is to identify a subset of features that share the highest mutual information with the class variable, most current MI-based techniques are greedy methods that make use of low dimensional MI quantities. The reason for using low dimensional approximation has been mostly attributed to the difficulty associated with estimating the high dimensional MI from limited samples. In this paper, we argue a different viewpoint that,  given a  very large amount of data, the high dimensional MI objective  is still problematic to be employed as a meaningful optimization criterion, due to its overfitting nature: the MI almost always increases as more features are added, thus leading to a trivial solution which includes all features. We  propose a novel approach to the MI-based feature selection problem, in which the overfitting phenomenon is controlled rigourously by means of a statistical test. We develop local and global optimization algorithms for this new feature selection model, and demonstrate its effectiveness in the applications of explaining variables and objects.","Reconsidering Mutual Information Based Feature Selection: A Statistical Significance View Mutual information (MI) based approaches are an important feature selection paradigm. Although the stated goal of MI-based feature selection is to identify a subset of features that share the highest mutual information with the class variable, most current MI-based techniques are greedy methods that make use of low dimensional MI quantities. The reason for using low dimensional approximation has been mostly attributed to the difficulty associated with estimating the high dimensional MI from limited samples. In this paper, we argue a different viewpoint that,  given a  very large amount of data, the high dimensional MI objective  is still problematic to be employed as a meaningful optimization criterion, due to its overfitting nature: the MI almost always increases as more features are added, thus leading to a trivial solution which includes all features. We  propose a novel approach to the MI-based feature selection problem, in which the overfitting phenomenon is controlled rigourously by means of a statistical test. We develop local and global optimization algorithms for this new feature selection model, and demonstrate its effectiveness in the applications of explaining variables and objects. feature selection
mutual information
global optimization",reconsid mutual inform base featur select statist signific view mutual inform mi base approach import featur select paradigm although state goal mibas featur select identifi subset featur share highest mutual inform class variabl current mibas techniqu greedi method make use low dimension mi quantiti reason use low dimension approxim most attribut difficulti associ estim high dimension mi limit sampl paper argu differ viewpoint given larg amount data high dimension mi object still problemat employ meaning optim criterion due overfit natur mi almost alway increas featur ad thus lead trivial solut includ featur propos novel approach mibas featur select problem overfit phenomenon control rigour mean statist test develop local global optim algorithm new featur select model demonstr effect applic explain variabl object featur select mutual inform global optim,5,-8.33753,-21.499332
Influence Maximization with Novelty Decay in Social Networks,"Shanshan Feng, Xuefeng Chen, Gao Cong, Yifeng Zeng, Yeow Meng Chee and Yanping Xiang",AI and the Web (AIW),"social networks
influence maximization
novelty decay",AIW: Social networking and community identification,"Influence maximization problem is to find a set of seed
nodes in a social network such that their influence
spread is maximized under certain propagation models.
A few algorithms have been proposed for this problem.
However, they have not considered the impact of
novelty decay in influence propagation, i.e., repeated
exposures will have diminishing influence on users. In
this paper, we consider the problem of influence max-
imization with novelty decay (IMND). We investigate
the effect of novelty decay on influence propagation in
real-life datasets and formulate the IMND problem. We
further analyze its relevant properties and propose an
influence estimation technique. We demonstrate perfor-
mance of our algorithms over four social networks.","Influence Maximization with Novelty Decay in Social Networks Influence maximization problem is to find a set of seed
nodes in a social network such that their influence
spread is maximized under certain propagation models.
A few algorithms have been proposed for this problem.
However, they have not considered the impact of
novelty decay in influence propagation, i.e., repeated
exposures will have diminishing influence on users. In
this paper, we consider the problem of influence max-
imization with novelty decay (IMND). We investigate
the effect of novelty decay on influence propagation in
real-life datasets and formulate the IMND problem. We
further analyze its relevant properties and propose an
influence estimation technique. We demonstrate perfor-
mance of our algorithms over four social networks. social networks
influence maximization
novelty decay",influenc maxim novelti decay social network influenc maxim problem find set seed node social network influenc spread maxim certain propag model algorithm propos problem howev consid impact novelti decay influenc propag ie repeat exposur diminish influenc user paper consid problem influenc max imize novelti decay imnd investig effect novelti decay influenc propag reallif dataset formul imnd problem analyz relev properti propos influenc estim techniqu demonstr perfor manc algorithm four social network social network influenc maxim novelti decay,5,12.266906,3.7036886
Solving Uncertain MDPs by Reusing State Information and Plans,"Ping Hou, William Yeoh and Tran Cao Son",Planning and Scheduling (PS),"Markov Decision Processes (MDPs)
Replanning
Incremental Search
Uncertain MDPs","PS: Probabilistic Planning
PS: Replanning and Plan Repair
PS: Planning (General/Other)","While Markov decision processes (MDPs) are powerful tools for modeling sequential decision making problems under uncertainty, they are sensitive to the accuracy of their parameters. MDPs with uncertainty in their parameters are called Uncertain MDPs. In this paper, we introduce a general framework that allows off-the-shelf MDP algorithms to solve Uncertain MDPs by planning based on currently available information and replan if and when the problem changes. We demonstrate the generality of this approach by showing that it can use the VI, TVI, ILAO*, LRTDP, and UCT algorithms to solve Uncertain MDPs. We experimentally show that our approach is typically faster than replanning from scratch and we also provide a way to estimate the amount of speedup based on the amount of information is reused.","Solving Uncertain MDPs by Reusing State Information and Plans While Markov decision processes (MDPs) are powerful tools for modeling sequential decision making problems under uncertainty, they are sensitive to the accuracy of their parameters. MDPs with uncertainty in their parameters are called Uncertain MDPs. In this paper, we introduce a general framework that allows off-the-shelf MDP algorithms to solve Uncertain MDPs by planning based on currently available information and replan if and when the problem changes. We demonstrate the generality of this approach by showing that it can use the VI, TVI, ILAO*, LRTDP, and UCT algorithms to solve Uncertain MDPs. We experimentally show that our approach is typically faster than replanning from scratch and we also provide a way to estimate the amount of speedup based on the amount of information is reused. Markov Decision Processes (MDPs)
Replanning
Incremental Search
Uncertain MDPs",solv uncertain mdps reus state inform plan markov decis process mdps power tool model sequenti decis make problem uncertainti sensit accuraci paramet mdps uncertainti paramet call uncertain mdps paper introduc general framework allow offtheshelf mdp algorithm solv uncertain mdps plan base current avail inform replan problem chang demonstr general approach show use vi tvi ilao lrtdp uct algorithm solv uncertain mdps experiment show approach typic faster replan scratch also provid way estim amount speedup base amount inform reus markov decis process mdps replan increment search uncertain mdps,7,-2.668718,-7.059934
Identifying Differences in Physician Communication Styles with a Log-Linear Transition Component Model,"Byron Wallace, Issa Dahabreh, Michael Barton Laws, Ira Wilson, Thomas Trikalinos and Eugene Charniak","Applications (APP)
Machine Learning Applications (MLA)
NLP and Machine Learning (NLPML)","Conversation modeling
Patient-doctor communication
Sequential component model","APP: Biomedical / Bioinformatics
MLA: Bio/Medicine
MLA: Applications of Supervised Learning
NLPML: Discourse and Dialogue","We consider the task of grouping doctors with respect to communication patterns exhibited in outpatient visits. We propose a novel approach toward this end in which we model speech act transitions in conversations via a log-linear model incorporating physician specific components. We train this model over transcripts of outpatient visits annotated with speech act codes and then cluster physicians in (a transformation of) this parameter space. We find significant correlations between the induced groupings and patient survey response data comprising ratings of physician communication. Furthermore, the novel sequential component model we leverage to induce this clustering allows us to explore differences across these groups. This work demonstrates how statistical AI might be used to better understand (and ultimately improve) physician communication.","Identifying Differences in Physician Communication Styles with a Log-Linear Transition Component Model We consider the task of grouping doctors with respect to communication patterns exhibited in outpatient visits. We propose a novel approach toward this end in which we model speech act transitions in conversations via a log-linear model incorporating physician specific components. We train this model over transcripts of outpatient visits annotated with speech act codes and then cluster physicians in (a transformation of) this parameter space. We find significant correlations between the induced groupings and patient survey response data comprising ratings of physician communication. Furthermore, the novel sequential component model we leverage to induce this clustering allows us to explore differences across these groups. This work demonstrates how statistical AI might be used to better understand (and ultimately improve) physician communication. Conversation modeling
Patient-doctor communication
Sequential component model",identifi differ physician communic style loglinear transit compon model consid task group doctor respect communic pattern exhibit outpati visit propos novel approach toward end model speech act transit convers via loglinear model incorpor physician specif compon train model transcript outpati visit annot speech act code cluster physician transform paramet space find signific correl induc group patient survey respons data compris rate physician communic furthermor novel sequenti compon model leverag induc cluster allow us explor differ across group work demonstr statist ai might use better understand ultim improv physician communic convers model patientdoctor communic sequenti compon model,7,4.711837,-0.030387217
Multi-Organ Exchange: The Whole is Greater than the Sum of its Parts,John Dickerson and Tuomas Sandholm,"Applications (APP)
Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Kidney exchange
Sparse random graphs
Computational economics","APP: Biomedical / Bioinformatics
GTEP: Auctions and Market-Based Systems
MAS: Mechanism Design","Kidney exchange, where candidates with organ failure trade incompatible but willing donors, is a life-saving alternative to the deceased donor waitlist, which has inadequate supply to meet demand.  While fielded kidney exchanges see huge benefit from altruistic kidney donors (who give an organ without a paired needy candidate), a significantly higher medical risk to the donor deters similar altruism with livers.  In this paper, we begin by proposing the idea of liver exchange, and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level.  We then explore cross-organ donation where kidneys and livers can be bartered for each other.  We show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool.  We support this result experimentally on demographically accurate multi-organ exchanges.  We conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view.","Multi-Organ Exchange: The Whole is Greater than the Sum of its Parts Kidney exchange, where candidates with organ failure trade incompatible but willing donors, is a life-saving alternative to the deceased donor waitlist, which has inadequate supply to meet demand.  While fielded kidney exchanges see huge benefit from altruistic kidney donors (who give an organ without a paired needy candidate), a significantly higher medical risk to the donor deters similar altruism with livers.  In this paper, we begin by proposing the idea of liver exchange, and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level.  We then explore cross-organ donation where kidneys and livers can be bartered for each other.  We show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool.  We support this result experimentally on demographically accurate multi-organ exchanges.  We conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view. Kidney exchange
Sparse random graphs
Computational economics",multiorgan exchang whole greater sum part kidney exchang candid organ failur trade incompat will donor lifesav altern deceas donor waitlist inadequ suppli meet demand field kidney exchang see huge benefit altruist kidney donor give organ without pair needi candid signific higher medic risk donor deter similar altruism liver paper begin propos idea liver exchang show demograph accur data vet kidney exchang algorithm adapt clear exchang nationwid level explor crossorgan donat kidney liver barter show theoret multiorgan exchang provid linear transplant run separ kidney liver exchang linear gain product altruist kidney donor creat chain thread liver pool support result experiment demograph accur multiorgan exchang conclud thought regard field nationwid liver joint liverkidney exchang legal comput point view kidney exchang spars random graph comput econom,9,16.907051,16.725689
A Latent Variable Model for Discovering Bird Species Commonly Misidentified by Citizen Scientists,"Jun Yu, Rebecca Hutchinson and Weng-Keen Wong",Computational Sustainability and AI (CSAI),"Probabilistic Graphical Model
Crowdsourcing
Citizen Science
Ecology",CSAI: Modeling and prediction of dynamic and spatiotemporal phenomena and systems,"Data quality is a common source of concern for large-scale citizen science projects like eBird. In the case of eBird, a major cause of poor quality data is the misidentification of bird species by inexperienced contributors. A proactive approach for improving data quality is to identify commonly misidentified bird species and to teach inexperienced birders the differences between these species. In this paper, we develop a latent variable graphical model that can identify groups of bird species that are often confused for each other by eBird participants. Our model is a multi-species extension of the classic occupancy-detection model in the ecology literature. This multi-species extension is non-trivial, requiring a structure learning step as well as a computationally expensive parameter learning stage which we make efficient through a variational approximation. We show that by including these species misidentifications in the model, we can not only discover these misidentifications but predictions of both species occupancy and detection are also more accurate.","A Latent Variable Model for Discovering Bird Species Commonly Misidentified by Citizen Scientists Data quality is a common source of concern for large-scale citizen science projects like eBird. In the case of eBird, a major cause of poor quality data is the misidentification of bird species by inexperienced contributors. A proactive approach for improving data quality is to identify commonly misidentified bird species and to teach inexperienced birders the differences between these species. In this paper, we develop a latent variable graphical model that can identify groups of bird species that are often confused for each other by eBird participants. Our model is a multi-species extension of the classic occupancy-detection model in the ecology literature. This multi-species extension is non-trivial, requiring a structure learning step as well as a computationally expensive parameter learning stage which we make efficient through a variational approximation. We show that by including these species misidentifications in the model, we can not only discover these misidentifications but predictions of both species occupancy and detection are also more accurate. Probabilistic Graphical Model
Crowdsourcing
Citizen Science
Ecology",latent variabl model discov bird speci common misidentifi citizen scientist data qualiti common sourc concern largescal citizen scienc project like ebird case ebird major caus poor qualiti data misidentif bird speci inexperienc contributor proactiv approach improv data qualiti identifi common misidentifi bird speci teach inexperienc birder differ speci paper develop latent variabl graphic model identifi group bird speci often confus ebird particip model multispeci extens classic occupancydetect model ecolog literatur multispeci extens nontrivi requir structur learn step well comput expens paramet learn stage make effici variat approxim show includ speci misidentif model discov misidentif predict speci occup detect also accur probabilist graphic model crowdsourc citizen scienc ecolog,6,3.3363073,-0.11001381
Cross-View Feature Learning for Scalable Social Image Analysis,"Wenxuan Xie, Yuxin Peng and Jianguo Xiao",AI and the Web (AIW),"Cross-View Learning
Feature Learning
Random Projection","AIW: AI for multimedia and multimodal web applications
AIW: Enhancing web search and information retrieval
AIW: Machine learning and the web","Nowadays images on social networking websites (e.g., Flickr) are mostly accompanied with user-contributed tags, which help cast a new light on the conventional content-based image analysis tasks such as image classification and retrieval. In order to establish a scalable social image analysis system, two issues need to be considered: 1) Supervised learning is a futile task in modeling the enormous number of concepts in the world, whereas unsupervised approaches overcome this hurdle; 2) Algorithms are required to be both spatially and temporally efficient to handle large-scale datasets. In this paper, we propose a cross-view feature learning (CVFL) framework to handle the problem of social image analysis effectively and efficiently. Through explicitly modeling the relevance between image content and tags (which is empirically shown to be visually and semantically meaningful), CVFL yields more promising results than existing methods in the experiments. More importantly, being general and descriptive, CVFL and its variants can be readily applied to other large-scale multi-view tasks in unsupervised setting.","Cross-View Feature Learning for Scalable Social Image Analysis Nowadays images on social networking websites (e.g., Flickr) are mostly accompanied with user-contributed tags, which help cast a new light on the conventional content-based image analysis tasks such as image classification and retrieval. In order to establish a scalable social image analysis system, two issues need to be considered: 1) Supervised learning is a futile task in modeling the enormous number of concepts in the world, whereas unsupervised approaches overcome this hurdle; 2) Algorithms are required to be both spatially and temporally efficient to handle large-scale datasets. In this paper, we propose a cross-view feature learning (CVFL) framework to handle the problem of social image analysis effectively and efficiently. Through explicitly modeling the relevance between image content and tags (which is empirically shown to be visually and semantically meaningful), CVFL yields more promising results than existing methods in the experiments. More importantly, being general and descriptive, CVFL and its variants can be readily applied to other large-scale multi-view tasks in unsupervised setting. Cross-View Learning
Feature Learning
Random Projection",crossview featur learn scalabl social imag analysi nowaday imag social network websit eg flickr most accompani usercontribut tag help cast new light convent contentbas imag analysi task imag classif retriev order establish scalabl social imag analysi system two issu need consid 1 supervis learn futil task model enorm number concept world wherea unsupervis approach overcom hurdl 2 algorithm requir spatial tempor effici handl largescal dataset paper propos crossview featur learn cvfl framework handl problem social imag analysi effect effici explicit model relev imag content tag empir shown visual semant meaning cvfl yield promis result exist method experi import general descript cvfl variant readili appli largescal multiview task unsupervis set crossview learn featur learn random project,1,-23.172932,-6.280497
Semantic Graph Construction for Weakly-Supervised Image Parsing,"Wenxuan Xie, Yuxin Peng and Jianguo Xiao",Vision (VIS),"Weakly-Supervised Learning
Image Parsing
Graph Construction","VIS: Categorization
VIS: Object Recognition","We investigate weakly-supervised image parsing, i.e., assigning class labels to image regions by using image-level labels only. Existing studies pay main attention to the formulation of the weakly-supervised learning problem, i.e., how to propagate class labels from images to regions given an affinity graph of regions. Notably, however, the affinity graph of regions, which is generally constructed in relatively simpler settings in existing methods, is of crucial importance to the parsing performance due to the fact that the weakly-supervised parsing problem cannot be solved within a single image, and that the affinity graph enables label propagation among multiple images. In order to embed more semantics into the affinity graph, we propose novel criteria by exploiting the weak supervision information carefully, and develop two graphs: L1 semantic graph and k-NN semantic graph. Experimental results demonstrate that the proposed semantic graphs not only capture more semantic relevance, but also perform significantly better than conventional graphs in image parsing.","Semantic Graph Construction for Weakly-Supervised Image Parsing We investigate weakly-supervised image parsing, i.e., assigning class labels to image regions by using image-level labels only. Existing studies pay main attention to the formulation of the weakly-supervised learning problem, i.e., how to propagate class labels from images to regions given an affinity graph of regions. Notably, however, the affinity graph of regions, which is generally constructed in relatively simpler settings in existing methods, is of crucial importance to the parsing performance due to the fact that the weakly-supervised parsing problem cannot be solved within a single image, and that the affinity graph enables label propagation among multiple images. In order to embed more semantics into the affinity graph, we propose novel criteria by exploiting the weak supervision information carefully, and develop two graphs: L1 semantic graph and k-NN semantic graph. Experimental results demonstrate that the proposed semantic graphs not only capture more semantic relevance, but also perform significantly better than conventional graphs in image parsing. Weakly-Supervised Learning
Image Parsing
Graph Construction",semant graph construct weaklysupervis imag pars investig weaklysupervis imag pars ie assign class label imag region use imagelevel label exist studi pay main attent formul weaklysupervis learn problem ie propag class label imag region given affin graph region notabl howev affin graph region general construct relat simpler set exist method crucial import pars perform due fact weaklysupervis pars problem cannot solv within singl imag affin graph enabl label propag among multipl imag order emb semant affin graph propos novel criteria exploit weak supervis inform care develop two graph l1 semant graph knn semant graph experiment result demonstr propos semant graph captur semant relev also perform signific better convent graph imag pars weaklysupervis learn imag pars graph construct,1,-9.433164,10.488644
The Importance of Cognition and Affect for Artificially Intelligent Decision Makers,"Celso de Melo, Jonathan Gratch and Peter Carnevale","Cognitive Modeling (CM)
Cognitive Systems (CS)
Humans and AI (HAI)","Mind perception
cognition
affect
Decision making
cooperation","CM: Simulating Humans
CS: Problem solving and decision making
HAI: Human-Computer Interaction
HAI: Understanding People, Theories, Concepts and Methods","Agency – the capacity to plan and act – and experience – the capacity to sense and feel – are two critical aspects that determine whether people will perceive non-human entities, such as autonomous agents, to have a mind. There is evidence that the absence of either can reduce cooperation. We present an experiment that tests the necessity of both for cooperation with agents. In this experiment we manipulated people’s perceptions about the cognitive and affective abilities of agents, when engaging in the ultimatum game. The results indicated that people offered more money to agents that were perceived to make decisions according to their intentions, rather than randomly. Additionally, the results showed that people offered more money to agents that expressed emotion, when compared to agents that did not. We discuss the implications of this agency-experience theoretical framework for the design of artificially intelligent decision makers.","The Importance of Cognition and Affect for Artificially Intelligent Decision Makers Agency – the capacity to plan and act – and experience – the capacity to sense and feel – are two critical aspects that determine whether people will perceive non-human entities, such as autonomous agents, to have a mind. There is evidence that the absence of either can reduce cooperation. We present an experiment that tests the necessity of both for cooperation with agents. In this experiment we manipulated people’s perceptions about the cognitive and affective abilities of agents, when engaging in the ultimatum game. The results indicated that people offered more money to agents that were perceived to make decisions according to their intentions, rather than randomly. Additionally, the results showed that people offered more money to agents that expressed emotion, when compared to agents that did not. We discuss the implications of this agency-experience theoretical framework for the design of artificially intelligent decision makers. Mind perception
cognition
affect
Decision making
cooperation",import cognit affect artifici intellig decis maker agenc – capac plan act – experi – capac sens feel – two critic aspect determin whether peopl perceiv nonhuman entiti autonom agent mind evid absenc either reduc cooper present experi test necess cooper agent experi manipul peopl percept cognit affect abil agent engag ultimatum game result indic peopl offer money agent perceiv make decis accord intent rather random addit result show peopl offer money agent express emot compar agent discuss implic agencyexperi theoret framework design artifici intellig decis maker mind percept cognit affect decis make cooper,9,5.1831293,13.91926
The Complexity of Reasoning with FODD and GFODD,Benjamin Hescott and Roni Khardon,Knowledge Representation and Reasoning (KRR),"Decision Diagrams
Computational Complexity
First Order Logic","KRR: Automated Reasoning and Theorem Proving
KRR: Computational Complexity of Reasoning
KRR: Knowledge Representation Languages","Recent work introduced Generalized First Order Decision Diagrams (GFODD) as a knowledge representation that is useful in mechanizing decision theoretic planning in relational domains. GFODDs generalize function-free first order logic and include numerical values and numerical generalizations of existential and universal quantification. Previous work presented heuristic inference algorithms for GFODDs. In this paper, we study the complexity of the evaluation problem, the satiability problem, and the equivalence problem for GFODDs under the assumption that the size of the intended model is given with the problem, a restriction that guarantees decidability. Our results provide a complete characterization. The same characterization applies to the corresponding restriction of problems in first order logic, giving an interesting new avenue for efficient inference when the number of objects is bounded. Our results show that for $\Sigma_k$ formulas, and for corresponding GFODDs, evaluation and satisfiability are $\Sigma_k^p$ complete, and equivalence is $\Pi_{k+1}^p$ complete. For $\Pi_k$ formulas evaluation is $\Pi_k^p$ complete, satisfiability is one level higher and is $\Sigma_{k+1}^p$ complete, and equivalence is $\Pi_{k+1}^p$ complete.","The Complexity of Reasoning with FODD and GFODD Recent work introduced Generalized First Order Decision Diagrams (GFODD) as a knowledge representation that is useful in mechanizing decision theoretic planning in relational domains. GFODDs generalize function-free first order logic and include numerical values and numerical generalizations of existential and universal quantification. Previous work presented heuristic inference algorithms for GFODDs. In this paper, we study the complexity of the evaluation problem, the satiability problem, and the equivalence problem for GFODDs under the assumption that the size of the intended model is given with the problem, a restriction that guarantees decidability. Our results provide a complete characterization. The same characterization applies to the corresponding restriction of problems in first order logic, giving an interesting new avenue for efficient inference when the number of objects is bounded. Our results show that for $\Sigma_k$ formulas, and for corresponding GFODDs, evaluation and satisfiability are $\Sigma_k^p$ complete, and equivalence is $\Pi_{k+1}^p$ complete. For $\Pi_k$ formulas evaluation is $\Pi_k^p$ complete, satisfiability is one level higher and is $\Sigma_{k+1}^p$ complete, and equivalence is $\Pi_{k+1}^p$ complete. Decision Diagrams
Computational Complexity
First Order Logic",complex reason fodd gfodd recent work introduc general first order decis diagram gfodd knowledg represent use mechan decis theoret plan relat domain gfodd general functionfre first order logic includ numer valu numer general existenti univers quantif previous work present heurist infer algorithm gfodd paper studi complex evalu problem satiabl problem equival problem gfodd assumpt size intend model given problem restrict guarante decid result provid complet character character appli correspond restrict problem first order logic give interest new avenu effici infer number object bound result show sigmak formula correspond gfodd evalu satisfi sigmakp complet equival pik1p complet pik formula evalu pikp complet satisfi one level higher sigmak1p complet equival pik1p complet decis diagram comput complex first order logic,5,0.5355117,-3.220045
Scalable Complex Contract Negotiation With Structured Search and Agenda Management,"Xiaoqin Zhang, Mark Klein and Ivan Marsa Maestre",Multiagent Systems (MAS),"Large-scale Negotiation
Interdependent Issues
Complex Contracts
agenda management","MAS: Distributed Problem Solving
MAS: Mechanism Design
MAS: Multiagent Systems (General/other)
SCS: Distributed CSP/Optimization","A large number of interdependent issues in complex contract poses a challenge for current negotiation approaches, which becomes even more apparent when negotiation problems scale up. To address this challenge, we present a structured anytime search process with agenda management mechanism using a hierarchical negotiation model, where agents search at various levels during the negotiation with the guidance from a mediator agent. This structured negotiation process increases computational efficiency, making negotiations scalable for large number of interdependent issues. To validate the contributions of our approach, 1) we developed anytime tree search negotiation process with an agenda management mechanism using a hierarchical problem structure and constraint-based preference model for real-world applications; 2) we defined a scenario matrix to capture various characteristics of negotiation scenarios and developed a scenario generator that produces testing cases according to this matrix; and 3) we performed an extensive set of experiments to study the performance of this structured negotiation protocol and the influence of different scenario parameters, and investigated the Pareto efficiency and social welfare optimality of the negotiation outcomes.","Scalable Complex Contract Negotiation With Structured Search and Agenda Management A large number of interdependent issues in complex contract poses a challenge for current negotiation approaches, which becomes even more apparent when negotiation problems scale up. To address this challenge, we present a structured anytime search process with agenda management mechanism using a hierarchical negotiation model, where agents search at various levels during the negotiation with the guidance from a mediator agent. This structured negotiation process increases computational efficiency, making negotiations scalable for large number of interdependent issues. To validate the contributions of our approach, 1) we developed anytime tree search negotiation process with an agenda management mechanism using a hierarchical problem structure and constraint-based preference model for real-world applications; 2) we defined a scenario matrix to capture various characteristics of negotiation scenarios and developed a scenario generator that produces testing cases according to this matrix; and 3) we performed an extensive set of experiments to study the performance of this structured negotiation protocol and the influence of different scenario parameters, and investigated the Pareto efficiency and social welfare optimality of the negotiation outcomes. Large-scale Negotiation
Interdependent Issues
Complex Contracts
agenda management",scalabl complex contract negoti structur search agenda manag larg number interdepend issu complex contract pose challeng current negoti approach becom even appar negoti problem scale address challeng present structur anytim search process agenda manag mechan use hierarch negoti model agent search various level negoti guidanc mediat agent structur negoti process increas comput effici make negoti scalabl larg number interdepend issu valid contribut approach 1 develop anytim tree search negoti process agenda manag mechan use hierarch problem structur constraintbas prefer model realworld applic 2 defin scenario matrix captur various characterist negoti scenario develop scenario generat produc test case accord matrix 3 perform extens set experi studi perform structur negoti protocol influenc differ scenario paramet investig pareto effici social welfar optim negoti outcom largescal negoti interdepend issu complex contract agenda manag,1,2.402848,2.1295133
Manifold Learning for Jointly Modeling Topic and Visualization,Tuan Le and Hady Lauw,Novel Machine Learning Algorithms (NMLA),"document visualization
dimensionality reduction
topic model
manifold learning","NMLA: Data Mining and Knowledge Discovery
NMLA: Dimension Reduction/Feature Selection
NMLA: Graphical Model Learning
NMLA: Unsupervised Learning (Other)","Classical approaches to visualization directly reduce a document's high-dimensional representation into visualizable two or three dimensions, using techniques such as multidimensional scaling.  More recent approaches consider an intermediate representation in topic space, between word space and visualization space, which preserves the semantics by topic modeling.  We call the latter semantic visualization problem, as it seeks to jointly model topic and visualization. While previous approaches aim to preserve the global consistency, they do not consider the local consistency in terms of the intrinsic geometric structure of the document manifold.  We therefore propose an unsupervised probabilistic model, called Semafore, which aims to preserve the manifold in the lower-dimensional spaces.  Comprehensive experiments on several real-life text datasets of news articles and web pages show that Semafore significantly outperforms the state-of-the-art baselines on objective evaluation metrics.","Manifold Learning for Jointly Modeling Topic and Visualization Classical approaches to visualization directly reduce a document's high-dimensional representation into visualizable two or three dimensions, using techniques such as multidimensional scaling.  More recent approaches consider an intermediate representation in topic space, between word space and visualization space, which preserves the semantics by topic modeling.  We call the latter semantic visualization problem, as it seeks to jointly model topic and visualization. While previous approaches aim to preserve the global consistency, they do not consider the local consistency in terms of the intrinsic geometric structure of the document manifold.  We therefore propose an unsupervised probabilistic model, called Semafore, which aims to preserve the manifold in the lower-dimensional spaces.  Comprehensive experiments on several real-life text datasets of news articles and web pages show that Semafore significantly outperforms the state-of-the-art baselines on objective evaluation metrics. document visualization
dimensionality reduction
topic model
manifold learning",manifold learn joint model topic visual classic approach visual direct reduc document highdimension represent visualiz two three dimens use techniqu multidimension scale recent approach consid intermedi represent topic space word space visual space preserv semant topic model call latter semant visual problem seek joint model topic visual previous approach aim preserv global consist consid local consist term intrins geometr structur document manifold therefor propos unsupervis probabilist model call semafor aim preserv manifold lowerdimension space comprehens experi sever reallif text dataset news articl web page show semafor signific outperform stateoftheart baselin object evalu metric document visual dimension reduct topic model manifold learn,1,7.3110228,-8.258834
Constructing Symbolic Representations for High-Level Planning,"George Konidaris, Leslie Kaelbling and Tomas Lozano-Perez",Novel Machine Learning Algorithms (NMLA),"Reinforcement learning
Planning
Representation","NMLA: Reinforcement Learning
PS: Learning Models for Planning and Diagnosis
ROB: Cognitive Robotics","We consider the problem of constructing a symbolic description of a continuous, low-level environment for use in planning. We show that  symbols that can represent the preconditions and effects of an agent's actions are both necessary and sufficient for high-level planning.  This enables reinforcement learning agents to acquire their own symbolic representations autonomously, and eliminates the symbol design problem when a representation must be constructed in advance. The resulting representation can be converted into PDDL, a canonical planning representation that enables very fast planning.","Constructing Symbolic Representations for High-Level Planning We consider the problem of constructing a symbolic description of a continuous, low-level environment for use in planning. We show that  symbols that can represent the preconditions and effects of an agent's actions are both necessary and sufficient for high-level planning.  This enables reinforcement learning agents to acquire their own symbolic representations autonomously, and eliminates the symbol design problem when a representation must be constructed in advance. The resulting representation can be converted into PDDL, a canonical planning representation that enables very fast planning. Reinforcement learning
Planning
Representation",construct symbol represent highlevel plan consid problem construct symbol descript continu lowlevel environ use plan show symbol repres precondit effect agent action necessari suffici highlevel plan enabl reinforc learn agent acquir symbol represent autonom elimin symbol design problem represent must construct advanc result represent convert pddl canon plan represent enabl fast plan reinforc learn plan represent,3,-1.7737733,16.810488
Towards Understanding Unscripted Gesture and Language Input for Human-Robot Interactions,"Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer and Dieter Fox","Humans and AI (HAI)
NLP and Machine Learning (NLPML)
Robotics (ROB)","Human-Robot Interaction
Robotics
Natural Language Processing
ML classifier features","APP: Other Applications
HAI: Language Acquisition
MLA: Machine Learning Applications (General/other)
NLPML: Natural Language Processing (General/Other)
NMLA: Time-Series/Data Streams
ROB: Human-Robot Interaction","As robots become more ubiquitous, it is increasingly important for untrained users to be able to interact with them intuitively. In this work, we investigate how people refer to objects in the world during relatively unstructured communication with robots. We collect a corpus of interactions from users describing objects, which we use to train language and gesture models that allow our robot to determine what objects are being indicated. We introduce a temporal extension to state-of-the-art hierarchical matching pursuit features to support gesture understanding, and demonstrate that combining multiple communication modalities more effectively captures user intent than relying on a single type of input. Finally, we present initial interactions with a robot that uses the learned models to follow commands while continuing to learn from user input.","Towards Understanding Unscripted Gesture and Language Input for Human-Robot Interactions As robots become more ubiquitous, it is increasingly important for untrained users to be able to interact with them intuitively. In this work, we investigate how people refer to objects in the world during relatively unstructured communication with robots. We collect a corpus of interactions from users describing objects, which we use to train language and gesture models that allow our robot to determine what objects are being indicated. We introduce a temporal extension to state-of-the-art hierarchical matching pursuit features to support gesture understanding, and demonstrate that combining multiple communication modalities more effectively captures user intent than relying on a single type of input. Finally, we present initial interactions with a robot that uses the learned models to follow commands while continuing to learn from user input. Human-Robot Interaction
Robotics
Natural Language Processing
ML classifier features",toward understand unscript gestur languag input humanrobot interact robot becom ubiquit increas import untrain user abl interact intuit work investig peopl refer object world relat unstructur communic robot collect corpus interact user describ object use train languag gestur model allow robot determin object indic introduc tempor extens stateoftheart hierarch match pursuit featur support gestur understand demonstr combin multipl communic modal effect captur user intent reli singl type input final present initi interact robot use learn model follow command continu learn user input humanrobot interact robot natur languag process ml classifi featur,0,-1.8180212,9.459737
Intelligent System for Urban Emergency Management During Large-scale Disaster,"Xuan Song, Quanshi Zhang and Ryosuke Shibasaki",Computational Sustainability and AI (CSAI),"Emergency Management
Disaster Informatics
Human Mobility",CSAI: Modeling and prediction of dynamic and spatiotemporal phenomena and systems,"The frequency and intensity of natural disasters has significantly increased over the past decades and this trend is predicted to continue. Facing these unexpected disasters, urban emergency management has become the especially important issue for the whole governments around the world. In this paper, we present a novel intelligent system for urban emergency management during the large-scale disasters. The proposed system stores and manages the global positioning system (GPS) records from mobile devices used by approximately 1.6 million people throughout Japan from 1 August 2010 to 31 July 2011. By mining and analyzing population movements after the Great East Japan Earthquake, our system is able to automatically learn a probabilistic model to better understand and simulate human mobility during the emergency situations. Based on the learning model, population mobility in various urban areas impacted by the earthquake throughout Japan is able to be automatically simulated or predicted. On the basis of such kind of system, it is easy for us to find some new features or population mobility patterns after the recent and unprecedented composite disasters, which are likely to provide the valuable experiences and play a vital role for future disaster management worldwide.","Intelligent System for Urban Emergency Management During Large-scale Disaster The frequency and intensity of natural disasters has significantly increased over the past decades and this trend is predicted to continue. Facing these unexpected disasters, urban emergency management has become the especially important issue for the whole governments around the world. In this paper, we present a novel intelligent system for urban emergency management during the large-scale disasters. The proposed system stores and manages the global positioning system (GPS) records from mobile devices used by approximately 1.6 million people throughout Japan from 1 August 2010 to 31 July 2011. By mining and analyzing population movements after the Great East Japan Earthquake, our system is able to automatically learn a probabilistic model to better understand and simulate human mobility during the emergency situations. Based on the learning model, population mobility in various urban areas impacted by the earthquake throughout Japan is able to be automatically simulated or predicted. On the basis of such kind of system, it is easy for us to find some new features or population mobility patterns after the recent and unprecedented composite disasters, which are likely to provide the valuable experiences and play a vital role for future disaster management worldwide. Emergency Management
Disaster Informatics
Human Mobility",intellig system urban emerg manag largescal disast frequenc intens natur disast signific increas past decad trend predict continu face unexpect disast urban emerg manag becom especi import issu whole govern around world paper present novel intellig system urban emerg manag largescal disast propos system store manag global posit system gps record mobil devic use approxim 16 million peopl throughout japan 1 august 2010 31 juli 2011 mine analyz popul movement great east japan earthquak system abl automat learn probabilist model better understand simul human mobil emerg situat base learn model popul mobil various urban area impact earthquak throughout japan abl automat simul predict basi kind system easi us find new featur popul mobil pattern recent unpreced composit disast like provid valuabl experi play vital role futur disast manag worldwid emerg manag disast informat human mobil,0,3.3628716,1.6364243
Cached Iterative Weakening for Optimal Multi-Way Number Partitioning,Ethan Schreiber and Richard Korf,"Heuristic Search and Optimization (HSO)
Planning and Scheduling (PS)
Search and Constraint Satisfaction (SCS)","Heuristic Search
Optimization
Search
Scheduling
Constraint Optimization","HSO: Heuristic Search
HSO: Optimization
HSO: Search (General/Other)
PS: Scheduling
SCS: Constraint Satisfaction","The NP-hard number-partitioning problem is to separate a multiset
  S of n positive integers into k subsets, such that the largest
  sum of the integers assigned to any subset is minimized. The classic
  application is scheduling a set of n jobs with different run times
  onto k identical machines such that the makespan, the time to
  complete the schedule, is minimized. We present a new algorithm,
  cached iterative weakening (CIW), for solving this problem
  optimally. It incorporates three ideas distinct from the previous
  state of the art: it explores the search space using iterative
  weakening instead of branch and bound; generates feasible subsets
  once and caches them instead of at each node of the search tree; and
  explores subsets in cardinality order instead of an arbitrary
  order. The previous state of the art is represented by three different
  algorithms depending on the values of n and k. We provide one
  algorithm which outperforms all previous algorithms for k >=
  4. Our run times are up to two orders of magnitude faster.","Cached Iterative Weakening for Optimal Multi-Way Number Partitioning The NP-hard number-partitioning problem is to separate a multiset
  S of n positive integers into k subsets, such that the largest
  sum of the integers assigned to any subset is minimized. The classic
  application is scheduling a set of n jobs with different run times
  onto k identical machines such that the makespan, the time to
  complete the schedule, is minimized. We present a new algorithm,
  cached iterative weakening (CIW), for solving this problem
  optimally. It incorporates three ideas distinct from the previous
  state of the art: it explores the search space using iterative
  weakening instead of branch and bound; generates feasible subsets
  once and caches them instead of at each node of the search tree; and
  explores subsets in cardinality order instead of an arbitrary
  order. The previous state of the art is represented by three different
  algorithms depending on the values of n and k. We provide one
  algorithm which outperforms all previous algorithms for k >=
  4. Our run times are up to two orders of magnitude faster. Heuristic Search
Optimization
Search
Scheduling
Constraint Optimization",cach iter weaken optim multiway number partit nphard numberpartit problem separ multiset n posit integ k subset largest sum integ assign subset minim classic applic schedul set n job differ run time onto k ident machin makespan time complet schedul minim present new algorithm cach iter weaken ciw solv problem optim incorpor three idea distinct previous state art explor search space use iter weaken instead branch bound generat feasibl subset cach instead node search tree explor subset cardin order instead arbitrari order previous state art repres three differ algorithm depend valu n k provid one algorithm outperform previous algorithm k 4 run time two order magnitud faster heurist search optim search schedul constraint optim,4,-12.799334,6.981708
Online Social Spammer Detection,"Xia Hu, Jiliang Tang and Huan Liu","AI and the Web (AIW)
Machine Learning Applications (MLA)","Social Media
Social Spammer
Online Learning","AIW: Machine learning and the web
AIW: Recognizing web spam such as link farms and splogs
AIW: Web personalization and user modeling
MLA: Machine Learning Applications (General/other)
NMLA: Data Mining and Knowledge Discovery
NMLA: Online Learning","The explosive use of social media also makes it a popular platform for malicious users, known as social spammers, to overwhelm normal users with unwanted content. One effective way for social spammer detection is to build a classifier based on content and social network information. However, social spammers are sophisticated and adaptable to game the system with fast evolving content and network patterns. First, social spammers continually change their spamming content patterns to avoid being detected.  Second, reflexive reciprocity makes it easier for social spammers to establish social influence and pretend to be normal users by quickly accumulating a large number of ``human"" friends. It is challenging for existing anti-spamming systems based on batch-mode learning to quickly respond to newly emerging patterns for effective social spammer detection. In this paper, we present a general optimization framework to collectively use content and network information for social spammer detection, and provide the solution for efficient online processing. Experimental results on Twitter datasets confirm the  effectiveness and efficiency of the proposed framework.","Online Social Spammer Detection The explosive use of social media also makes it a popular platform for malicious users, known as social spammers, to overwhelm normal users with unwanted content. One effective way for social spammer detection is to build a classifier based on content and social network information. However, social spammers are sophisticated and adaptable to game the system with fast evolving content and network patterns. First, social spammers continually change their spamming content patterns to avoid being detected.  Second, reflexive reciprocity makes it easier for social spammers to establish social influence and pretend to be normal users by quickly accumulating a large number of ``human"" friends. It is challenging for existing anti-spamming systems based on batch-mode learning to quickly respond to newly emerging patterns for effective social spammer detection. In this paper, we present a general optimization framework to collectively use content and network information for social spammer detection, and provide the solution for efficient online processing. Experimental results on Twitter datasets confirm the  effectiveness and efficiency of the proposed framework. Social Media
Social Spammer
Online Learning",onlin social spammer detect explos use social media also make popular platform malici user known social spammer overwhelm normal user unwant content one effect way social spammer detect build classifi base content social network inform howev social spammer sophist adapt game system fast evolv content network pattern first social spammer continu chang spam content pattern avoid detect second reflex reciproc make easier social spammer establish social influenc pretend normal user quick accumul larg number human friend challeng exist antispam system base batchmod learn quick respond newli emerg pattern effect social spammer detect paper present general optim framework collect use content network inform social spammer detect provid solut effici onlin process experiment result twitter dataset confirm effect effici propos framework social media social spammer onlin learn,0,13.730706,5.974893
Modeling and Predicting Popularity Dynamics via Reinforced Poisson Process,"Huawei Shen, Dashun Wang, Chaoming Song and Albert Laszlo Barabasi",Applications (APP),"Social Dynamics
Poisson Process
Popularity Prediction","APP: Computational Social Science
APP: Social Networks",An ability to predict the popularity dynamics of individual items within a complex evolving system has important implications in an array of areas. Here we propose a generative probabilistic framework using a reinforced Poisson process to model explicitly the process through which individual items gain their popularity. This model distinguishes itself from existing models via its capability of modeling the arrival process of popularity and its remarkable power at predicting the popularity of individual items. It possesses the flexibility of applying Bayesian treatment to further improve the predictive power using a conjugate prior. Extensive experiments on a longitudinal citation dataset demonstrate that this model consistently outperforms existing popularity prediction methods,"Modeling and Predicting Popularity Dynamics via Reinforced Poisson Process An ability to predict the popularity dynamics of individual items within a complex evolving system has important implications in an array of areas. Here we propose a generative probabilistic framework using a reinforced Poisson process to model explicitly the process through which individual items gain their popularity. This model distinguishes itself from existing models via its capability of modeling the arrival process of popularity and its remarkable power at predicting the popularity of individual items. It possesses the flexibility of applying Bayesian treatment to further improve the predictive power using a conjugate prior. Extensive experiments on a longitudinal citation dataset demonstrate that this model consistently outperforms existing popularity prediction methods Social Dynamics
Poisson Process
Popularity Prediction",model predict popular dynam via reinforc poisson process abil predict popular dynam individu item within complex evolv system import implic array area propos generat probabilist framework use reinforc poisson process model explicit process individu item gain popular model distinguish exist model via capabl model arriv process popular remark power predict popular individu item possess flexibl appli bayesian treatment improv predict power use conjug prior extens experi longitudin citat dataset demonstr model consist outperform exist popular predict method social dynam poisson process popular predict,0,5.424304,1.8944741
The Computational Rise and Fall of Fairness,"John Dickerson, Jonathan Goldman, Jeremy Karp, Ariel Procaccia and Tuomas Sandholm",Game Theory and Economic Paradigms (GTEP),"Fair division
Computational social choice
Envy-free allocation
Phase transition",GTEP: Social Choice / Voting,"The fair division of indivisible goods has long been an important topic in economics and, more recently, computer science.  We investigate the existence of envy-free allocations of indivisible goods, that is, allocations where each player values her own allocated set of goods at least as highly as any other player's allocated set of goods.  Under additive valuations, we show that even when the number of goods is larger than the number of agents by a linear fraction, envy-free allocations are unlikely to exist.  We then show that when the number of goods is larger by a logarithmic factor, such allocations exist with high probability.  We support these results experimentally and show that the asymptotic behavior of the theory holds even when the number of goods and agents is quite small.  We demonstrate that there is a sharp phase transition from nonexistence to existence of envy-free allocations, and that on average the computational problem is hardest at that transition.","The Computational Rise and Fall of Fairness The fair division of indivisible goods has long been an important topic in economics and, more recently, computer science.  We investigate the existence of envy-free allocations of indivisible goods, that is, allocations where each player values her own allocated set of goods at least as highly as any other player's allocated set of goods.  Under additive valuations, we show that even when the number of goods is larger than the number of agents by a linear fraction, envy-free allocations are unlikely to exist.  We then show that when the number of goods is larger by a logarithmic factor, such allocations exist with high probability.  We support these results experimentally and show that the asymptotic behavior of the theory holds even when the number of goods and agents is quite small.  We demonstrate that there is a sharp phase transition from nonexistence to existence of envy-free allocations, and that on average the computational problem is hardest at that transition. Fair division
Computational social choice
Envy-free allocation
Phase transition",comput rise fall fair fair divis indivis good long import topic econom recent comput scienc investig exist envyfre alloc indivis good alloc player valu alloc set good least high player alloc set good addit valuat show even number good larger number agent linear fraction envyfre alloc unlik exist show number good larger logarithm factor alloc exist high probabl support result experiment show asymptot behavior theori hold even number good agent quit small demonstr sharp phase transit nonexist exist envyfre alloc averag comput problem hardest transit fair divis comput social choic envyfre alloc phase transit,9,-17.92594,-18.795069
Type-based Exploration  for Satisficing Planning with Multiple Search Queues,"Fan Xie, Martin Mueller and Robert Holte","Heuristic Search and Optimization (HSO)
Planning and Scheduling (PS)","Satisficing Planning
Heuristic Search
Greedy Best First Search","HSO: Heuristic Search
PS: Deterministic Planning","Utilizing multiple queues in Greedy Best-First Search (GBFS) has been proven to be a very effective approach to satisficing planning. Successful applications include extra queues based on Helpful Actions (or Preferred Operators), as well as using Multiple Heuristics. One weakness of all standard GBFS algorithms is their lack of exploration. All queues used in these methods work as priority queues sorted by heuristic values. Therefore, misleading heuristics, especially early in the search process, cause the search to become ineffective.

Type systems, as introduced for heuristic search by Lelis et al, are a recent development of ideas for exploration related to the classic stratified sampling approach. The current work introduces a search algorithm that utilizes type systems in a new way – for exploration within a GBFS multiqueue frame- work, in satisficing planning.

A careful case study shows the benefits of such exploration for overcoming deficiencies of the heuristic. The proposed new baseline algorithm Type-GBFS solves almost 200 more problems than baseline GBFS over all International Planning Competition problems. Type-LAMA, a new planner which integrates Type-GBFS into LAMA-2011, substantially improves upon LAMA in terms of both coverage and speed.","Type-based Exploration  for Satisficing Planning with Multiple Search Queues Utilizing multiple queues in Greedy Best-First Search (GBFS) has been proven to be a very effective approach to satisficing planning. Successful applications include extra queues based on Helpful Actions (or Preferred Operators), as well as using Multiple Heuristics. One weakness of all standard GBFS algorithms is their lack of exploration. All queues used in these methods work as priority queues sorted by heuristic values. Therefore, misleading heuristics, especially early in the search process, cause the search to become ineffective.

Type systems, as introduced for heuristic search by Lelis et al, are a recent development of ideas for exploration related to the classic stratified sampling approach. The current work introduces a search algorithm that utilizes type systems in a new way – for exploration within a GBFS multiqueue frame- work, in satisficing planning.

A careful case study shows the benefits of such exploration for overcoming deficiencies of the heuristic. The proposed new baseline algorithm Type-GBFS solves almost 200 more problems than baseline GBFS over all International Planning Competition problems. Type-LAMA, a new planner which integrates Type-GBFS into LAMA-2011, substantially improves upon LAMA in terms of both coverage and speed. Satisficing Planning
Heuristic Search
Greedy Best First Search",typebas explor satisf plan multipl search queue util multipl queue greedi bestfirst search gbfs proven effect approach satisf plan success applic includ extra queue base help action prefer oper well use multipl heurist one weak standard gbfs algorithm lack explor queue use method work prioriti queue sort heurist valu therefor mislead heurist especi earli search process caus search becom ineffect type system introduc heurist search leli et al recent develop idea explor relat classic stratifi sampl approach current work introduc search algorithm util type system new way – explor within gbfs multiqueu frame work satisf plan care case studi show benefit explor overcom defici heurist propos new baselin algorithm typegbf solv almost 200 problem baselin gbfs intern plan competit problem typelama new planner integr typegbf lama2011 substanti improv upon lama term coverag speed satisf plan heurist search greedi best first search,5,-14.809614,7.5694304
Exact Subspace Clustering in Linear Time,"Shusen Wang, Bojun Tu, Congfu Xu and Zhihua Zhang",Novel Machine Learning Algorithms (NMLA),"subspace clustering
data selection
scalable algorithm
robust principal component analysis",NMLA: Clustering,"Subspace clustering is an important unsupervised learning problem with wide applications in computer vision and data analysis. However, the state-of-the-art methods for this problem suffer from high time complexity---quadratic or cubic in $n$ (the number of data instances). In this paper we exploit a data selection algorithm to speedup computation and the robust principal component analysis to strengthen robustness. Accordingly, we devise a scalable and robust subspace clustering method which costs time only linear in $n$. We prove theoretically that under certain mild assumptions our method solves the subspace clustering problem exactly even for grossly corrupted data. Our algorithm is based on very simple ideas, yet it is the only linear time algorithm with noiseless or noisy recovery guarantee. Finally, empirical results verify our theoretical analysis.","Exact Subspace Clustering in Linear Time Subspace clustering is an important unsupervised learning problem with wide applications in computer vision and data analysis. However, the state-of-the-art methods for this problem suffer from high time complexity---quadratic or cubic in $n$ (the number of data instances). In this paper we exploit a data selection algorithm to speedup computation and the robust principal component analysis to strengthen robustness. Accordingly, we devise a scalable and robust subspace clustering method which costs time only linear in $n$. We prove theoretically that under certain mild assumptions our method solves the subspace clustering problem exactly even for grossly corrupted data. Our algorithm is based on very simple ideas, yet it is the only linear time algorithm with noiseless or noisy recovery guarantee. Finally, empirical results verify our theoretical analysis. subspace clustering
data selection
scalable algorithm
robust principal component analysis",exact subspac cluster linear time subspac cluster import unsupervis learn problem wide applic comput vision data analysi howev stateoftheart method problem suffer high time complexityquadrat cubic n number data instanc paper exploit data select algorithm speedup comput robust princip compon analysi strengthen robust accord devis scalabl robust subspac cluster method cost time linear n prove theoret certain mild assumpt method solv subspac cluster problem exact even grossli corrupt data algorithm base simpl idea yet linear time algorithm noiseless noisi recoveri guarante final empir result verifi theoret analysi subspac cluster data select scalabl algorithm robust princip compon analysi,7,-4.8067217,-18.060143
Robust Bayesian Inverse Reinforcement Learning with Sparse Behavior Noise,"Jiangchuan Zheng, Siyuan Liu and Lionel Ni",Novel Machine Learning Algorithms (NMLA),"Inverse reinforcement learning
Robust model
Sparse behavior noise
Variational inference",NMLA: Reinforcement Learning,"Inverse reinforcement learning (IRL) aims to recover the reward function underlying a Markov Decision Process from behaviors of experts in support of decision-making. Most recent work on IRL assumes the same level of trustworthiness of all expert behaviors, and frames IRL as a process of seeking reward function that makes those behaviors appear (near)-optimal. However, it is common in reality that noisy expert behaviors disobeying the optimal policy exist, which may degrade the IRL performance significantly. To address this issue, in this paper, we develop a robust IRL framework that can accurately estimate the reward function in the presence of behavior noise. In particular, we focus on a special type of behavior noise referred to as sparse noise due to its wide popularity in real-world behavior data. To model such noise, we introduce a novel latent variable characterizing the reliability of each expert action and use Laplace distribution as its prior. We then device an EM algorithm with a novel variational inference procedure in the E-step, which can automatically identify and remove behavior noise in reward learning. Experiments on both synthetic data and real vehicle routing data with noticeable behavior noise show significant improvement of our method over previous approaches in learning accuracy, and also demonstrate its power in de-noising behavior data.","Robust Bayesian Inverse Reinforcement Learning with Sparse Behavior Noise Inverse reinforcement learning (IRL) aims to recover the reward function underlying a Markov Decision Process from behaviors of experts in support of decision-making. Most recent work on IRL assumes the same level of trustworthiness of all expert behaviors, and frames IRL as a process of seeking reward function that makes those behaviors appear (near)-optimal. However, it is common in reality that noisy expert behaviors disobeying the optimal policy exist, which may degrade the IRL performance significantly. To address this issue, in this paper, we develop a robust IRL framework that can accurately estimate the reward function in the presence of behavior noise. In particular, we focus on a special type of behavior noise referred to as sparse noise due to its wide popularity in real-world behavior data. To model such noise, we introduce a novel latent variable characterizing the reliability of each expert action and use Laplace distribution as its prior. We then device an EM algorithm with a novel variational inference procedure in the E-step, which can automatically identify and remove behavior noise in reward learning. Experiments on both synthetic data and real vehicle routing data with noticeable behavior noise show significant improvement of our method over previous approaches in learning accuracy, and also demonstrate its power in de-noising behavior data. Inverse reinforcement learning
Robust model
Sparse behavior noise
Variational inference",robust bayesian invers reinforc learn spars behavior nois invers reinforc learn irl aim recov reward function under markov decis process behavior expert support decisionmak recent work irl assum level trustworthi expert behavior frame irl process seek reward function make behavior appear nearoptim howev common realiti noisi expert behavior disobey optim polici exist may degrad irl perform signific address issu paper develop robust irl framework accur estim reward function presenc behavior nois particular focus special type behavior nois refer spars nois due wide popular realworld behavior data model nois introduc novel latent variabl character reliabl expert action use laplac distribut prior devic em algorithm novel variat infer procedur estep automat identifi remov behavior nois reward learn experi synthet data real vehicl rout data notic behavior nois show signific improv method previous approach learn accuraci also demonstr power denois behavior data invers reinforc learn robust model spars behavior nois variat infer,7,-10.412593,-15.35396
Lazy Defenders Are Almost Optimal Against Diligent Attackers,"Avrim Blum, Nika Haghtalab and Ariel Procaccia",Game Theory and Economic Paradigms (GTEP),"Security games
Approximation
Sampling","GTEP: Game Theory
GTEP: Imperfect Information","Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender's randomized assignment of resources to targets. This assumption has been challenged by recent papers, which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance. We analytically demonstrate that in zero-sum security games, lazy defenders, who simply keep optimizing against perfectly informed attackers, are almost optimal against diligent attackers, who go to the effort of gathering a reasonable number of observations. This result implies that, in some realistic situations, limited surveillance may not need to be explicitly addressed.","Lazy Defenders Are Almost Optimal Against Diligent Attackers Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender's randomized assignment of resources to targets. This assumption has been challenged by recent papers, which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance. We analytically demonstrate that in zero-sum security games, lazy defenders, who simply keep optimizing against perfectly informed attackers, are almost optimal against diligent attackers, who go to the effort of gathering a reasonable number of observations. This result implies that, in some realistic situations, limited surveillance may not need to be explicitly addressed. Security games
Approximation
Sampling",lazi defend almost optim dilig attack work build stackelberg secur game model assum attack perfect observ defend random assign resourc target assumpt challeng recent paper design tailormad algorithm comput optim defend strategi secur game limit surveil analyt demonstr zerosum secur game lazi defend simpli keep optim perfect inform attack almost optim dilig attack go effort gather reason number observ result impli realist situat limit surveil may need explicit address secur game approxim sampl,2,3.5913024,21.890678
PREGO: An Action Language for Belief-Based Cognitive Robotics in Continuous Domains,Vaishak Belle and Hector Levesque,Knowledge Representation and Reasoning (KRR),"knowledge representation
situation calculus
cognitive robotics
reasoning about beliefs
action and change
action languages","KRR: Action, Change, and Causality
KRR: Knowledge Representation Languages
KRR: Reasoning with Beliefs
KRR: Knowledge Representation (General/Other)","Cognitive robotics is often subject to the criticism that the proposals
investigated in the literature are far removed from the kind of continuous
uncertainty and noise seen in actual real-world robotics. This paper
proposes a new language and an implemented system, called PREGO, based on
the situation calculus, that is able to reason effectively about degrees of
belief against noisy sensors and effectors in continuous domains. It
embodies the representational richness of conventional logic-based action
languages, such as context-sensitive successor state axioms, but is still
shown to be efficient using a number of empirical evaluations. We believe
that PREGO is a simple yet powerful dialect to explore real-time reactivity
and an interesting bridge between logic and probability for cognitive
robotics applications.","PREGO: An Action Language for Belief-Based Cognitive Robotics in Continuous Domains Cognitive robotics is often subject to the criticism that the proposals
investigated in the literature are far removed from the kind of continuous
uncertainty and noise seen in actual real-world robotics. This paper
proposes a new language and an implemented system, called PREGO, based on
the situation calculus, that is able to reason effectively about degrees of
belief against noisy sensors and effectors in continuous domains. It
embodies the representational richness of conventional logic-based action
languages, such as context-sensitive successor state axioms, but is still
shown to be efficient using a number of empirical evaluations. We believe
that PREGO is a simple yet powerful dialect to explore real-time reactivity
and an interesting bridge between logic and probability for cognitive
robotics applications. knowledge representation
situation calculus
cognitive robotics
reasoning about beliefs
action and change
action languages",prego action languag beliefbas cognit robot continu domain cognit robot often subject critic propos investig literatur far remov kind continu uncertainti nois seen actual realworld robot paper propos new languag implement system call prego base situat calculus abl reason effect degre belief noisi sensor effector continu domain embodi represent rich convent logicbas action languag contextsensit successor state axiom still shown effici use number empir evalu believ prego simpl yet power dialect explor realtim reactiv interest bridg logic probabl cognit robot applic knowledg represent situat calculus cognit robot reason belief action chang action languag,3,-1.1945503,10.006141
Dropout Training for Support Vector Machines,"Ning Chen, Jun Zhu, Jianfei Chen and Bo Zhang","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Dropout traning
Support Vector Machines
Data Augmentation
Feature Noising","MLA: Applications of Supervised Learning
NMLA: Classification
NMLA: Supervised Learning (Other)
NMLA: Machine Learning (General/other)","Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closed-form solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs.","Dropout Training for Support Vector Machines Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closed-form solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs. Dropout traning
Support Vector Machines
Data Augmentation
Feature Noising",dropout train support vector machin dropout featur nois scheme shown promis result control overfit artifici corrupt train data though extens theoret empir studi perform general linear model littl work done support vector machin svms one success approach supervis learn paper present dropout train linear svms deal intract expect nonsmooth hing loss corrupt distribut develop iter reweight least squar irl algorithm explor data augment techniqu algorithm iter minim expect reweight least squar problem reweight closedform solut similar idea appli develop new irl algorithm expect logist loss corrupt distribut algorithm offer insight connect differ hing loss logist loss dropout train empir result sever real dataset demonstr effect dropout train signific boost classif accuraci linear svms dropout trane support vector machin data augment featur nois,1,-3.3557947,-4.7556148
Game-theoretic Resource Allocation for Protecting Large Public Events,"Yue Yin, Bo An and Manish Jain","Applications (APP)
Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Security
Game Theory
Stackelberg Games","APP: Security and Privacy
GTEP: Game Theory
MAS: Multiagent Systems (General/other)","High profile large scale public events are attractive targets for terrorist attacks. The recent Boston Marathon bombings on April 15, 2013 have further emphasized the importance of protecting public events. The security challenge is exacerbated by the dynamic nature of such events: e.g., the impact of an attack at different locations changes over time as the Boston marathon participants and spectators move along the race track. In addition, the defender can relocate security resources among potential attack targets at any time and the attacker may act at any time during the event.

This paper focuses on developing efficient patrolling algorithms for such dynamic domains with continuous strategy spaces for both the defender and the attacker. We aim at computing optimal pure defender strategies, since an attacker does not have an opportunity to learn and respond to mixed strategies due to the relative infrequency of such events. We propose SCOUT-A, which makes assumptions on relocation cost, exploits payoff representation and computes optimal solutions efficiently. We also propose SCOUT-C to compute the exact optimal defender strategy for general cases despite the continuous strategy spaces. SCOUT-C computes the optimal defender strategy by constructing an equivalent game with discrete defender strategy space, then solving the constructed game. Experimental results show that both SCOUT-A and SCOUT-C significantly outperform other existing strategies.","Game-theoretic Resource Allocation for Protecting Large Public Events High profile large scale public events are attractive targets for terrorist attacks. The recent Boston Marathon bombings on April 15, 2013 have further emphasized the importance of protecting public events. The security challenge is exacerbated by the dynamic nature of such events: e.g., the impact of an attack at different locations changes over time as the Boston marathon participants and spectators move along the race track. In addition, the defender can relocate security resources among potential attack targets at any time and the attacker may act at any time during the event.

This paper focuses on developing efficient patrolling algorithms for such dynamic domains with continuous strategy spaces for both the defender and the attacker. We aim at computing optimal pure defender strategies, since an attacker does not have an opportunity to learn and respond to mixed strategies due to the relative infrequency of such events. We propose SCOUT-A, which makes assumptions on relocation cost, exploits payoff representation and computes optimal solutions efficiently. We also propose SCOUT-C to compute the exact optimal defender strategy for general cases despite the continuous strategy spaces. SCOUT-C computes the optimal defender strategy by constructing an equivalent game with discrete defender strategy space, then solving the constructed game. Experimental results show that both SCOUT-A and SCOUT-C significantly outperform other existing strategies. Security
Game Theory
Stackelberg Games",gametheoret resourc alloc protect larg public event high profil larg scale public event attract target terrorist attack recent boston marathon bomb april 15 2013 emphas import protect public event secur challeng exacerb dynam natur event eg impact attack differ locat chang time boston marathon particip spectat move along race track addit defend reloc secur resourc among potenti attack target time attack may act time event paper focus develop effici patrol algorithm dynam domain continu strategi space defend attack aim comput optim pure defend strategi sinc attack opportun learn respond mix strategi due relat infrequ event propos scouta make assumpt reloc cost exploit payoff represent comput optim solut effici also propos scoutc comput exact optim defend strategi general case despit continu strategi space scoutc comput optim defend strategi construct equival game discret defend strategi space solv construct game experiment result show scouta scoutc signific outperform exist strategi secur game theori stackelberg game,2,3.3773143,22.071499
Quality-based Learning for Web Data Classification,Ou Wu,"AI and the Web (AIW)
Machine Learning Applications (MLA)","Information quality
Multi-task learning
Web data classification","AIW: Machine learning and the web
MLA: Applications of Supervised Learning","The types of web data vary in terms of information quantity and quality. For example, some pages contain numerous texts, whereas some others contain few texts; some web videos are in high resolution, whereas some other web videos are in low resolution. As a consequence, the quality of extracted features from different web data may also vary greatly. Existing learning algorithms on web data classification usually ignore the variations of information quality or quantity. In this paper, the information quantity and quality of web data are described by quality-related factors such as text length and image quantity, and a new learning method is proposed to train classifiers based on quality-related factors. The method divides training data into subsets according to the clustering results of quality-related factors and then trains classifiers by using a multi-task learning strategy for each subset. Experimental results indicate that the quality-related factors are useful in web data classification, and the proposed method outperforms conventional algorithms that do not consider information quantity and quality.","Quality-based Learning for Web Data Classification The types of web data vary in terms of information quantity and quality. For example, some pages contain numerous texts, whereas some others contain few texts; some web videos are in high resolution, whereas some other web videos are in low resolution. As a consequence, the quality of extracted features from different web data may also vary greatly. Existing learning algorithms on web data classification usually ignore the variations of information quality or quantity. In this paper, the information quantity and quality of web data are described by quality-related factors such as text length and image quantity, and a new learning method is proposed to train classifiers based on quality-related factors. The method divides training data into subsets according to the clustering results of quality-related factors and then trains classifiers by using a multi-task learning strategy for each subset. Experimental results indicate that the quality-related factors are useful in web data classification, and the proposed method outperforms conventional algorithms that do not consider information quantity and quality. Information quality
Multi-task learning
Web data classification",qualitybas learn web data classif type web data vari term inform quantiti qualiti exampl page contain numer text wherea other contain text web video high resolut wherea web video low resolut consequ qualiti extract featur differ web data may also vari great exist learn algorithm web data classif usual ignor variat inform qualiti quantiti paper inform quantiti qualiti web data describ qualityrel factor text length imag quantiti new learn method propos train classifi base qualityrel factor method divid train data subset accord cluster result qualityrel factor train classifi use multitask learn strategi subset experiment result indic qualityrel factor use web data classif propos method outperform convent algorithm consid inform quantiti qualiti inform qualiti multitask learn web data classif,6,-8.576545,-4.853252
A Strategy-Proof Online Auction with Time Discounting Values,"Fan Wu, Junming Liu, Zhenzhe Zheng and Guihai Chen",Game Theory and Economic Paradigms (GTEP),"Online Auction
Mechanism Design
Game Theory",GTEP: Auctions and Market-Based Systems,"Online mechanism design has been widely applied to various practical applications. However, designing a strategy-proof online mechanism is much more challenging than that in a static scenario due to short of knowledge of future information. In this paper, we investigate online auctions with time discounting values, in contrast to the flat values studied in most of existing work. We present a strategy-proof 2-competitive online auction mechanism despite of time discounting values. We also implement our design and compare it with off-line optimal solution. Our numerical results show that our design achieves good performance in terms of social welfare, revenue, average winning delay, and average valuation loss.","A Strategy-Proof Online Auction with Time Discounting Values Online mechanism design has been widely applied to various practical applications. However, designing a strategy-proof online mechanism is much more challenging than that in a static scenario due to short of knowledge of future information. In this paper, we investigate online auctions with time discounting values, in contrast to the flat values studied in most of existing work. We present a strategy-proof 2-competitive online auction mechanism despite of time discounting values. We also implement our design and compare it with off-line optimal solution. Our numerical results show that our design achieves good performance in terms of social welfare, revenue, average winning delay, and average valuation loss. Online Auction
Mechanism Design
Game Theory",strategyproof onlin auction time discount valu onlin mechan design wide appli various practic applic howev design strategyproof onlin mechan much challeng static scenario due short knowledg futur inform paper investig onlin auction time discount valu contrast flat valu studi exist work present strategyproof 2competit onlin auction mechan despit time discount valu also implement design compar offlin optim solut numer result show design achiev good perform term social welfar revenu averag win delay averag valuat loss onlin auction mechan design game theori,9,12.064175,13.453153
ReLISH: Reliable Label Inference via Smoothness Hypothesis,"Chen Gong, Dacheng Tao, Keren Fu and Jie Yang",Novel Machine Learning Algorithms (NMLA),"Semi-supervised learning
Local smoothness
Regularization","NMLA: Classification
NMLA: Semisupervised Learning","The smoothness hypothesis is critical for graph-based semi-supervised learning. This paper defines local smoothness, based on which a new algorithm, Reliable Label Inference via Smoothness Hypothesis (ReLISH), is proposed. ReLISH has produced smoother labels than some existing methods for both labeled and unlabeled examples. Theoretical analyses demonstrate good stability and generalizability of ReLISH. Using real-world datasets, our empirical analyses reveal that ReLISH is promising for both transductive and inductive tasks, when compared with representative algorithms, including Harmonic Functions, Local and Global Consistency, Constraint Metric Learning, Linear Neighborhood Propagation, and Manifold Regularization.","ReLISH: Reliable Label Inference via Smoothness Hypothesis The smoothness hypothesis is critical for graph-based semi-supervised learning. This paper defines local smoothness, based on which a new algorithm, Reliable Label Inference via Smoothness Hypothesis (ReLISH), is proposed. ReLISH has produced smoother labels than some existing methods for both labeled and unlabeled examples. Theoretical analyses demonstrate good stability and generalizability of ReLISH. Using real-world datasets, our empirical analyses reveal that ReLISH is promising for both transductive and inductive tasks, when compared with representative algorithms, including Harmonic Functions, Local and Global Consistency, Constraint Metric Learning, Linear Neighborhood Propagation, and Manifold Regularization. Semi-supervised learning
Local smoothness
Regularization",relish reliabl label infer via smooth hypothesi smooth hypothesi critic graphbas semisupervis learn paper defin local smooth base new algorithm reliabl label infer via smooth hypothesi relish propos relish produc smoother label exist method label unlabel exampl theoret analys demonstr good stabil generaliz relish use realworld dataset empir analys reveal relish promis transduct induct task compar repres algorithm includ harmon function local global consist constraint metric learn linear neighborhood propag manifold regular semisupervis learn local smooth regular,6,-8.092444,-9.99218
"Parallel Materialisation of Datalog Programs in Centralised, Main-Memory RDF Systems","Boris Motik, Yavor Nenov, Robert Piro, Ian Horrocks and Dan Olteanu","AI and the Web (AIW)
Knowledge Representation and Reasoning (KRR)","datalog
materialization
fixpoint computation
parallelism
big data","AIW: Question answering on the web
AIW: Representing, reasoning, and using provenance, trust, privacy, and security on the web
KRR: Ontologies
KRR: Automated Reasoning and Theorem Proving
KRR: Logic Programming","We present a novel approach to parallel materialisation (i.e., fixpoint computation) of datalog programs in centralised, main-memory, multi-core RDF systems. The approach comprises an algorithm that evenly distributes the workload to cores, and an RDF indexing data structure that supports efficient, 'mostly' lock-free parallel updates. Our empirical evaluation shows that our approach parallelises computation very well so, with 16 physical cores, materialisation can be up to 13.9 times faster than with just one core.","Parallel Materialisation of Datalog Programs in Centralised, Main-Memory RDF Systems We present a novel approach to parallel materialisation (i.e., fixpoint computation) of datalog programs in centralised, main-memory, multi-core RDF systems. The approach comprises an algorithm that evenly distributes the workload to cores, and an RDF indexing data structure that supports efficient, 'mostly' lock-free parallel updates. Our empirical evaluation shows that our approach parallelises computation very well so, with 16 physical cores, materialisation can be up to 13.9 times faster than with just one core. datalog
materialization
fixpoint computation
parallelism
big data",parallel materialis datalog program centralis mainmemori rdf system present novel approach parallel materialis ie fixpoint comput datalog program centralis mainmemori multicor rdf system approach compris algorithm even distribut workload core rdf index data structur support effici most lockfre parallel updat empir evalu show approach parallelis comput well 16 physic core materialis 139 time faster one core datalog materi fixpoint comput parallel big data,8,-11.176476,3.6148574
Non-linear Label Ranking for Large-scale Prediction of Long-Term User Interests,"Nemanja Djuric, Mihajlo Grbovic, Vladan Radosavljevic, Narayan Bhamidipati and Slobodan Vucetic","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Computational advertising
Label ranking
Online learning
Large-scale learning
Big data","MLA: Applications of Supervised Learning
NMLA: Big Data / Scalability
NMLA: Preferences/Ranking Learning","We consider the problem of personalization of online services from the viewpoint of display ad targeting, where we seek to find the best ad categories to be shown to each user, resulting in improved user experience and increased advertiser's revenue. We propose to address this problem as a task of ranking the ad categories by each user's preferences, and introduce a novel label ranking approach capable of efficiently learning non-linear, highly accurate models in large-scale settings. Experiments on real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-K retrieval performance, strongly suggesting the benefit of using the proposed model on large-scale ranking problems.","Non-linear Label Ranking for Large-scale Prediction of Long-Term User Interests We consider the problem of personalization of online services from the viewpoint of display ad targeting, where we seek to find the best ad categories to be shown to each user, resulting in improved user experience and increased advertiser's revenue. We propose to address this problem as a task of ranking the ad categories by each user's preferences, and introduce a novel label ranking approach capable of efficiently learning non-linear, highly accurate models in large-scale settings. Experiments on real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-K retrieval performance, strongly suggesting the benefit of using the proposed model on large-scale ranking problems. Computational advertising
Label ranking
Online learning
Large-scale learning
Big data",nonlinear label rank largescal predict longterm user interest consid problem person onlin servic viewpoint display ad target seek find best ad categori shown user result improv user experi increas advertis revenu propos address problem task rank ad categori user prefer introduc novel label rank approach capabl effici learn nonlinear high accur model largescal set experi realworld advertis data set 32 million user show propos algorithm outperform exist solut term rank loss topk retriev perform strong suggest benefit use propos model largescal rank problem comput advertis label rank onlin learn largescal learn big data,6,10.766187,9.227109
Efficient Object Detection via Adaptive Online Selection of Sensor-Array Elements,Matthai Philipose,"Novel Machine Learning Algorithms (NMLA)
Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)
Vision (VIS)","object detection
low power
value of information
adaptive submodular optimization
online optimization","MLA: Machine Learning Applications (General/other)
NMLA: Active Learning
NMLA: Bayesian Learning
PS: Probabilistic Planning
PS: Temporal Planning
RU: Bayesian Networks
RU: Decision/Utility Theory
RU: Probabilistic Inference
RU: Sequential Decision Making
VIS: Categorization
VIS: Object Detection
VIS: Statistical Methods and Learning
VIS: Videos","We examine how to use emerging far-infrared imager ensembles to detect certain
objects of interest (e.g., faces, hands, people and animals) in synchronized
RGB video streams at very low power. We formulate the problem as one of selecting
subsets of sensing elements (among many thousand possibilities) from the
ensembles for  tests. The subset selection  problem is naturally {\em adaptive} and {\em online}: testing certain elements early can obviate the need for testing many others later, and selection policies must be updated at inference time. We pose the ensemble sensor selection problem as a structured extension of test-cost-sensitive classification, propose a principled suite of techniques to exploit ensemble structure to  speed up processing and show how to re-estimate policies fast. We estimate reductions in power consumption of roughly 50x relative to even highly optimized implementations of face detection, a canonical object-detection problem. We also illustrate the benefits of adaptivity and online estimation.","Efficient Object Detection via Adaptive Online Selection of Sensor-Array Elements We examine how to use emerging far-infrared imager ensembles to detect certain
objects of interest (e.g., faces, hands, people and animals) in synchronized
RGB video streams at very low power. We formulate the problem as one of selecting
subsets of sensing elements (among many thousand possibilities) from the
ensembles for  tests. The subset selection  problem is naturally {\em adaptive} and {\em online}: testing certain elements early can obviate the need for testing many others later, and selection policies must be updated at inference time. We pose the ensemble sensor selection problem as a structured extension of test-cost-sensitive classification, propose a principled suite of techniques to exploit ensemble structure to  speed up processing and show how to re-estimate policies fast. We estimate reductions in power consumption of roughly 50x relative to even highly optimized implementations of face detection, a canonical object-detection problem. We also illustrate the benefits of adaptivity and online estimation. object detection
low power
value of information
adaptive submodular optimization
online optimization",effici object detect via adapt onlin select sensorarray element examin use emerg farinfrar imag ensembl detect certain object interest eg face hand peopl anim synchron rgb video stream low power formul problem one select subset sens element among mani thousand possibl ensembl test subset select problem natur em adapt em onlin test certain element earli obviat need test mani other later select polici must updat infer time pose ensembl sensor select problem structur extens testcostsensit classif propos principl suit techniqu exploit ensembl structur speed process show reestim polici fast estim reduct power consumpt rough 50x relat even high optim implement face detect canon objectdetect problem also illustr benefit adapt onlin estim object detect low power valu inform adapt submodular optim onlin optim,1,2.2008595,-8.152227
Simultaneous Cake Cutting,"Eric Balkanski, Simina Brânzei, David Kurokawa and Ariel Procaccia",Game Theory and Economic Paradigms (GTEP),"Cake cutting
Fair division
Computational social choice",GTEP: Social Choice / Voting,"We introduce the simultaneous model for cake cutting (the fair allocation of a divisible good), in which agents simultaneously send messages containing a sketch of their preferences over the cake. We show that this model enables the computation of divisions that satisfy proportionality — a popular fairness notion — using a protocol that circumvents a standard lower bound via parallel information elicitation. Cake divisions satisfying another prominent fairness notion, envy-freeness, are impossible to compute in the simultaneous model, but such allocations admit arbitrarily good approximations.","Simultaneous Cake Cutting We introduce the simultaneous model for cake cutting (the fair allocation of a divisible good), in which agents simultaneously send messages containing a sketch of their preferences over the cake. We show that this model enables the computation of divisions that satisfy proportionality — a popular fairness notion — using a protocol that circumvents a standard lower bound via parallel information elicitation. Cake divisions satisfying another prominent fairness notion, envy-freeness, are impossible to compute in the simultaneous model, but such allocations admit arbitrarily good approximations. Cake cutting
Fair division
Computational social choice",simultan cake cut introduc simultan model cake cut fair alloc divis good agent simultan send messag contain sketch prefer cake show model enabl comput divis satisfi proport — popular fair notion — use protocol circumv standard lower bound via parallel inform elicit cake divis satisfi anoth promin fair notion envyfre imposs comput simultan model alloc admit arbitrarili good approxim cake cut fair divis comput social choic,9,-17.466824,-18.717123
Recovering from Selection Bias in Causal and Statistical Inference,"Elias Bareinboim, Jin Tian and Judea Pearl","Knowledge Representation and Reasoning (KRR)
Reasoning under Uncertainty (RU)","Causal Inference
Causal Reasoning
Do-calculus
Causality
Selection Bias
Sampling Selection Bias
Case-control studies
Bias Removal
Backdoor criterion","KRR: Action, Change, and Causality
RU: Bayesian Networks
RU: Uncertainty in AI (General/Other)","Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected  in either experimental or observational studies.  Extending the results of (Bareinboim and Pearl 2012), we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data.  We further provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection.","Recovering from Selection Bias in Causal and Statistical Inference Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected  in either experimental or observational studies.  Extending the results of (Bareinboim and Pearl 2012), we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data.  We further provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection. Causal Inference
Causal Reasoning
Do-calculus
Causality
Selection Bias
Sampling Selection Bias
Case-control studies
Bias Removal
Backdoor criterion",recov select bias causal statist infer select bias caus preferenti exclus unit sampl repres major obstacl valid causal statist infer cannot remov random experi rare detect either experiment observ studi extend result bareinboim pearl 2012 provid complet graphic algorithm condit recov condit probabl select bias data provid graphic condit recover unbias data avail subset variabl final provid graphic condit general backdoor criterion serv recov causal effect data collect preferenti select causal infer causal reason docalculus causal select bias sampl select bias casecontrol studi bias remov backdoor criterion,6,15.979508,-5.224186
Semi-supervised Target Alignment via Label-Aware Base Kernels,"Qiaojun Wang, Kai Zhang and Ivan Marsic",Novel Machine Learning Algorithms (NMLA),"Semi-supervised Kernel Learning
Eigenfunction Extrapolation
Kernel Target Alignment
Ideal Kernel","NMLA: Kernel Methods
NMLA: Semisupervised Learning","Currently, a large family of kernel methods for semi-supervised learning(SSL) problems builds the kernel by weighted average of predefined base kernels (i.e., those spanned by kernel eigenvectors). Optimization of the base kernel weights has been studied extensively in the literatures. However, little attention was devoted to designing high-quality base kernels. Note that the eigenvectors of the kernel matrix, which are computed irrespective of class labels, may not always reveal useful structures of the target. As a result, the generalization performance can be poor however hard the base kernel weighting is tuned. On the other hand, there are many SSL algorithms whose focus is not on kernel design but instead the estimation of the class labels directly. Motivated by the label propagation approach, in this paper we propose to construct novel kernel eigenvectors by injecting the class label information under the framework of eigenfunction extrapolation. A set of ``label-aware'' base kernels can be obtained with greatly improved quality, which leads to higher target alignment and henceforth better performance. Our approach is computationally efficient, and demonstrates encouraging performance in semi-supervised classification and regression tasks.","Semi-supervised Target Alignment via Label-Aware Base Kernels Currently, a large family of kernel methods for semi-supervised learning(SSL) problems builds the kernel by weighted average of predefined base kernels (i.e., those spanned by kernel eigenvectors). Optimization of the base kernel weights has been studied extensively in the literatures. However, little attention was devoted to designing high-quality base kernels. Note that the eigenvectors of the kernel matrix, which are computed irrespective of class labels, may not always reveal useful structures of the target. As a result, the generalization performance can be poor however hard the base kernel weighting is tuned. On the other hand, there are many SSL algorithms whose focus is not on kernel design but instead the estimation of the class labels directly. Motivated by the label propagation approach, in this paper we propose to construct novel kernel eigenvectors by injecting the class label information under the framework of eigenfunction extrapolation. A set of ``label-aware'' base kernels can be obtained with greatly improved quality, which leads to higher target alignment and henceforth better performance. Our approach is computationally efficient, and demonstrates encouraging performance in semi-supervised classification and regression tasks. Semi-supervised Kernel Learning
Eigenfunction Extrapolation
Kernel Target Alignment
Ideal Kernel",semisupervis target align via labelawar base kernel current larg famili kernel method semisupervis learningssl problem build kernel weight averag predefin base kernel ie span kernel eigenvector optim base kernel weight studi extens literatur howev littl attent devot design highqual base kernel note eigenvector kernel matrix comput irrespect class label may alway reveal use structur target result general perform poor howev hard base kernel weight tune hand mani ssl algorithm whose focus kernel design instead estim class label direct motiv label propag approach paper propos construct novel kernel eigenvector inject class label inform framework eigenfunct extrapol set labelawar base kernel obtain great improv qualiti lead higher target align henceforth better perform approach comput effici demonstr encourag perform semisupervis classif regress task semisupervis kernel learn eigenfunct extrapol kernel target align ideal kernel,6,-1.5730199,-23.516958
Active Learning with Model Selection via Nested Cross-Validation,"Alnur Ali, Rich Caruana and Ashish Kapoor",Humans and AI (HAI),"active learning
model selection
machine learning",NMLA: Active Learning,"Most work on active learning avoids the issue of model selection by training models of only one type (SVMs, boosted trees, etc.) using one pre-defined set of model hyperparameters.  We propose an algorithm that actively samples data to simultaneously train a set of candidate models (different model types and/or different hyperparameters) and also to select the best model from this set of candidates.  The algorithm actively samples points for training that are most likely to improve the accuracy of the more promising candidate models, and also samples points to use for model selection---all samples count against the same ﬁxed labeling budget. This exposes a natural trade-off between the focused active sampling that is most effective for training models, and the unbiased uniform sampling that is better for model selection.  We empirically demonstrate on six test problems that this algorithm is nearly as effective as an active learning oracle that knows the optimal model in advance.","Active Learning with Model Selection via Nested Cross-Validation Most work on active learning avoids the issue of model selection by training models of only one type (SVMs, boosted trees, etc.) using one pre-defined set of model hyperparameters.  We propose an algorithm that actively samples data to simultaneously train a set of candidate models (different model types and/or different hyperparameters) and also to select the best model from this set of candidates.  The algorithm actively samples points for training that are most likely to improve the accuracy of the more promising candidate models, and also samples points to use for model selection---all samples count against the same ﬁxed labeling budget. This exposes a natural trade-off between the focused active sampling that is most effective for training models, and the unbiased uniform sampling that is better for model selection.  We empirically demonstrate on six test problems that this algorithm is nearly as effective as an active learning oracle that knows the optimal model in advance. active learning
model selection
machine learning",activ learn model select via nest crossvalid work activ learn avoid issu model select train model one type svms boost tree etc use one predefin set model hyperparamet propos algorithm activ sampl data simultan train set candid model differ model type andor differ hyperparamet also select best model set candid algorithm activ sampl point train like improv accuraci promis candid model also sampl point use model selectional sampl count ﬁxed label budget expos natur tradeoff focus activ sampl effect train model unbias uniform sampl better model select empir demonstr six test problem algorithm near effect activ learn oracl know optim model advanc activ learn model select machin learn,6,4.1031375,-8.661763
Sketch Recognition with Natural Correction and Editing,"Jie Wu, Changhu Wang, Liqing Zhang and Yong Rui","AI and the Web (AIW)
Humans and AI (HAI)
Machine Learning Applications (MLA)
Vision (VIS)","Sketch Recognition
Symbol Recognition
User Interface
Correction and Editing
Shape Knowledge","AIW: Intelligent user interfaces for web systems
HAI: Human-Computer Interaction
HAI: Interaction Techniques and Devices
MLA: Applications of Supervised Learning
NMLA: Data Mining and Knowledge Discovery
VIS: Object Recognition","In this paper, we target at the problem of sketch recognition. We systematically study how to incorporate users' natural correction and editing into isolated and full sketch recognition. This is a natural and necessary interaction in real systems such as Visio where extremely similar shapes exist. First, a novel algorithm is proposed to mine the prior shape knowledge for three editing modes. Second, to differentiate visually similar shapes, a novel symbol recognition algorithm is introduced by leveraging the learnt shape knowledge. Then, a novel correction/editing detection algorithm is proposed to facilitate symbol recognition. Furthermore, both of the symbol recognizer and the correction/editing detector are systematically incorporated into the full sketch recognition. Finally, based on the proposed algorithms, a real-time sketch recognition system is built to recognize hand-drawn flowchart/diagram with flexible interactions. Extensive experiments on benchmark datasets show the effectiveness of the proposed algorithms.","Sketch Recognition with Natural Correction and Editing In this paper, we target at the problem of sketch recognition. We systematically study how to incorporate users' natural correction and editing into isolated and full sketch recognition. This is a natural and necessary interaction in real systems such as Visio where extremely similar shapes exist. First, a novel algorithm is proposed to mine the prior shape knowledge for three editing modes. Second, to differentiate visually similar shapes, a novel symbol recognition algorithm is introduced by leveraging the learnt shape knowledge. Then, a novel correction/editing detection algorithm is proposed to facilitate symbol recognition. Furthermore, both of the symbol recognizer and the correction/editing detector are systematically incorporated into the full sketch recognition. Finally, based on the proposed algorithms, a real-time sketch recognition system is built to recognize hand-drawn flowchart/diagram with flexible interactions. Extensive experiments on benchmark datasets show the effectiveness of the proposed algorithms. Sketch Recognition
Symbol Recognition
User Interface
Correction and Editing
Shape Knowledge",sketch recognit natur correct edit paper target problem sketch recognit systemat studi incorpor user natur correct edit isol full sketch recognit natur necessari interact real system visio extrem similar shape exist first novel algorithm propos mine prior shape knowledg three edit mode second differenti visual similar shape novel symbol recognit algorithm introduc leverag learnt shape knowledg novel correctionedit detect algorithm propos facilit symbol recognit furthermor symbol recogn correctionedit detector systemat incorpor full sketch recognit final base propos algorithm realtim sketch recognit system built recogn handdrawn flowchartdiagram flexibl interact extens experi benchmark dataset show effect propos algorithm sketch recognit symbol recognit user interfac correct edit shape knowledg,1,3.9042902,-5.4184947
Generalized Label Reduction for Merge-and-Shrink Heuristics,"Silvan Sievers, Martin Wehrle and Malte Helmert","Heuristic Search and Optimization (HSO)
Planning and Scheduling (PS)","classical planning
heuristic search
merge-and-shrink abstractions
label reduction","HSO: Heuristic Search
HSO: Optimization
HSO: Evaluation and Analysis (Search and Optimization)
PS: Deterministic Planning","Label reduction is a technique for simplifying families of labeled transition systems by dropping distinctions between certain transition labels. While label reduction is critical to the efficient computation of merge-and-shrink heuristics, current theory only permits reducing labels in a limited number of cases. We generalize this theory so that labels can be reduced in every intermediate abstraction of a merge-and-shrink tree. This is particularly important for efficiently computing merge-and-shrink abstractions based on non-linear merge strategies. As a case study, we implement a non-linear merge strategy based on the original work on merge-and-shrink heuristics in model checking by Dräger et al.","Generalized Label Reduction for Merge-and-Shrink Heuristics Label reduction is a technique for simplifying families of labeled transition systems by dropping distinctions between certain transition labels. While label reduction is critical to the efficient computation of merge-and-shrink heuristics, current theory only permits reducing labels in a limited number of cases. We generalize this theory so that labels can be reduced in every intermediate abstraction of a merge-and-shrink tree. This is particularly important for efficiently computing merge-and-shrink abstractions based on non-linear merge strategies. As a case study, we implement a non-linear merge strategy based on the original work on merge-and-shrink heuristics in model checking by Dräger et al. classical planning
heuristic search
merge-and-shrink abstractions
label reduction",general label reduct mergeandshrink heurist label reduct techniqu simplifi famili label transit system drop distinct certain transit label label reduct critic effici comput mergeandshrink heurist current theori permit reduc label limit number case general theori label reduc everi intermedi abstract mergeandshrink tree particular import effici comput mergeandshrink abstract base nonlinear merg strategi case studi implement nonlinear merg strategi base origin work mergeandshrink heurist model check dräger et al classic plan heurist search mergeandshrink abstract label reduct,6,-8.954594,-10.555233
Predicting Emotions in User-Generated Videos,Yu-Gang Jiang and Baohan Xu,AI and the Web (AIW),"Emotion
User-generated videos
Multimodal features",AIW: AI for multimedia and multimodal web applications,"User-generated video collections are expanding rapidly in recent years, and systems for automatic analysis of these collections are in high demands. While extensive research efforts have been devoted to recognizing semantics like ""birthday party"" and ""skiing"", little attempts have been made to understand the emotions carried by the videos, e.g., ""joy"" and ""sadness"". In this paper, we propose a comprehensive computational framework for predicting emotions in user-generated videos. We first introduce a rigorously designed dataset collected from popular video-sharing websites with manual annotations, which can serve as a valuable benchmark for future research. A large set of features are extracted from this dataset, ranging from popular low-level visual descriptors, audio features, to high-level semantic attributes. Results of a comprehensive set of experiments indicate that combining multiple types of features---such as the joint use of the audio and visual clues---is important, and attribute features such as those containing sentiment-level semantics are very effective.","Predicting Emotions in User-Generated Videos User-generated video collections are expanding rapidly in recent years, and systems for automatic analysis of these collections are in high demands. While extensive research efforts have been devoted to recognizing semantics like ""birthday party"" and ""skiing"", little attempts have been made to understand the emotions carried by the videos, e.g., ""joy"" and ""sadness"". In this paper, we propose a comprehensive computational framework for predicting emotions in user-generated videos. We first introduce a rigorously designed dataset collected from popular video-sharing websites with manual annotations, which can serve as a valuable benchmark for future research. A large set of features are extracted from this dataset, ranging from popular low-level visual descriptors, audio features, to high-level semantic attributes. Results of a comprehensive set of experiments indicate that combining multiple types of features---such as the joint use of the audio and visual clues---is important, and attribute features such as those containing sentiment-level semantics are very effective. Emotion
User-generated videos
Multimodal features",predict emot usergener video usergener video collect expand rapid recent year system automat analysi collect high demand extens research effort devot recogn semant like birthday parti ski littl attempt made understand emot carri video eg joy sad paper propos comprehens comput framework predict emot usergener video first introduc rigor design dataset collect popular videoshar websit manual annot serv valuabl benchmark futur research larg set featur extract dataset rang popular lowlevel visual descriptor audio featur highlevel semant attribut result comprehens set experi indic combin multipl type featuressuch joint use audio visual cluesi import attribut featur contain sentimentlevel semant effect emot usergener video multimod featur,6,-20.172976,-0.27069992
Emotion Classification in Microblog Texts Using Class Sequential Rules,Shiyang Wen and Xiaojun Wan,AI and the Web (AIW),"Emotion Classification
Chinese Microblogs
Class Sequential Rules","AIW: Knowledge acquisition from the web
AIW: Web-based opinion extraction and trend spotting","This paper studies the problem of emotion classification in microblog texts. Given a microblog text which consists of several sentences, we classify its emotion as anger, disgust, fear, happiness, like, sadness or surprise if possible. Existing methods can be categorized as lexicon based methods or machine learning based methods. However, due to some intrinsic characteristics of the microblog texts, previous studies using these methods always get unsatisfactory results. This paper introduces a novel approach based on class sequential rules for emotion classification of microblog texts. The approach first obtains two potential emotion labels for each sentence in a microblog text by using an emotion lexicon and a machine learning approach respectively, and regards each microblog text as a data sequence. It then mines class sequential rules from the sequence set and finally derives new features from the mined rules for emotion classification of microblog texts. Experimental results on a Chinese benchmark dataset show the superior performance of the proposed approach.","Emotion Classification in Microblog Texts Using Class Sequential Rules This paper studies the problem of emotion classification in microblog texts. Given a microblog text which consists of several sentences, we classify its emotion as anger, disgust, fear, happiness, like, sadness or surprise if possible. Existing methods can be categorized as lexicon based methods or machine learning based methods. However, due to some intrinsic characteristics of the microblog texts, previous studies using these methods always get unsatisfactory results. This paper introduces a novel approach based on class sequential rules for emotion classification of microblog texts. The approach first obtains two potential emotion labels for each sentence in a microblog text by using an emotion lexicon and a machine learning approach respectively, and regards each microblog text as a data sequence. It then mines class sequential rules from the sequence set and finally derives new features from the mined rules for emotion classification of microblog texts. Experimental results on a Chinese benchmark dataset show the superior performance of the proposed approach. Emotion Classification
Chinese Microblogs
Class Sequential Rules",emot classif microblog text use class sequenti rule paper studi problem emot classif microblog text given microblog text consist sever sentenc classifi emot anger disgust fear happi like sad surpris possibl exist method categor lexicon base method machin learn base method howev due intrins characterist microblog text previous studi use method alway get unsatisfactori result paper introduc novel approach base class sequenti rule emot classif microblog text approach first obtain two potenti emot label sentenc microblog text use emot lexicon machin learn approach respect regard microblog text data sequenc mine class sequenti rule sequenc set final deriv new featur mine rule emot classif microblog text experiment result chines benchmark dataset show superior perform propos approach emot classif chines microblog class sequenti rule,6,-7.740232,-2.6988184
k-CoRating: Filling up Data to Obtain Privacy and Utility,"Feng Zhang, Victor E Lee and Ruoming Jin","AI and the Web (AIW)
Applications (APP)
Novel Machine Learning Algorithms (NMLA)","Privacy-preserving Collaborative Filtering Recommender Systems
Data Privacy
Parallel Computing","AIW: Web-based recommendation systems
APP: Security and Privacy
NMLA: Recommender Systems","For datasets in Collaborative Filtering (CF) recommendations, even if the identifier is deleted and some trivial perturbation operations are applied to ratings before they are released, there are research results claiming that the adversary could discriminate the individual's identity with a little bit of information. In this paper, we propose $k$-coRating, a novel privacy-preserving model, to retain data privacy by replacing some null ratings with significantly predicted scores. They do not only mask the original ratings such that a $k$-anonymity-like data privacy is preserved, but also enhance the data utility (measured by prediction accuracy in this paper), which shows that the traditional assumption that accuracy and privacy are two goals in conflict is not necessarily correct. We show that the optimal $k$-coRated mapping is an NP-hard problem and design a naive but efficient algorithm to achieve $k$-coRating. All claims are verified by experimental results.","k-CoRating: Filling up Data to Obtain Privacy and Utility For datasets in Collaborative Filtering (CF) recommendations, even if the identifier is deleted and some trivial perturbation operations are applied to ratings before they are released, there are research results claiming that the adversary could discriminate the individual's identity with a little bit of information. In this paper, we propose $k$-coRating, a novel privacy-preserving model, to retain data privacy by replacing some null ratings with significantly predicted scores. They do not only mask the original ratings such that a $k$-anonymity-like data privacy is preserved, but also enhance the data utility (measured by prediction accuracy in this paper), which shows that the traditional assumption that accuracy and privacy are two goals in conflict is not necessarily correct. We show that the optimal $k$-coRated mapping is an NP-hard problem and design a naive but efficient algorithm to achieve $k$-coRating. All claims are verified by experimental results. Privacy-preserving Collaborative Filtering Recommender Systems
Data Privacy
Parallel Computing",kcorat fill data obtain privaci util dataset collabor filter cf recommend even identifi delet trivial perturb oper appli rate releas research result claim adversari could discrimin individu ident littl bit inform paper propos kcorat novel privacypreserv model retain data privaci replac null rate signific predict score mask origin rate kanonymitylik data privaci preserv also enhanc data util measur predict accuraci paper show tradit assumpt accuraci privaci two goal conflict necessarili correct show optim kcorat map nphard problem design naiv effici algorithm achiev kcorat claim verifi experiment result privacypreserv collabor filter recommend system data privaci parallel comput,0,-11.004839,-4.9919357
Adaptive Multi-Compositionality for Recursive Neural Models with Applications to Sentiment Analysis,"Li Dong, Furu Wei, Ming Zhou and Ke Xu",NLP and Machine Learning (NLPML),"recursive neural network
sentiment analysis
semantic composition
recursive neural model
neural network
deep learning",NLPML: Natural Language Processing (General/Other),"Recursive neural models have achieved promising results in many natural language processing tasks. The main difference among these models lies in the composition function, i.e., how to obtain the vector representation for a phrase or sentence using the representations of words it contains. This paper introduces a novel Adaptive Multi-Compositionality (AdaMC) layer to recursive neural models. The basic idea is to use more than one composition functions and adaptively select them depending on the input vectors. We present a general framework to model each semantic composition as a distribution over these composition functions. The composition functions and parameters used for adaptive selection are learned jointly from data. We integrate AdaMC into existing recursive neural models and conduct extensive experiments on the Stanford Sentiment Treebank. The results illustrate that AdaMC significantly outperforms state-of-the-art sentiment classification methods. It helps push the best accuracy of sentence-level negative/positive classification from 85.4% up to 88.5%.","Adaptive Multi-Compositionality for Recursive Neural Models with Applications to Sentiment Analysis Recursive neural models have achieved promising results in many natural language processing tasks. The main difference among these models lies in the composition function, i.e., how to obtain the vector representation for a phrase or sentence using the representations of words it contains. This paper introduces a novel Adaptive Multi-Compositionality (AdaMC) layer to recursive neural models. The basic idea is to use more than one composition functions and adaptively select them depending on the input vectors. We present a general framework to model each semantic composition as a distribution over these composition functions. The composition functions and parameters used for adaptive selection are learned jointly from data. We integrate AdaMC into existing recursive neural models and conduct extensive experiments on the Stanford Sentiment Treebank. The results illustrate that AdaMC significantly outperforms state-of-the-art sentiment classification methods. It helps push the best accuracy of sentence-level negative/positive classification from 85.4% up to 88.5%. recursive neural network
sentiment analysis
semantic composition
recursive neural model
neural network
deep learning",adapt multicomposit recurs neural model applic sentiment analysi recurs neural model achiev promis result mani natur languag process task main differ among model lie composit function ie obtain vector represent phrase sentenc use represent word contain paper introduc novel adapt multicomposit adamc layer recurs neural model basic idea use one composit function adapt select depend input vector present general framework model semant composit distribut composit function composit function paramet use adapt select learn joint data integr adamc exist recurs neural model conduct extens experi stanford sentiment treebank result illustr adamc signific outperform stateoftheart sentiment classif method help push best accuraci sentencelevel negativeposit classif 854 885 recurs neural network sentiment analysi semant composit recurs neural model neural network deep learn,4,10.407825,-6.8334394
Predicting the Hardness of Learning Bayesian Networks,"Brandon Malone, Kustaa Kangas, Matti Järvisalo, Mikko Koivisto and Petri Myllymäki","Heuristic Search and Optimization (HSO)
Novel Machine Learning Algorithms (NMLA)","Bayesian networks
structure learning
algorithm portfolios
empirical hardness models","HSO: Metareasoning and Metaheuristics
HSO: Evaluation and Analysis (Search and Optimization)
NMLA: Graphical Model Learning
NMLA: Evaluation and Analysis (Machine Learning)","There are various algorithms for finding a Bayesian network structure
(BNS) that is optimal with respect to a given scoring function. No
single algorithm dominates the others in speed, and given a problem
instance, it is a priori unclear which algorithm will perform
best and how fast it will solve the problem. Estimating the running
times directly is extremely difficult as they are complicated functions
of the instance. The main contribution of this paper is characterization
of the empirical hardness of an instance for a given algorithm based on
a novel collection of non-trivial, yet efficiently computable features.
Our empirical results, based on the largest evaluation of
state-of-the-art BNS learning algorithms to date, demonstrate that we
can predict the runtimes to a reasonable degree of accuracy, and
effectively select algorithms that perform well on a particular
instance. Moreover, we also show how the results can be utilized in
building a portfolio algorithm that combines several individual
algorithms in an almost optimal manner.","Predicting the Hardness of Learning Bayesian Networks There are various algorithms for finding a Bayesian network structure
(BNS) that is optimal with respect to a given scoring function. No
single algorithm dominates the others in speed, and given a problem
instance, it is a priori unclear which algorithm will perform
best and how fast it will solve the problem. Estimating the running
times directly is extremely difficult as they are complicated functions
of the instance. The main contribution of this paper is characterization
of the empirical hardness of an instance for a given algorithm based on
a novel collection of non-trivial, yet efficiently computable features.
Our empirical results, based on the largest evaluation of
state-of-the-art BNS learning algorithms to date, demonstrate that we
can predict the runtimes to a reasonable degree of accuracy, and
effectively select algorithms that perform well on a particular
instance. Moreover, we also show how the results can be utilized in
building a portfolio algorithm that combines several individual
algorithms in an almost optimal manner. Bayesian networks
structure learning
algorithm portfolios
empirical hardness models",predict hard learn bayesian network various algorithm find bayesian network structur bns optim respect given score function singl algorithm domin other speed given problem instanc priori unclear algorithm perform best fast solv problem estim run time direct extrem difficult complic function instanc main contribut paper character empir hard instanc given algorithm base novel collect nontrivi yet effici comput featur empir result base largest evalu stateoftheart bns learn algorithm date demonstr predict runtim reason degre accuraci effect select algorithm perform well particular instanc moreov also show result util build portfolio algorithm combin sever individu algorithm almost optim manner bayesian network structur learn algorithm portfolio empir hard model,4,10.5398855,1.3141143
"Stochastic Privacy: Model, Methods, and Experiments","Adish Singla, Ece Kamar, Ryen White and Eric Horvitz",AI and the Web (AIW),"privacy tradeoff
value of information
online services
web search personalization
submodular optimization","AIW: Representing, reasoning, and using provenance, trust, privacy, and security on the web
AIW: Web personalization and user modeling
APP: Security and Privacy
RU: Decision/Utility Theory","Online services such as web search and e-commerce applications typically rely on the collection of data about users, including details of their activities on the web.  Such personal data is used to enhance the quality of service via personalization of content and to maximize revenues via better targeting of advertisements and deeper engagement of users on sites.  To date, service providers have largely followed the approach of either requiring or requesting consent for opting-in to share their data.  Users may be willing to share private information in return for better quality of service or for incentives, or in return for assurances about the nature and extend of the logging of data. We introduce \emph{stochastic privacy}, a new approach to privacy centering on a simple concept: A guarantee is provided to users about the upper-bound on the probability that their personal data will be used. Such a probability, which we refer to as \emph{privacy risk}, can be assessed by users as a preference or communicated as a policy by a service provider.  Service providers can work to personalize and to optimize revenues in accordance with preferences about privacy risk.  We present procedures, proofs, and an overall system for maximizing the quality of services, while respecting bounds on allowable or communicated privacy risk. We demonstrate the methodology with a case study and evaluation of the procedures applied to web search personalization.  We show how we can achieve near-optimal utility of accessing information with provable guarantees on the probability of sharing data.","Stochastic Privacy: Model, Methods, and Experiments Online services such as web search and e-commerce applications typically rely on the collection of data about users, including details of their activities on the web.  Such personal data is used to enhance the quality of service via personalization of content and to maximize revenues via better targeting of advertisements and deeper engagement of users on sites.  To date, service providers have largely followed the approach of either requiring or requesting consent for opting-in to share their data.  Users may be willing to share private information in return for better quality of service or for incentives, or in return for assurances about the nature and extend of the logging of data. We introduce \emph{stochastic privacy}, a new approach to privacy centering on a simple concept: A guarantee is provided to users about the upper-bound on the probability that their personal data will be used. Such a probability, which we refer to as \emph{privacy risk}, can be assessed by users as a preference or communicated as a policy by a service provider.  Service providers can work to personalize and to optimize revenues in accordance with preferences about privacy risk.  We present procedures, proofs, and an overall system for maximizing the quality of services, while respecting bounds on allowable or communicated privacy risk. We demonstrate the methodology with a case study and evaluation of the procedures applied to web search personalization.  We show how we can achieve near-optimal utility of accessing information with provable guarantees on the probability of sharing data. privacy tradeoff
value of information
online services
web search personalization
submodular optimization",stochast privaci model method experi onlin servic web search ecommerc applic typic reli collect data user includ detail activ web person data use enhanc qualiti servic via person content maxim revenu via better target advertis deeper engag user site date servic provid larg follow approach either requir request consent optingin share data user may will share privat inform return better qualiti servic incent return assur natur extend log data introduc emphstochast privaci new approach privaci center simpl concept guarante provid user upperbound probabl person data use probabl refer emphprivaci risk assess user prefer communic polici servic provid servic provid work person optim revenu accord prefer privaci risk present procedur proof overal system maxim qualiti servic respect bound allow communic privaci risk demonstr methodolog case studi evalu procedur appli web search person show achiev nearoptim util access inform provabl guarante probabl share data privaci tradeoff valu inform onlin servic web search person submodular optim,5,-10.1450205,-4.369199
A Parameterized Complexity Analysis of Generalized CP-Nets,"Martin Kronegger, Martin Lackner, Andreas Pfandler and Reinhard Pichler","Game Playing and Interactive Entertainment (GPIE)
Knowledge Representation and Reasoning (KRR)","Computational social choice
CP-nets
Fixed-parameter tractable algorithms
(Parameterized) complexity","GTEP: Social Choice / Voting
KRR: Computational Complexity of Reasoning
KRR: Preferences","Generalized CP-nets (GCP-nets) allow a succinct representation of preferences over multi-attribute domains. As a consequence of their succinct representation, many GCP-net related tasks are computationally hard. Even finding the more preferable of two outcomes is PSPACE-complete. In this work, we employ the framework of parameterized complexity to achieve two goals: First, we want to gain a deeper understanding of the complexity of GCP-nets. Second, we search for efficient fixed-parameter tractable algorithms.","A Parameterized Complexity Analysis of Generalized CP-Nets Generalized CP-nets (GCP-nets) allow a succinct representation of preferences over multi-attribute domains. As a consequence of their succinct representation, many GCP-net related tasks are computationally hard. Even finding the more preferable of two outcomes is PSPACE-complete. In this work, we employ the framework of parameterized complexity to achieve two goals: First, we want to gain a deeper understanding of the complexity of GCP-nets. Second, we search for efficient fixed-parameter tractable algorithms. Computational social choice
CP-nets
Fixed-parameter tractable algorithms
(Parameterized) complexity",parameter complex analysi general cpnet general cpnet gcpnet allow succinct represent prefer multiattribut domain consequ succinct represent mani gcpnet relat task comput hard even find prefer two outcom pspacecomplet work employ framework parameter complex achiev two goal first want gain deeper understand complex gcpnet second search effici fixedparamet tractabl algorithm comput social choic cpnet fixedparamet tractabl algorithm parameter complex,9,17.787724,-3.6664402
Solving Imperfect Information Games Using Decomposition,"Neil Burch, Michael Johanson and Michael Bowling",Game Theory and Economic Paradigms (GTEP),"game theory
equilibrium theory
extensive-form games
imperfect information games","GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information","Decomposition, i.e., independently analyzing possible subgames, has proven to be an essential principle for effective decision-making in perfect information games.  However, in imperfect information games, decomposition has proven to be problematic.  To date, all proposed techniques for decomposition in imperfect information games have abandoned theoretical guarantees.  This work presents the first technique for decomposing an imperfect information game into subgames that can be solved independently, while retaining optimality guarantees on the full-game solution.  We can use this technique to construct theoretically justified algorithms that make better use of information available at run-time, overcome memory or disk limitations at run-time, or make a time/space trade-off to overcome memory or disk limitations while solving a game.  In particular, we present an algorithm for subgame solving which guarantees performance in the whole game, in contrast to existing methods which may have unbounded error.  In addition, we present an offline game solving algorithm, CFR-D, which can produce a Nash equilibrium for a game that is larger than available storage.","Solving Imperfect Information Games Using Decomposition Decomposition, i.e., independently analyzing possible subgames, has proven to be an essential principle for effective decision-making in perfect information games.  However, in imperfect information games, decomposition has proven to be problematic.  To date, all proposed techniques for decomposition in imperfect information games have abandoned theoretical guarantees.  This work presents the first technique for decomposing an imperfect information game into subgames that can be solved independently, while retaining optimality guarantees on the full-game solution.  We can use this technique to construct theoretically justified algorithms that make better use of information available at run-time, overcome memory or disk limitations at run-time, or make a time/space trade-off to overcome memory or disk limitations while solving a game.  In particular, we present an algorithm for subgame solving which guarantees performance in the whole game, in contrast to existing methods which may have unbounded error.  In addition, we present an offline game solving algorithm, CFR-D, which can produce a Nash equilibrium for a game that is larger than available storage. game theory
equilibrium theory
extensive-form games
imperfect information games",solv imperfect inform game use decomposit decomposit ie independ analyz possibl subgam proven essenti principl effect decisionmak perfect inform game howev imperfect inform game decomposit proven problemat date propos techniqu decomposit imperfect inform game abandon theoret guarante work present first techniqu decompos imperfect inform game subgam solv independ retain optim guarante fullgam solut use techniqu construct theoret justifi algorithm make better use inform avail runtim overcom memori disk limit runtim make timespac tradeoff overcom memori disk limit solv game particular present algorithm subgam solv guarante perform whole game contrast exist method may unbound error addit present offlin game solv algorithm cfrd produc nash equilibrium game larger avail storag game theori equilibrium theori extensiveform game imperfect inform game,2,7.3179183,20.2693
Robust Visual Robot Localization Across Seasons using Network Flows,"Tayyab Naseer, Luciano Spinello, Wolfram Burgard and Cyrill Stachniss",Robotics (ROB),"robotics
visual localization
seasons","ROB: Localization, Mapping, and Navigation","Image-based localization is an important problem in robotics and an integral part of visual mapping and navigation systems. An approach to robustly matching images to previously recorded ones must be able to cope with seasonal changes especially when it is supposed to work reliably over long periods of time.  In this paper, we present a novel approach to visual localization of mobile robots in outdoor environments, which is able to deal with substantial seasonal changes. We formulate image matching as a minimum cost flow problem in a data association graph to effectively exploit sequence information. This allows us to deal with non-matching image sequences that result from temporal occlusions or from visiting new places. We present extensive experimental evaluations under substantial seasonal changes. They suggest that our approach allows for an accurate matching across seasons and outperforms existing state-of-the-art methods such as FABMAP2 and SeqSLAM in such a context.","Robust Visual Robot Localization Across Seasons using Network Flows Image-based localization is an important problem in robotics and an integral part of visual mapping and navigation systems. An approach to robustly matching images to previously recorded ones must be able to cope with seasonal changes especially when it is supposed to work reliably over long periods of time.  In this paper, we present a novel approach to visual localization of mobile robots in outdoor environments, which is able to deal with substantial seasonal changes. We formulate image matching as a minimum cost flow problem in a data association graph to effectively exploit sequence information. This allows us to deal with non-matching image sequences that result from temporal occlusions or from visiting new places. We present extensive experimental evaluations under substantial seasonal changes. They suggest that our approach allows for an accurate matching across seasons and outperforms existing state-of-the-art methods such as FABMAP2 and SeqSLAM in such a context. robotics
visual localization
seasons",robust visual robot local across season use network flow imagebas local import problem robot integr part visual map navig system approach robust match imag previous record one must abl cope season chang especi suppos work reliabl long period time paper present novel approach visual local mobil robot outdoor environ abl deal substanti season chang formul imag match minimum cost flow problem data associ graph effect exploit sequenc inform allow us deal nonmatch imag sequenc result tempor occlus visit new place present extens experiment evalu substanti season chang suggest approach allow accur match across season outperform exist stateoftheart method fabmap2 seqslam context robot visual local season,1,0.46897438,8.01942
Small-variance Asymptotics for Dirichlet Process Mixtures of SVMs,Yining Wang and Jun Zhu,Novel Machine Learning Algorithms (NMLA),"small-variance asymptotics
Bayesian nonparametric modeling
infinite SVM
collapsed Gibbs sampling","NMLA: Bayesian Learning
NMLA: Big Data / Scalability
NMLA: Classification
NMLA: Clustering","Infinite SVMs (iSVM) is a Dirichlet process (DP) mixture of large-margin classifiers. Though flexible in learning nonlinear classifiers and discovering latent clustering structures, iSVM has a difficult inference task and existing methods could hinder its applicability to large-scale problems. This paper presents a small-variance asymptotic analysis to derive a simple and efficient algorithm, which monotonically optimizes a max-margin DP-means (M2 DPM) problem, an extension of DP-means for both predictive learning and descriptive clustering. Our analysis is built on Gibbs infinite SVMs, an alternative DP mixture of large-margin machines,
which admits a partially collapsed Gibbs sampler without truncation by exploring data augmentation techniques. Experimental results show that M2 DPM runs much faster than similar algorithms without sacrificing prediction accuracies.","Small-variance Asymptotics for Dirichlet Process Mixtures of SVMs Infinite SVMs (iSVM) is a Dirichlet process (DP) mixture of large-margin classifiers. Though flexible in learning nonlinear classifiers and discovering latent clustering structures, iSVM has a difficult inference task and existing methods could hinder its applicability to large-scale problems. This paper presents a small-variance asymptotic analysis to derive a simple and efficient algorithm, which monotonically optimizes a max-margin DP-means (M2 DPM) problem, an extension of DP-means for both predictive learning and descriptive clustering. Our analysis is built on Gibbs infinite SVMs, an alternative DP mixture of large-margin machines,
which admits a partially collapsed Gibbs sampler without truncation by exploring data augmentation techniques. Experimental results show that M2 DPM runs much faster than similar algorithms without sacrificing prediction accuracies. small-variance asymptotics
Bayesian nonparametric modeling
infinite SVM
collapsed Gibbs sampling",smallvari asymptot dirichlet process mixtur svms infinit svms isvm dirichlet process dp mixtur largemargin classifi though flexibl learn nonlinear classifi discov latent cluster structur isvm difficult infer task exist method could hinder applic largescal problem paper present smallvari asymptot analysi deriv simpl effici algorithm monoton optim maxmargin dpmean m2 dpm problem extens dpmean predict learn descript cluster analysi built gibb infinit svms altern dp mixtur largemargin machin admit partial collaps gibb sampler without truncat explor data augment techniqu experiment result show m2 dpm run much faster similar algorithm without sacrif predict accuraci smallvari asymptot bayesian nonparametr model infinit svm collaps gibb sampl,7,-2.526066,-4.2438607
Online (Budgeted) Social Choice,Joel Oren and Brendan Lucier,Game Theory and Economic Paradigms (GTEP),"Online algorithms
online learning
approximation algorithms
computational social choice","GTEP: Game Theory
GTEP: Social Choice / Voting
GTEP: Adversarial Learning
GTEP: Imperfect Information
MAS: E-Commerce","We consider a classic social choice problem in an online setting.  
In each round, a decision maker observes a single agent's preferences over
a set of $m$ candidates, and must choose whether to irrevocably add a candidate 
to a selection set of limited cardinality $k$.  Each agent's (positional) score 
depends on the candidates in the set when he arrives, and the decision-maker's 
goal is to maximize average (over all agents) score.

We prove that no algorithm (even randomized) can achieve an approximation
factor better than $O(\log\log m/ \log m)$.  In contrast, if the agents 
arrive in random order, we present a $(1 - 1/e - o(1))$-approximate
algorithm, matching a lower bound for the offline problem.
We show that improved performance is possible for natural input distributions
or scoring rules.

Finally, if the algorithm is permitted to revoke decisions at a fixed
cost, we apply regret-minimization techniques to achieve approximation 
$1 - 1/e - o(1)$ even for arbitrary inputs.","Online (Budgeted) Social Choice We consider a classic social choice problem in an online setting.  
In each round, a decision maker observes a single agent's preferences over
a set of $m$ candidates, and must choose whether to irrevocably add a candidate 
to a selection set of limited cardinality $k$.  Each agent's (positional) score 
depends on the candidates in the set when he arrives, and the decision-maker's 
goal is to maximize average (over all agents) score.

We prove that no algorithm (even randomized) can achieve an approximation
factor better than $O(\log\log m/ \log m)$.  In contrast, if the agents 
arrive in random order, we present a $(1 - 1/e - o(1))$-approximate
algorithm, matching a lower bound for the offline problem.
We show that improved performance is possible for natural input distributions
or scoring rules.

Finally, if the algorithm is permitted to revoke decisions at a fixed
cost, we apply regret-minimization techniques to achieve approximation 
$1 - 1/e - o(1)$ even for arbitrary inputs. Online algorithms
online learning
approximation algorithms
computational social choice",onlin budget social choic consid classic social choic problem onlin set round decis maker observ singl agent prefer set candid must choos whether irrevoc add candid select set limit cardin k agent posit score depend candid set arriv decisionmak goal maxim averag agent score prove algorithm even random achiev approxim factor better ologlog log contrast agent arriv random order present 1 1e o1approxim algorithm match lower bound offlin problem show improv perform possibl natur input distribut score rule final algorithm permit revok decis fix cost appli regretminim techniqu achiev approxim 1 1e o1 even arbitrari input onlin algorithm onlin learn approxim algorithm comput social choic,9,17.155558,2.108768
Using The Matrix Ridge Approximation to Speedup Determinantal Point Processes Sampling Algorithms,"Shusen Wang, Chao Zhang, Hui Qian and Zhihua Zhang",Novel Machine Learning Algorithms (NMLA),"kernel approximation
determinantal point process (DPP)
matrix ridge approximation (MRA)
the Nystrom method","NMLA: Big Data / Scalability
NMLA: Kernel Methods","Determinantal point process (DPP) is an important probabilistic model that has extensive applications in artificial intelligence. The exact sampling algorithm of DPP requires the full eigenvalue decomposition of the kernel matrix which has high time and space complexities. This prohibits the applications of DPP from large-scale datasets. Previous work has applied the Nystrom method to speedup the sampling algorithm of DPP, and error bounds have been established for the approximation. In this paper we  employ the matrix ridge approximation (MRA) to speedup the sampling algorithm of DPP, and we show that our approach MRA-DPP has stronger error bound than the Nystrom-DPP. In certain circumstance our MRA-DPP is provably exact, whereas the Nystrom-DPP is far from the ground truth. Finally, experiments on several real-world datasets show that our MRA-DPP is much more accurate than the other approximation approaches.","Using The Matrix Ridge Approximation to Speedup Determinantal Point Processes Sampling Algorithms Determinantal point process (DPP) is an important probabilistic model that has extensive applications in artificial intelligence. The exact sampling algorithm of DPP requires the full eigenvalue decomposition of the kernel matrix which has high time and space complexities. This prohibits the applications of DPP from large-scale datasets. Previous work has applied the Nystrom method to speedup the sampling algorithm of DPP, and error bounds have been established for the approximation. In this paper we  employ the matrix ridge approximation (MRA) to speedup the sampling algorithm of DPP, and we show that our approach MRA-DPP has stronger error bound than the Nystrom-DPP. In certain circumstance our MRA-DPP is provably exact, whereas the Nystrom-DPP is far from the ground truth. Finally, experiments on several real-world datasets show that our MRA-DPP is much more accurate than the other approximation approaches. kernel approximation
determinantal point process (DPP)
matrix ridge approximation (MRA)
the Nystrom method",use matrix ridg approxim speedup determinant point process sampl algorithm determinant point process dpp import probabilist model extens applic artifici intellig exact sampl algorithm dpp requir full eigenvalu decomposit kernel matrix high time space complex prohibit applic dpp largescal dataset previous work appli nystrom method speedup sampl algorithm dpp error bound establish approxim paper employ matrix ridg approxim mra speedup sampl algorithm dpp show approach mradpp stronger error bound nystromdpp certain circumst mradpp provabl exact wherea nystromdpp far ground truth final experi sever realworld dataset show mradpp much accur approxim approach kernel approxim determinant point process dpp matrix ridg approxim mra nystrom method,1,-1.4374297,-4.78386
TopicMF: Simultaneously Exploiting Ratings and Reviews for Recommendation,"Yang Bao, Hui Fang and Jie Zhang","AI and the Web (AIW)
Knowledge Representation and Reasoning (KRR)","recommender system
ratings and free-form reviews
Non-negative matrix factorization","AIW: Web-based recommendation systems
KRR: Preferences
NMLA: Recommender Systems","Although users' preference is semantically reflected in the free-form review texts, this wealth of information was not fully exploited for learning recommender models. Specifically, almost all existing recommendation algorithms only exploit rating scores in order to find users' preference, but ignore the review texts accompanied with rating information. In this paper, we propose a novel matrix factorization model (called TopicMF) which simultaneously considers the ratings and accompanied review texts. Experimental results on 20 real-world datasets show the superiority of our model over the state-of-the-art models, demonstrating its effectiveness for recommendation task.","TopicMF: Simultaneously Exploiting Ratings and Reviews for Recommendation Although users' preference is semantically reflected in the free-form review texts, this wealth of information was not fully exploited for learning recommender models. Specifically, almost all existing recommendation algorithms only exploit rating scores in order to find users' preference, but ignore the review texts accompanied with rating information. In this paper, we propose a novel matrix factorization model (called TopicMF) which simultaneously considers the ratings and accompanied review texts. Experimental results on 20 real-world datasets show the superiority of our model over the state-of-the-art models, demonstrating its effectiveness for recommendation task. recommender system
ratings and free-form reviews
Non-negative matrix factorization",topicmf simultan exploit rate review recommend although user prefer semant reflect freeform review text wealth inform fulli exploit learn recommend model specif almost exist recommend algorithm exploit rate score order find user prefer ignor review text accompani rate inform paper propos novel matrix factor model call topicmf simultan consid rate accompani review text experiment result 20 realworld dataset show superior model stateoftheart model demonstr effect recommend task recommend system rate freeform review nonneg matrix factor,0,17.265007,7.7609177
OurAgent'13: A Champion Adaptive Power Trading Agent,Daniel Urieli and Peter Stone,"Applications (APP)
Computational Sustainability and AI (CSAI)
Game Theory and Economic Paradigms (GTEP)
Machine Learning Applications (MLA)
Multiagent Systems (MAS)
Novel Machine Learning Algorithms (NMLA)
Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","Autonomous Electricity Trading Agents
Machine Learning
Reinforcement Learning
Online Learning
Smart Grid
Trading agents competition
Sustainable Energy","APP: Other Applications
CSAI: Control and optimization of dynamic and spatiotemporal systems
CSAI: Modeling and control of complex high-dimensional systems
CSAI: Modeling the interactions of agents with different and often conflicting interests
GTEP: Auctions and Market-Based Systems
MLA: Environmental
MLA: Applications of Supervised Learning
MLA: Applications of Reinforcement Learning
MAS: Multiagent Systems (General/other)
NMLA: Reinforcement Learning
PS: Markov Models of Environments
RU: Sequential Decision Making","Sustainable energy systems of the future will no longer be able to
rely on the current paradigm that energy supply follows demand. Many
of the renewable energy resources do not necessarily produce the
energy when it is needed, and therefore there is a need for new market
structures that motivate sustainable behaviors by participants.  The
Power Trading Agent Competition ($\powertac$) is a new annual
competition that focuses on the design and operation of future retail
power markets, specifically in smart grid environments with renewable
energy production, smart metering, and autonomous agents acting on
behalf of customers and retailers. It uses a rich, open-source
simulation platform that is based on real-world data and
state-of-the-art customer models. Its purpose is to help researchers
understand the dynamics of customer and retailer decision-making, as
well as the robustness of proposed market designs. This paper
introduces OurAgent'13, the champion agent from the inaugural
competition in 2013. OurAgent is an adaptive agent that learns and
reacts to the environment in which it operates, by heavily relying on
reinforcement-learning and prediction methods. This paper describes the
constituent components of our agent and examines the success of the
complete agent through analysis of competition results and subsequent
controlled experiments.","OurAgent'13: A Champion Adaptive Power Trading Agent Sustainable energy systems of the future will no longer be able to
rely on the current paradigm that energy supply follows demand. Many
of the renewable energy resources do not necessarily produce the
energy when it is needed, and therefore there is a need for new market
structures that motivate sustainable behaviors by participants.  The
Power Trading Agent Competition ($\powertac$) is a new annual
competition that focuses on the design and operation of future retail
power markets, specifically in smart grid environments with renewable
energy production, smart metering, and autonomous agents acting on
behalf of customers and retailers. It uses a rich, open-source
simulation platform that is based on real-world data and
state-of-the-art customer models. Its purpose is to help researchers
understand the dynamics of customer and retailer decision-making, as
well as the robustness of proposed market designs. This paper
introduces OurAgent'13, the champion agent from the inaugural
competition in 2013. OurAgent is an adaptive agent that learns and
reacts to the environment in which it operates, by heavily relying on
reinforcement-learning and prediction methods. This paper describes the
constituent components of our agent and examines the success of the
complete agent through analysis of competition results and subsequent
controlled experiments. Autonomous Electricity Trading Agents
Machine Learning
Reinforcement Learning
Online Learning
Smart Grid
Trading agents competition
Sustainable Energy",ouragent13 champion adapt power trade agent sustain energi system futur longer abl reli current paradigm energi suppli follow demand mani renew energi resourc necessarili produc energi need therefor need new market structur motiv sustain behavior particip power trade agent competit powertac new annual competit focus design oper futur retail power market specif smart grid environ renew energi product smart meter autonom agent act behalf custom retail use rich opensourc simul platform base realworld data stateoftheart custom model purpos help research understand dynam custom retail decisionmak well robust propos market design paper introduc ouragent13 champion agent inaugur competit 2013 ourag adapt agent learn react environ oper heavili reli reinforcementlearn predict method paper describ constitu compon agent examin success complet agent analysi competit result subsequ control experi autonom electr trade agent machin learn reinforc learn onlin learn smart grid trade agent competit sustain energi,9,6.613535,11.853638
Fast and Accurate Influence Maximization on Large Networks with Pruned Monte-Carlo Simulations,"Naoto Ohsaka, Takuya Akiba, Yuichi Yoshida and Ken-Ichi Kawarabayashi",AI and the Web (AIW),"influence maximization
viral marketing
independent cascade model
social networks",AIW: Social networking and community identification,"Influence maximization is a problem to find small sets of highly influential individuals in a social network to maximize the spread of influence under stochastic cascade models of propagation. Although the problem has been well-studied, it is still highly challenging to find solutions of high quality in large-scale networks of the day. While Monte-Carlo-simulation-based methods produce nearly optimal solutions with a theoretical guarantee, they are prohibitively slow for large graphs. As a result, many heuristic methods without any theoretical guarantee have been developed, but all of them substantially compromise solution quality. To address this issue, we propose a new method for the influence maximization problem. Unlike other recent heuristic methods, the proposed method is a Monte-Carlo-simulation-based method, and thus it consistently produces solutions of high quality with the theoretical guarantee. On the other hand, unlike other previous Monte-Carlo-simulation-based methods, it runs as fast as other state-of-the-art methods, and can be applied to large networks of the day. Through our extensive experiments, we demonstrate the scalability and the solution quality of the proposed method.","Fast and Accurate Influence Maximization on Large Networks with Pruned Monte-Carlo Simulations Influence maximization is a problem to find small sets of highly influential individuals in a social network to maximize the spread of influence under stochastic cascade models of propagation. Although the problem has been well-studied, it is still highly challenging to find solutions of high quality in large-scale networks of the day. While Monte-Carlo-simulation-based methods produce nearly optimal solutions with a theoretical guarantee, they are prohibitively slow for large graphs. As a result, many heuristic methods without any theoretical guarantee have been developed, but all of them substantially compromise solution quality. To address this issue, we propose a new method for the influence maximization problem. Unlike other recent heuristic methods, the proposed method is a Monte-Carlo-simulation-based method, and thus it consistently produces solutions of high quality with the theoretical guarantee. On the other hand, unlike other previous Monte-Carlo-simulation-based methods, it runs as fast as other state-of-the-art methods, and can be applied to large networks of the day. Through our extensive experiments, we demonstrate the scalability and the solution quality of the proposed method. influence maximization
viral marketing
independent cascade model
social networks",fast accur influenc maxim larg network prune montecarlo simul influenc maxim problem find small set high influenti individu social network maxim spread influenc stochast cascad model propag although problem wellstudi still high challeng find solut high qualiti largescal network day montecarlosimulationbas method produc near optim solut theoret guarante prohibit slow larg graph result mani heurist method without theoret guarante develop substanti compromis solut qualiti address issu propos new method influenc maxim problem unlik recent heurist method propos method montecarlosimulationbas method thus consist produc solut high qualiti theoret guarante hand unlik previous montecarlosimulationbas method run fast stateoftheart method appli larg network day extens experi demonstr scalabl solut qualiti propos method influenc maxim viral market independ cascad model social network,5,11.955874,3.3236473
Fixing a Balanced Knockout Tournament,"Haris Aziz, Serge Gaspers, Simon Mackenzie, Nicholas Mattei, Paul Stursberg and Toby Walsh",Game Theory and Economic Paradigms (GTEP),"knockout tournaments
tournament fixing problem
manipulation","GTEP: Game Theory
GTEP: Social Choice / Voting","Balanced knockout tournaments are one of the most common formats for sports competitions, and are also used in elections and decision-making. We consider the computational problem of finding the optimal draw for a particular player in such a tournament. The problem has generated considerable research within AI in recent years. We prove that checking whether there exists a draw in which a player wins is NP-complete, thereby settling an outstanding open problem. Our main result has a number of interesting implications on related counting and approximation problems. We present a memoization-based algorithm for the problem that is faster than previous approaches. Moreover, we highlight two natural cases that can be solved in polynomial time. All of our results also hold for the more general problem of counting the number of draws in which a given player is the winner.","Fixing a Balanced Knockout Tournament Balanced knockout tournaments are one of the most common formats for sports competitions, and are also used in elections and decision-making. We consider the computational problem of finding the optimal draw for a particular player in such a tournament. The problem has generated considerable research within AI in recent years. We prove that checking whether there exists a draw in which a player wins is NP-complete, thereby settling an outstanding open problem. Our main result has a number of interesting implications on related counting and approximation problems. We present a memoization-based algorithm for the problem that is faster than previous approaches. Moreover, we highlight two natural cases that can be solved in polynomial time. All of our results also hold for the more general problem of counting the number of draws in which a given player is the winner. knockout tournaments
tournament fixing problem
manipulation",fix balanc knockout tournament balanc knockout tournament one common format sport competit also use elect decisionmak consid comput problem find optim draw particular player tournament problem generat consider research within ai recent year prove check whether exist draw player win npcomplet therebi settl outstand open problem main result number interest implic relat count approxim problem present memoizationbas algorithm problem faster previous approach moreov highlight two natur case solv polynomi time result also hold general problem count number draw given player winner knockout tournament tournament fix problem manipul,7,-15.477794,-3.7251556
Querying Inconsistent Description Logic Knowledge Bases under Preferred Repair Semantics,"Camille Bourgaux, Meghyn Bienvenu and François Goasdoué",Knowledge Representation and Reasoning (KRR),"inconsistency-tolerant query answering
complexity of query answering
DL-Lite
conjunctive queries","KRR: Ontologies
KRR: Computational Complexity of Reasoning
KRR: Description Logics
KRR: Preferences","Recently several inconsistency-tolerant semantics have been introduced for querying 
inconsistent description logic knowledge bases. Most of these semantics rely on the notion of a repair, defined as an inclusion-maximal subset of the facts (ABox) which is consistent with the ontology (TBox). In this paper, we investigate variants of two popular inconsistency-tolerant semantics obtained by replacing the classical notion of repair by different types of preferred repairs. For each of the resulting semantics, we analyze the complexity of conjunctive query answering over knowledge bases expressed in the lighweight logic DL-Lite. Unsurprisingly, query answering is intractable in all cases, but we nonetheless identify one notion of preferred repair, based upon assigning facts to priority levels, whose data complexity is ``only"" coNP-complete. This leads us to propose an approach combining incomplete tractable methods with calls to a SAT solver. An experimental evaluation of the approach shows good scalability on realistic cases.","Querying Inconsistent Description Logic Knowledge Bases under Preferred Repair Semantics Recently several inconsistency-tolerant semantics have been introduced for querying 
inconsistent description logic knowledge bases. Most of these semantics rely on the notion of a repair, defined as an inclusion-maximal subset of the facts (ABox) which is consistent with the ontology (TBox). In this paper, we investigate variants of two popular inconsistency-tolerant semantics obtained by replacing the classical notion of repair by different types of preferred repairs. For each of the resulting semantics, we analyze the complexity of conjunctive query answering over knowledge bases expressed in the lighweight logic DL-Lite. Unsurprisingly, query answering is intractable in all cases, but we nonetheless identify one notion of preferred repair, based upon assigning facts to priority levels, whose data complexity is ``only"" coNP-complete. This leads us to propose an approach combining incomplete tractable methods with calls to a SAT solver. An experimental evaluation of the approach shows good scalability on realistic cases. inconsistency-tolerant query answering
complexity of query answering
DL-Lite
conjunctive queries",queri inconsist descript logic knowledg base prefer repair semant recent sever inconsistencytoler semant introduc queri inconsist descript logic knowledg base semant reli notion repair defin inclusionmaxim subset fact abox consist ontolog tbox paper investig variant two popular inconsistencytoler semant obtain replac classic notion repair differ type prefer repair result semant analyz complex conjunct queri answer knowledg base express lighweight logic dllite unsurpris queri answer intract case nonetheless identifi one notion prefer repair base upon assign fact prioriti level whose data complex conpcomplet lead us propos approach combin incomplet tractabl method call sat solver experiment evalu approach show good scalabl realist case inconsistencytoler queri answer complex queri answer dllite conjunct queri,8,22.79579,3.478745
Incomplete Preferences in Single-Peaked Electorates,Martin Lackner,Game Theory and Economic Paradigms (GTEP),"Computational social choice
Preferences
Incomplete information
Structure
Single-peaked
Algorithms","GTEP: Social Choice / Voting
GTEP: Imperfect Information","Incomplete preferences are likely to arise in real-world preference aggregation and voting systems.
This paper deals with determining whether an incomplete preference profile is single-peaked.
This is essential information since many intractable voting problems become tractable for single-peaked profiles.
We prove that for incomplete profiles the problem of determining single-peakedness is NP-complete.
Despite this computational hardness result, we find four polynomial-time algorithms for reasonably restricted settings.","Incomplete Preferences in Single-Peaked Electorates Incomplete preferences are likely to arise in real-world preference aggregation and voting systems.
This paper deals with determining whether an incomplete preference profile is single-peaked.
This is essential information since many intractable voting problems become tractable for single-peaked profiles.
We prove that for incomplete profiles the problem of determining single-peakedness is NP-complete.
Despite this computational hardness result, we find four polynomial-time algorithms for reasonably restricted settings. Computational social choice
Preferences
Incomplete information
Structure
Single-peaked
Algorithms",incomplet prefer singlepeak elector incomplet prefer like aris realworld prefer aggreg vote system paper deal determin whether incomplet prefer profil singlepeak essenti inform sinc mani intract vote problem becom tractabl singlepeak profil prove incomplet profil problem determin singlepeaked npcomplet despit comput hard result find four polynomialtim algorithm reason restrict set comput social choic prefer incomplet inform structur singlepeak algorithm,9,18.716978,-2.2272987
Forecasting Potential Diabetes Complications,"Yang Yang, Walter Luyten, Lu Liu, Marie-Francine Moens, Juanzi Li and Jie Tang",Applications (APP),"forecast diabetes complications
feature sparseness
sparse factor graph",APP: Biomedical / Bioinformatics,"Diabetes complications often afflict diabetes patients seriously: over 68% of diabetes-related mortality is caused by diabetes complications. In this paper, we study the problem of automatically diagnosing diabetes complications from patients’ lab test results. The objective problem has two main challenges: 1) feature sparseness: a patient only takes 1:26% lab tests on average, and 65:5% types of lab tests are taken by less than 10 patients; 2) knowledge skewness: it lacks comprehensive detailed domain knowledge of association between diabetes complications and lab tests. To address these challenges, we propose a novel probabilistic model called Sparse Factor Graph Model (SparseFGM). SparseFGM projects sparse features onto a lower-dimensional latent space, which alleviates the problem of sparseness. SparseFGM is also able to capture the associations between complications and lab tests, which help handle the knowledge skewness. We evaluate the proposed model on a large collections of real medical records. SparseFGM outperforms (+20% by F1) baselines significantly and gives detailed associations between diabetes complications and lab tests.","Forecasting Potential Diabetes Complications Diabetes complications often afflict diabetes patients seriously: over 68% of diabetes-related mortality is caused by diabetes complications. In this paper, we study the problem of automatically diagnosing diabetes complications from patients’ lab test results. The objective problem has two main challenges: 1) feature sparseness: a patient only takes 1:26% lab tests on average, and 65:5% types of lab tests are taken by less than 10 patients; 2) knowledge skewness: it lacks comprehensive detailed domain knowledge of association between diabetes complications and lab tests. To address these challenges, we propose a novel probabilistic model called Sparse Factor Graph Model (SparseFGM). SparseFGM projects sparse features onto a lower-dimensional latent space, which alleviates the problem of sparseness. SparseFGM is also able to capture the associations between complications and lab tests, which help handle the knowledge skewness. We evaluate the proposed model on a large collections of real medical records. SparseFGM outperforms (+20% by F1) baselines significantly and gives detailed associations between diabetes complications and lab tests. forecast diabetes complications
feature sparseness
sparse factor graph",forecast potenti diabet complic diabet complic often afflict diabet patient serious 68 diabetesrel mortal caus diabet complic paper studi problem automat diagnos diabet complic patient lab test result object problem two main challeng 1 featur spars patient take 126 lab test averag 655 type lab test taken less 10 patient 2 knowledg skew lack comprehens detail domain knowledg associ diabet complic lab test address challeng propos novel probabilist model call spars factor graph model sparsefgm sparsefgm project spars featur onto lowerdimension latent space allevi problem spars sparsefgm also abl captur associ complic lab test help handl knowledg skew evalu propos model larg collect real medic record sparsefgm outperform 20 f1 baselin signific give detail associ diabet complic lab test forecast diabet complic featur spars spars factor graph,4,2.1355715,-0.26109427
Knowledge Graph Embedding by Translating on Hyperplanes,"Zhen Wang, Jianwen Zhang, Jianlin Feng and Zheng Chen","Knowledge Representation and Reasoning (KRR)
Machine Learning Applications (MLA)
NLP and Knowledge Representation (NLPKR)
Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)","Knowledge Embedding
Knowledge Graph
Knowledge Reasoning
Knowledge Completion
Fact Extraction
Representation Learning","KRR: Knowledge Representation (General/Other)
MLA: Machine Learning Applications (General/other)
NLPKR: Semantics and Summarization
NLPTM: Information Extraction
NMLA: Relational/Graph-Based Learning
RU: Uncertainty Representations
RU: Uncertainty in AI (General/Other)","We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently which is very efficient while achieving the state-of-the-art predictive performance. We discuss about some mapping properties of relations which should be considered in embedding, such as symmetric, one-to-many, many-to-one, and many-to-many. We point out that TransE does not do well in dealing with these properties. Some complex models are capable to preserve these mapping properties but sacrificing the efficiency. To make a good trade-off between model capacity and efficiency, in this paper we propose a method where a relation is modeled as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Meanwhile, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on the tasks of link prediction, triplet classification and fact extraction on benchmark data sets from WordNet and Freebase. They show impressive improvements on predictive accuracy and also the capability to scale up.","Knowledge Graph Embedding by Translating on Hyperplanes We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently which is very efficient while achieving the state-of-the-art predictive performance. We discuss about some mapping properties of relations which should be considered in embedding, such as symmetric, one-to-many, many-to-one, and many-to-many. We point out that TransE does not do well in dealing with these properties. Some complex models are capable to preserve these mapping properties but sacrificing the efficiency. To make a good trade-off between model capacity and efficiency, in this paper we propose a method where a relation is modeled as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Meanwhile, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on the tasks of link prediction, triplet classification and fact extraction on benchmark data sets from WordNet and Freebase. They show impressive improvements on predictive accuracy and also the capability to scale up. Knowledge Embedding
Knowledge Graph
Knowledge Reasoning
Knowledge Completion
Fact Extraction
Representation Learning",knowledg graph embed translat hyperplan deal embed larg scale knowledg graph compos entiti relat continu vector space trans promis method propos recent effici achiev stateoftheart predict perform discuss map properti relat consid embed symmetr onetomani manytoon manytomani point trans well deal properti complex model capabl preserv map properti sacrif effici make good tradeoff model capac effici paper propos method relat model hyperplan togeth translat oper way well preserv map properti relat almost model complex trans meanwhil practic knowledg graph often far complet construct negat exampl reduc fals negat label train import util onetomanymanytoon map properti relat propos simpl trick reduc possibl fals negat label conduct extens experi task link predict triplet classif fact extract benchmark data set wordnet freebas show impress improv predict accuraci also capabl scale knowledg embed knowledg graph knowledg reason knowledg complet fact extract represent learn,6,-4.459688,-0.2781244
A Control Dichotomy for Pure Scoring Rules,"Edith Hemaspaandra, Lane A. Hemaspaandra and Henning Schnoor",Game Theory and Economic Paradigms (GTEP),"voting systems
computational social choice
complexity
scoring rules
control of elections
dichotomy theorems",GTEP: Social Choice / Voting,"Scoring systems are an extremely important class of election systems. A length-m (so-called) scoring vector applies only to m-candidate elections. To handle general elections, one must use a family of vectors, one per length.

The most elegant approach to making sure such families are ""family-like"" is the recently introduced notion of (polynomial-time uniform) pure scoring rules (Betzler and Dorn 2010), where each scoring vector is obtained from its precursor by adding one new coefficient.

We obtain the first dichotomy theorem for pure scoring rules for a control problem. In particular, for constructive control by adding voters (CCAV), which is arguably the most important control type, we show that CCAV is solvable in polynomial time for k-approval with k<=3, k-veto with k<=2, every pure scoring rule in which only the two top-rated candidates gain nonzero scores, and a particular rule that is a ""hybrid"" of 1-approval and 1-veto. For all other pure scoring rules, CCAV is NP-complete.

We also investigate the descriptive richness of different models for defining pure scoring rules, proving how more rule-generation time gives more rules, proving that rationals give more rules than do the natural numbers, and proving that some restrictions previously thought to be ""w.l.o.g."" in fact do lose generality.","A Control Dichotomy for Pure Scoring Rules Scoring systems are an extremely important class of election systems. A length-m (so-called) scoring vector applies only to m-candidate elections. To handle general elections, one must use a family of vectors, one per length.

The most elegant approach to making sure such families are ""family-like"" is the recently introduced notion of (polynomial-time uniform) pure scoring rules (Betzler and Dorn 2010), where each scoring vector is obtained from its precursor by adding one new coefficient.

We obtain the first dichotomy theorem for pure scoring rules for a control problem. In particular, for constructive control by adding voters (CCAV), which is arguably the most important control type, we show that CCAV is solvable in polynomial time for k-approval with k<=3, k-veto with k<=2, every pure scoring rule in which only the two top-rated candidates gain nonzero scores, and a particular rule that is a ""hybrid"" of 1-approval and 1-veto. For all other pure scoring rules, CCAV is NP-complete.

We also investigate the descriptive richness of different models for defining pure scoring rules, proving how more rule-generation time gives more rules, proving that rationals give more rules than do the natural numbers, and proving that some restrictions previously thought to be ""w.l.o.g."" in fact do lose generality. voting systems
computational social choice
complexity
scoring rules
control of elections
dichotomy theorems",control dichotomi pure score rule score system extrem import class elect system lengthm socal score vector appli mcandid elect handl general elect one must use famili vector one per length eleg approach make sure famili familylik recent introduc notion polynomialtim uniform pure score rule betzler dorn 2010 score vector obtain precursor ad one new coeffici obtain first dichotomi theorem pure score rule control problem particular construct control ad voter ccav arguabl import control type show ccav solvabl polynomi time kapprov k3 kveto k2 everi pure score rule two toprat candid gain nonzero score particular rule hybrid 1approv 1veto pure score rule ccav npcomplet also investig descript rich differ model defin pure score rule prove rulegener time give rule prove ration give rule natur number prove restrict previous thought wlog fact lose general vote system comput social choic complex score rule control elect dichotomi theorem,9,15.904424,-16.464968
Fast consistency checking of very large real-world RCC-8 constraint  networks using graph partitioning,Charalampos Nikolaou and Manolis Koubarakis,"Knowledge Representation and Reasoning (KRR)
Search and Constraint Satisfaction (SCS)","qualitative spatial reasoning
consistency checking
graph partitioning","KRR: Geometric, Spatial, and Temporal Reasoning
KRR: Qualitative Reasoning
SCS: Constraint Satisfaction","We present a new reasoner for RCC-8 constraint networks, called gp-rcc8, that is based on the patchwork property of path-consistent tractable RCC-8 networks and graph partitioning. We compare gp-rcc8 with state of the art reasoners that are based on constraint propagation and backtracking search as well as one that is based on graph partitioning and SAT solving. Our evaluation considers very large real-world RCC-8 networks and medium-sized synthetic ones, and shows that gp-rcc8
outperforms the other reasoners for these networks, while it is less efficient for smaller networks.","Fast consistency checking of very large real-world RCC-8 constraint  networks using graph partitioning We present a new reasoner for RCC-8 constraint networks, called gp-rcc8, that is based on the patchwork property of path-consistent tractable RCC-8 networks and graph partitioning. We compare gp-rcc8 with state of the art reasoners that are based on constraint propagation and backtracking search as well as one that is based on graph partitioning and SAT solving. Our evaluation considers very large real-world RCC-8 networks and medium-sized synthetic ones, and shows that gp-rcc8
outperforms the other reasoners for these networks, while it is less efficient for smaller networks. qualitative spatial reasoning
consistency checking
graph partitioning",fast consist check larg realworld rcc8 constraint network use graph partit present new reason rcc8 constraint network call gprcc8 base patchwork properti pathconsist tractabl rcc8 network graph partit compar gprcc8 state art reason base constraint propag backtrack search well one base graph partit sat solv evalu consid larg realworld rcc8 network mediums synthet one show gprcc8 outperform reason network less effici smaller network qualit spatial reason consist check graph partit,5,-5.826483,9.787081
Encoding Tree Sparsity in Multi-Task Learning: A Probabilistic Framework,"Lei Han, Yu Zhang, Guojie Song and Kunqing Xie",Novel Machine Learning Algorithms (NMLA),"Multi-Task Learning
Sparsity
Probabilistic Modeling","NMLA: Transfer, Adaptation, Multitask Learning","Multi-task learning seeks to improve the generalization performance by sharing common information among multiple related tasks. A key assumption in most MTL algorithms is that all tasks are related, which, however, may not hold in many real-world applications. Existing techniques, which attempt to address this issue, aim to identify groups of related tasks using group sparsity. In this paper, we propose a probabilistic tree sparsity (PTS) model to utilize the tree structure to obtain the sparse solution instead of the group structure. Specifically, each model coefficient in the learning model is decomposed into a product of multiple component coefficients each of which corresponds to a node in the tree. Based on the decomposition, Gaussian and Cauchy distributions are placed on the component coefficients as priors to restrict the model complexity. We devise an efficient expectation maximization algorithm to learn the model parameters. Experiments conducted on both synthetic and real-world problems show the effectiveness of our model compared with state-of-the-art baselines.","Encoding Tree Sparsity in Multi-Task Learning: A Probabilistic Framework Multi-task learning seeks to improve the generalization performance by sharing common information among multiple related tasks. A key assumption in most MTL algorithms is that all tasks are related, which, however, may not hold in many real-world applications. Existing techniques, which attempt to address this issue, aim to identify groups of related tasks using group sparsity. In this paper, we propose a probabilistic tree sparsity (PTS) model to utilize the tree structure to obtain the sparse solution instead of the group structure. Specifically, each model coefficient in the learning model is decomposed into a product of multiple component coefficients each of which corresponds to a node in the tree. Based on the decomposition, Gaussian and Cauchy distributions are placed on the component coefficients as priors to restrict the model complexity. We devise an efficient expectation maximization algorithm to learn the model parameters. Experiments conducted on both synthetic and real-world problems show the effectiveness of our model compared with state-of-the-art baselines. Multi-Task Learning
Sparsity
Probabilistic Modeling",encod tree sparsiti multitask learn probabilist framework multitask learn seek improv general perform share common inform among multipl relat task key assumpt mtl algorithm task relat howev may hold mani realworld applic exist techniqu attempt address issu aim identifi group relat task use group sparsiti paper propos probabilist tree sparsiti pts model util tree structur obtain spars solut instead group structur specif model coeffici learn model decompos product multipl compon coeffici correspond node tree base decomposit gaussian cauchi distribut place compon coeffici prior restrict model complex devis effici expect maxim algorithm learn model paramet experi conduct synthet realworld problem show effect model compar stateoftheart baselin multitask learn sparsiti probabilist model,4,3.5676754,-13.32847
Efficient Generalized Fused Lasso with Application to the Diagnosis of Alzheimer’s Disease,"Bo Xin, Yoshinubo Kawahara, Yizhou Wang and Wen Gao","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Generalized Fused Lasso
Alzhemier's Disease
Parametric graph cut","MLA: Bio/Medicine
NMLA: Dimension Reduction/Feature Selection
NMLA: Relational/Graph-Based Learning
NMLA: Structured Prediction","Generalized Fused Lasso (GFL) penalizes variables with L1 norms both on variables and their pairwise differences. GFL is useful when applied to data where prior information expressed with a graph is available in the domain. However, the existing algorithms for GFL take high computational cost and do not scale to large size problems. In this paper, we propose a fast and scalable algorithm for GFL. Based on the fact that the fusion penalty is the Lov´asz extension of a cut function, we show that the key building block is equivalent to recursively solving graph cut problems. We then solve GFL efficiently via a parametric flow algorithm. Runtime comparison demonstrate a significant speed-up over existing algorithms for GFL. Benefited from the scalability of our algorithm, we propose to formulate the diagnosis of Alzheimer’s Disease as GFL. Experiments demonstrate that GFL seems to be a natural way to formulate such a problem. Not only is the diagnosis performance promising, but the selected critical voxels are well structured, consistent across tasks and in accordance with clinical prior knowledge.","Efficient Generalized Fused Lasso with Application to the Diagnosis of Alzheimer’s Disease Generalized Fused Lasso (GFL) penalizes variables with L1 norms both on variables and their pairwise differences. GFL is useful when applied to data where prior information expressed with a graph is available in the domain. However, the existing algorithms for GFL take high computational cost and do not scale to large size problems. In this paper, we propose a fast and scalable algorithm for GFL. Based on the fact that the fusion penalty is the Lov´asz extension of a cut function, we show that the key building block is equivalent to recursively solving graph cut problems. We then solve GFL efficiently via a parametric flow algorithm. Runtime comparison demonstrate a significant speed-up over existing algorithms for GFL. Benefited from the scalability of our algorithm, we propose to formulate the diagnosis of Alzheimer’s Disease as GFL. Experiments demonstrate that GFL seems to be a natural way to formulate such a problem. Not only is the diagnosis performance promising, but the selected critical voxels are well structured, consistent across tasks and in accordance with clinical prior knowledge. Generalized Fused Lasso
Alzhemier's Disease
Parametric graph cut",effici general fuse lasso applic diagnosi alzheim diseas general fuse lasso gfl penal variabl l1 norm variabl pairwis differ gfl use appli data prior inform express graph avail domain howev exist algorithm gfl take high comput cost scale larg size problem paper propos fast scalabl algorithm gfl base fact fusion penalti lov´asz extens cut function show key build block equival recurs solv graph cut problem solv gfl effici via parametr flow algorithm runtim comparison demonstr signific speedup exist algorithm gfl benefit scalabl algorithm propos formul diagnosi alzheim diseas gfl experi demonstr gfl seem natur way formul problem diagnosi perform promis select critic voxel well structur consist across task accord clinic prior knowledg general fuse lasso alzhemi diseas parametr graph cut,5,-1.5385818,2.1024578
How Do Your Friends on Social Media Disclose Your Emotions?,"Yang Yang, Jia Jia, Shumei Zhang, Boya Wu, Jie Tang and Juanzi Li",Applications (APP),"emotion inference
images and comments
generative model",APP: Social Networks,"Mining emotions hidden in images has attracted significant interest, in particular with the rapid development of social networks. The emotional impact is very important for understanding the intrinsic meanings of images. Despite many studies have been done, most existing methods focus on image content, but ignore the emotions of the user who has published the image. To understand the emotional impact from images, one interesting question is: How does social effect correlate with the emotion expressed in an image? Specifically, can we leverage friends interactions (e.g., discussions) related to an image to help discover the emotions? In this paper, we formally formalize the problem and propose a novel emotion learning method by jointly modeling images posted by social users and comments added by friends. One advantage of the model is that it can distinguish those comments that are closely related to the emotion expression for an image from other irrelevant ones. Experiments on an open Flickr dataset show that the proposed model can significantly improve (+37.4% by F1) the accuracy for inferring emotions from images. More interestingly, we found that half of the improvements are due to interactions between 1% of the closest friends.","How Do Your Friends on Social Media Disclose Your Emotions? Mining emotions hidden in images has attracted significant interest, in particular with the rapid development of social networks. The emotional impact is very important for understanding the intrinsic meanings of images. Despite many studies have been done, most existing methods focus on image content, but ignore the emotions of the user who has published the image. To understand the emotional impact from images, one interesting question is: How does social effect correlate with the emotion expressed in an image? Specifically, can we leverage friends interactions (e.g., discussions) related to an image to help discover the emotions? In this paper, we formally formalize the problem and propose a novel emotion learning method by jointly modeling images posted by social users and comments added by friends. One advantage of the model is that it can distinguish those comments that are closely related to the emotion expression for an image from other irrelevant ones. Experiments on an open Flickr dataset show that the proposed model can significantly improve (+37.4% by F1) the accuracy for inferring emotions from images. More interestingly, we found that half of the improvements are due to interactions between 1% of the closest friends. emotion inference
images and comments
generative model",friend social media disclos emot mine emot hidden imag attract signific interest particular rapid develop social network emot impact import understand intrins mean imag despit mani studi done exist method focus imag content ignor emot user publish imag understand emot impact imag one interest question social effect correl emot express imag specif leverag friend interact eg discuss relat imag help discov emot paper formal formal problem propos novel emot learn method joint model imag post social user comment ad friend one advantag model distinguish comment close relat emot express imag irrelev one experi open flickr dataset show propos model signific improv 374 f1 accuraci infer emot imag interest found half improv due interact 1 closest friend emot infer imag comment generat model,1,-22.523474,-5.423901
Contextually Supervised Source Separation with Application to Energy Disaggregation,Matt Wytock and Zico Kolter,"Computational Sustainability and AI (CSAI)
Machine Learning Applications (MLA)","energy disaggregation
convex optimization
source separation","CSAI: Modeling and prediction of dynamic and spatiotemporal phenomena and systems
MLA: Environmental
MLA: Applications of Unsupervised Learning","We propose a new framework for single-channel source separation that lies
between the fully supervised and unsupervised setting. Instead of supervision,
we provide input features for each source signal and use convex methods to
estimate the correlations between these features and the unobserved signal
decomposition. Contextually supervised source separation is a natural fit for
domains with large amounts of data but no explicit supervision; our motivating
application is energy disaggregation of hourly smart meter data (the separation
of whole-home power signals into different energy uses). Here contextual
supervision allows us to provide itemized energy usage for thousands homes, a task
previously impossible due to the need for specialized data collection hardware.
On smaller datasets which include labels, we demonstrate that contextual
supervision improves significantly over a reasonable baseline and existing
unsupervised methods for source separation. Finally, we analyze the case of
$\ell_2$ loss theoretically and show that recovery of the signal components
depends only on cross-correlation between features for different signals, not on
correlations between features for the same signal.","Contextually Supervised Source Separation with Application to Energy Disaggregation We propose a new framework for single-channel source separation that lies
between the fully supervised and unsupervised setting. Instead of supervision,
we provide input features for each source signal and use convex methods to
estimate the correlations between these features and the unobserved signal
decomposition. Contextually supervised source separation is a natural fit for
domains with large amounts of data but no explicit supervision; our motivating
application is energy disaggregation of hourly smart meter data (the separation
of whole-home power signals into different energy uses). Here contextual
supervision allows us to provide itemized energy usage for thousands homes, a task
previously impossible due to the need for specialized data collection hardware.
On smaller datasets which include labels, we demonstrate that contextual
supervision improves significantly over a reasonable baseline and existing
unsupervised methods for source separation. Finally, we analyze the case of
$\ell_2$ loss theoretically and show that recovery of the signal components
depends only on cross-correlation between features for different signals, not on
correlations between features for the same signal. energy disaggregation
convex optimization
source separation",contextu supervis sourc separ applic energi disaggreg propos new framework singlechannel sourc separ lie fulli supervis unsupervis set instead supervis provid input featur sourc signal use convex method estim correl featur unobserv signal decomposit contextu supervis sourc separ natur fit domain larg amount data explicit supervis motiv applic energi disaggreg hour smart meter data separ wholehom power signal differ energi use contextu supervis allow us provid item energi usag thousand home task previous imposs due need special data collect hardwar smaller dataset includ label demonstr contextu supervis improv signific reason baselin exist unsupervis method sourc separ final analyz case ell2 loss theoret show recoveri signal compon depend crosscorrel featur differ signal correl featur signal energi disaggreg convex optim sourc separ,6,-5.3546133,-2.5997639
The Computational Complexity of Structure-Based Causality,"Hana Chockler, Gadi Aleksandrowicz, Joseph Y. Halpern and Alexander Ivrii","Knowledge Representation and Reasoning (KRR)
Reasoning under Uncertainty (RU)","Knowledge Representation and Reasoning
Causal Models
Structural Causality
Complexity
Strong Cause
Actual Cause
Polynomial Hierarchy","KRR: Action, Change, and Causality
KRR: Computational Complexity of Reasoning
KRR: Qualitative Reasoning
KRR: Reasoning with Beliefs
RU: Uncertainty Representations","Halpern and Pearl 2001 introduced a definition of
actual causality; Eiter and Lukasiewicz 2001 showed that
computing whether X=x is a cause of Y=y is NP-complete in binary
models (where all variables can take on only two values) and
Sigma_2^P-complete in general models.  In the final version of their
paper, Halpern and Pearl (2005) slightly modified the definition of
actual cause, in order to deal with problems pointed by Hopkins and
Pearl in 2003.  As we show, this modification has a
nontrivial impact on the complexity of computing actual cause.
To characterize the complexity, a new family D_k, k= 1, 2, 3, ...
is introduced, which generalizes the class D^P introduced by
Papadimitriou and Yannakakis (1984)  (D^P is just D_1.)
We show that the complexity of computing causality is D_2-complete
under the new definition.  Chockler and Halpern 2004 extended the
definition of causality by introducing notions of responsibility
and blame. They characterized the complexity of determining the
degree of responsibility and blame using the original definition of
causality.  Again, we show that changing the definition of causality
affects the complexity, and completely characterize the complexity of
determining the degree of responsibility and blame with the new definition.","The Computational Complexity of Structure-Based Causality Halpern and Pearl 2001 introduced a definition of
actual causality; Eiter and Lukasiewicz 2001 showed that
computing whether X=x is a cause of Y=y is NP-complete in binary
models (where all variables can take on only two values) and
Sigma_2^P-complete in general models.  In the final version of their
paper, Halpern and Pearl (2005) slightly modified the definition of
actual cause, in order to deal with problems pointed by Hopkins and
Pearl in 2003.  As we show, this modification has a
nontrivial impact on the complexity of computing actual cause.
To characterize the complexity, a new family D_k, k= 1, 2, 3, ...
is introduced, which generalizes the class D^P introduced by
Papadimitriou and Yannakakis (1984)  (D^P is just D_1.)
We show that the complexity of computing causality is D_2-complete
under the new definition.  Chockler and Halpern 2004 extended the
definition of causality by introducing notions of responsibility
and blame. They characterized the complexity of determining the
degree of responsibility and blame using the original definition of
causality.  Again, we show that changing the definition of causality
affects the complexity, and completely characterize the complexity of
determining the degree of responsibility and blame with the new definition. Knowledge Representation and Reasoning
Causal Models
Structural Causality
Complexity
Strong Cause
Actual Cause
Polynomial Hierarchy",comput complex structurebas causal halpern pearl 2001 introduc definit actual causal eiter lukasiewicz 2001 show comput whether xx caus yy npcomplet binari model variabl take two valu sigma2pcomplet general model final version paper halpern pearl 2005 slight modifi definit actual caus order deal problem point hopkin pearl 2003 show modif nontrivi impact complex comput actual caus character complex new famili dk k 1 2 3 introduc general class dp introduc papadimitriou yannakaki 1984 dp d1 show complex comput causal d2complet new definit chockler halpern 2004 extend definit causal introduc notion respons blame character complex determin degre respons blame use origin definit causal show chang definit causal affect complex complet character complex determin degre respons blame new definit knowledg represent reason causal model structur causal complex strong caus actual caus polynomi hierarchi,9,16.29342,-4.75462
Biased Games,"Ioannis Caragiannis, David Kurokawa and Ariel Procaccia",Game Theory and Economic Paradigms (GTEP),"Equilibrium existence
Equilibrium computation
Solutions concepts","GTEP: Game Theory
GTEP: Equilibrium","We present a novel extension of normal form games that we call biased games. In these games, a player's utility is influenced by the distance between his mixed strategy and a given base strategy. We argue that biased games capture important aspects of the interaction between software agents. Our main result is that biased games satisfying certain mild conditions always admit an equilibrium. We also tackle the computation of equilibria in biased games.","Biased Games We present a novel extension of normal form games that we call biased games. In these games, a player's utility is influenced by the distance between his mixed strategy and a given base strategy. We argue that biased games capture important aspects of the interaction between software agents. Our main result is that biased games satisfying certain mild conditions always admit an equilibrium. We also tackle the computation of equilibria in biased games. Equilibrium existence
Equilibrium computation
Solutions concepts",bias game present novel extens normal form game call bias game game player util influenc distanc mix strategi given base strategi argu bias game captur import aspect interact softwar agent main result bias game satisfi certain mild condit alway admit equilibrium also tackl comput equilibria bias game equilibrium exist equilibrium comput solut concept,2,7.3577843,19.289843
Grounding Acoustic Echoes In Single View Geometry Estimation,"Muhammad Wajahat Hussain, Javier Civera and Luis Montano","Knowledge Representation and Reasoning (KRR)
Machine Learning Applications (MLA)
Vision (VIS)","Scene layout
Scene understanding
Acoustic echoes
Room geometry","KRR: Geometric, Spatial, and Temporal Reasoning
MLA: Applications of Supervised Learning
MLA: Machine Learning Applications (General/other)
VIS: Perception","Extracting the 3D geometry plays an important part in scene understanding. Recently, structured prediction-based, robust visual descriptors are proposed for extracting the indoor scene layout from a passive agent’s perspective, i.e., single image. This robustness is mainly due to modeling the physical interaction of the underlying room geometry with the objects and the humans present in the room. In this work we add the physical constraints coming from acoustic echoes, generated by an audio source, to this visual model. Our audio-visual 3D geometry descriptor improves over the state of the art in passive perception models as shown by experiments.","Grounding Acoustic Echoes In Single View Geometry Estimation Extracting the 3D geometry plays an important part in scene understanding. Recently, structured prediction-based, robust visual descriptors are proposed for extracting the indoor scene layout from a passive agent’s perspective, i.e., single image. This robustness is mainly due to modeling the physical interaction of the underlying room geometry with the objects and the humans present in the room. In this work we add the physical constraints coming from acoustic echoes, generated by an audio source, to this visual model. Our audio-visual 3D geometry descriptor improves over the state of the art in passive perception models as shown by experiments. Scene layout
Scene understanding
Acoustic echoes
Room geometry",ground acoust echo singl view geometri estim extract 3d geometri play import part scene understand recent structur predictionbas robust visual descriptor propos extract indoor scene layout passiv agent perspect ie singl imag robust main due model physic interact under room geometri object human present room work add physic constraint come acoust echo generat audio sourc visual model audiovisu 3d geometri descriptor improv state art passiv percept model shown experi scene layout scene understand acoust echo room geometri,1,6.486,-4.898737
Preference Elicitation and Interview Minimization in Stable Matchings,Joanna Drummond and Craig Boutilier,"Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Stable Matching
Preference Elicitation
Computational Social Choice","APP: Computational Social Science
GTEP: Social Choice / Voting
KRR: Preferences
RU: Decision/Utility Theory","While stable matching problems are widely studied, little work has investigated schemes for effectively eliciting agent preferences using either preference (e.g., comparison) queries or interviews (to form such comparisons); and no work has addressed how to combine both. We develop a new model for representing and assessing agent preferences that accommodates both forms of information and (heuristically) minimizes the number of queries and interviews required to determine a stable matching. Our Refine-then-Interview (RtI) scheme uses coarse preference queries to refine knowledge of agent preferences and relies on interviews only to assess comparisons of relatively ""close"" options. Empirical results show RtI to compare favorably to a recent pure interview minimization algorithm, and that the number of interviews is generally independent of the size of the market.","Preference Elicitation and Interview Minimization in Stable Matchings While stable matching problems are widely studied, little work has investigated schemes for effectively eliciting agent preferences using either preference (e.g., comparison) queries or interviews (to form such comparisons); and no work has addressed how to combine both. We develop a new model for representing and assessing agent preferences that accommodates both forms of information and (heuristically) minimizes the number of queries and interviews required to determine a stable matching. Our Refine-then-Interview (RtI) scheme uses coarse preference queries to refine knowledge of agent preferences and relies on interviews only to assess comparisons of relatively ""close"" options. Empirical results show RtI to compare favorably to a recent pure interview minimization algorithm, and that the number of interviews is generally independent of the size of the market. Stable Matching
Preference Elicitation
Computational Social Choice",prefer elicit interview minim stabl match stabl match problem wide studi littl work investig scheme effect elicit agent prefer use either prefer eg comparison queri interview form comparison work address combin develop new model repres assess agent prefer accommod form inform heurist minim number queri interview requir determin stabl match refinetheninterview rti scheme use coars prefer queri refin knowledg agent prefer reli interview assess comparison relat close option empir result show rti compar favor recent pure interview minim algorithm number interview general independ size market stabl match prefer elicit comput social choic,9,18.932049,0.31598997
Placement of Loading Stations for Electric Vehicles: No Detours Necessary!,"Stefan Funke, André Nusser and Sabine Storandt",Computational Sustainability and AI (CSAI),"energy efficiency
electric vehicles
facility location","CSAI: Control and optimization of dynamic and spatiotemporal systems
CSAI: Network modeling, prediction, and optimization.","Compared to conventional cars, electric vehicles still suffer from a considerably shorter cruising range. Combined with the sparsity of battery loading stations, the complete transition to E-mobility still seems a long way to go. In this paper, we consider the problem of placing as few loading stations as possible such that on any shortest path there are enough to guarantee sufficient energy supply. This means, that EV owners no longer have to plan their trips ahead incorporating loading station positions, and are no longer forced to accept long detours to reach their destinations. We show how to model this problem and introduce heuristics which provide close-to-optimal solutions even in large road networks.","Placement of Loading Stations for Electric Vehicles: No Detours Necessary! Compared to conventional cars, electric vehicles still suffer from a considerably shorter cruising range. Combined with the sparsity of battery loading stations, the complete transition to E-mobility still seems a long way to go. In this paper, we consider the problem of placing as few loading stations as possible such that on any shortest path there are enough to guarantee sufficient energy supply. This means, that EV owners no longer have to plan their trips ahead incorporating loading station positions, and are no longer forced to accept long detours to reach their destinations. We show how to model this problem and introduce heuristics which provide close-to-optimal solutions even in large road networks. energy efficiency
electric vehicles
facility location",placement load station electr vehicl detour necessari compar convent car electr vehicl still suffer consider shorter cruis rang combin sparsiti batteri load station complet transit emobl still seem long way go paper consid problem place load station possibl shortest path enough guarante suffici energi suppli mean ev owner longer plan trip ahead incorpor load station posit longer forc accept long detour reach destin show model problem introduc heurist provid closetooptim solut even larg road network energi effici electr vehicl facil locat,5,7.272037,10.169393
Extracting Keyphrases from Research Papers using Citation Networks,Sujatha Das Gollapalli and Cornelia Caragea,NLP and Text Mining (NLPTM),"CiteTextRank
Citation Network
PageRank","NLPTM: Information Extraction
NLPTM: Natural Language Processing (General/Other)","Keyphrases for a document concisely describe the document using a small set of 
phrases. Keyphrases have been previously
shown to improve several document processing and retrieval tasks. In this work, we study keyphrase extraction from research papers by leveraging citation networks. 
We propose CiteTextRank for extracting keyphrases,
a graph-based algorithm that incorporates evidence from 
both a document's content as well as the contexts
in which the document is referenced within the citation network. Our model obtains significant 
improvements over the state-of-the-art models for this task. Specifically, on several datasets of research papers, 
CiteTextRank improves precision at rank $1$ by as much as 16-60\%","Extracting Keyphrases from Research Papers using Citation Networks Keyphrases for a document concisely describe the document using a small set of 
phrases. Keyphrases have been previously
shown to improve several document processing and retrieval tasks. In this work, we study keyphrase extraction from research papers by leveraging citation networks. 
We propose CiteTextRank for extracting keyphrases,
a graph-based algorithm that incorporates evidence from 
both a document's content as well as the contexts
in which the document is referenced within the citation network. Our model obtains significant 
improvements over the state-of-the-art models for this task. Specifically, on several datasets of research papers, 
CiteTextRank improves precision at rank $1$ by as much as 16-60\% CiteTextRank
Citation Network
PageRank",extract keyphras research paper use citat network keyphras document concis describ document use small set phrase keyphras previous shown improv sever document process retriev task work studi keyphras extract research paper leverag citat network propos citetextrank extract keyphras graphbas algorithm incorpor evid document content well context document referenc within citat network model obtain signific improv stateoftheart model task specif sever dataset research paper citetextrank improv precis rank 1 much 1660 citetextrank citat network pagerank,0,8.173188,2.817659
Learning Parametric Models for Social Infectivity in Multi-dimensional Hawkes Processes,Liangda Li and Hongyuan Zha,"AI and the Web (AIW)
Applications (APP)
Machine Learning Applications (MLA)","Social infectivity
diffusion network
Hawkes process
time-varying feature","AIW: Machine learning and the web
AIW: Social networking and community identification
APP: Social Networks
MLA: Networks
NMLA: Time-Series/Data Streams","Efficient and effective learning of social infectivity is a critical challenge in modeling diffusion phenomenons in social networks and other applications.
Existing methods require substantial amount of event cascades to guarantee the learning accuracy, while only time-invariant infectivity is considered.
Our paper overcomes those two drawbacks by constructing a more compact model and parameterizing the infectivity using time-varying features, thus dramatically reduces the data requirement, and enable the learning of time-varying infectivity which also takes into account the underlying network topology.
We replace the pairwise infectivity in the multidimensional Hawkes processes with linear combinations of those time-varying features, and optimize the associated coefficients with lasso regularization on coefficients. To efficiently solve the resulting optimization problem, we employ the technique of alternating direction method of multipliers, and under that framework update each coefficient independently, by optimizing a surrogate function which upper-bounds the original objective function. On both synthetic and real world data, the proposed method performs better than alternatives in terms of both recovering the hidden diffusion network and predicting the occurrence time of social events.","Learning Parametric Models for Social Infectivity in Multi-dimensional Hawkes Processes Efficient and effective learning of social infectivity is a critical challenge in modeling diffusion phenomenons in social networks and other applications.
Existing methods require substantial amount of event cascades to guarantee the learning accuracy, while only time-invariant infectivity is considered.
Our paper overcomes those two drawbacks by constructing a more compact model and parameterizing the infectivity using time-varying features, thus dramatically reduces the data requirement, and enable the learning of time-varying infectivity which also takes into account the underlying network topology.
We replace the pairwise infectivity in the multidimensional Hawkes processes with linear combinations of those time-varying features, and optimize the associated coefficients with lasso regularization on coefficients. To efficiently solve the resulting optimization problem, we employ the technique of alternating direction method of multipliers, and under that framework update each coefficient independently, by optimizing a surrogate function which upper-bounds the original objective function. On both synthetic and real world data, the proposed method performs better than alternatives in terms of both recovering the hidden diffusion network and predicting the occurrence time of social events. Social infectivity
diffusion network
Hawkes process
time-varying feature",learn parametr model social infect multidimension hawk process effici effect learn social infect critic challeng model diffus phenomenon social network applic exist method requir substanti amount event cascad guarante learn accuraci timeinvari infect consid paper overcom two drawback construct compact model parameter infect use timevari featur thus dramat reduc data requir enabl learn timevari infect also take account under network topolog replac pairwis infect multidimension hawk process linear combin timevari featur optim associ coeffici lasso regular coeffici effici solv result optim problem employ techniqu altern direct method multipli framework updat coeffici independ optim surrog function upperbound origin object function synthet real world data propos method perform better altern term recov hidden diffus network predict occurr time social event social infect diffus network hawk process timevari featur,5,11.506545,5.134756
Sub-Selective Quantization for Large-Scale Image Search,"Yeqing Li, Chen Chen, Wei Liu and Junzhou Huang","Machine Learning Applications (MLA)
Vision (VIS)","subselection
image retrieval
binary embedding
image hashing
similarity search",VIS: Image and Video Retrieval,"Recently with the explosive growth of visual content on the Internet, large-scale image search has attracted intensive attention. It has been shown that mapping high-dimensional image descriptors to compact binary codes can lead to considerable efficiency gains in both storage and similarity computation of images. However, most existing methods still suffer from expensive training devoted to large-scale binary code learning. To address this issue, we propose a sub-selection based matrix manipulation algorithm which can significantly reduce the computational cost of code learning. As case studies, we apply the sub-selection algorithm to two popular quantization techniques PCA Quantization (PCAQ) and Iterative Quantization (ITQ). Crucially, we can justify the resulting sub-selective quantization by proving its theoretic properties. Extensive experiments are carried out on three image benchmarks with up to one million samples, corroborating the efficacy of the sub-selective quantization method in terms of image retrieval.","Sub-Selective Quantization for Large-Scale Image Search Recently with the explosive growth of visual content on the Internet, large-scale image search has attracted intensive attention. It has been shown that mapping high-dimensional image descriptors to compact binary codes can lead to considerable efficiency gains in both storage and similarity computation of images. However, most existing methods still suffer from expensive training devoted to large-scale binary code learning. To address this issue, we propose a sub-selection based matrix manipulation algorithm which can significantly reduce the computational cost of code learning. As case studies, we apply the sub-selection algorithm to two popular quantization techniques PCA Quantization (PCAQ) and Iterative Quantization (ITQ). Crucially, we can justify the resulting sub-selective quantization by proving its theoretic properties. Extensive experiments are carried out on three image benchmarks with up to one million samples, corroborating the efficacy of the sub-selective quantization method in terms of image retrieval. subselection
image retrieval
binary embedding
image hashing
similarity search",subselect quantize largescal imag search recent explos growth visual content internet largescal imag search attract intens attent shown map highdimension imag descriptor compact binari code lead consider effici gain storag similar comput imag howev exist method still suffer expens train devot largescal binari code learn address issu propos subselect base matrix manipul algorithm signific reduc comput cost code learn case studi appli subselect algorithm two popular quantize techniqu pca quantize pcaq iter quantize itq crucial justifi result subselect quantize prove theoret properti extens experi carri three imag benchmark one million sampl corrobor efficaci subselect quantize method term imag retriev subselect imag retriev binari embed imag hash similar search,1,-21.93886,-7.0409603
Accurate Integration of Aerosol Predictions by Smoothing on a Manifold,Shuai Zheng and James Kwok,Machine Learning Applications (MLA),"aerosol optical depth
manifold
Gaussian random field",MLA: Environmental,"Accurately measuring the aerosol optical depth (AOD) is essential for our understanding of the climate. Currently, AOD can be measured by (i) satellite instruments, which operate on a global scale but have limited accuracies; and (ii) ground-based instruments, which are more accurate but not widely available. Recent approaches focus on integrating measurements from these two sources to complement each other. In this paper,  we further improve the prediction accuracy by using the observation that the AOD varies slowly in the spatial domain. Using a probabilistic approach, we impose this smoothness constraint by a Gaussian random field on the Earth's surface, which can be considered as a two-dimensional manifold. The proposed integration approach is computationally simple, and experimental results on both synthetic and real-world data sets show that it significantly outperforms the state-of-the-art.","Accurate Integration of Aerosol Predictions by Smoothing on a Manifold Accurately measuring the aerosol optical depth (AOD) is essential for our understanding of the climate. Currently, AOD can be measured by (i) satellite instruments, which operate on a global scale but have limited accuracies; and (ii) ground-based instruments, which are more accurate but not widely available. Recent approaches focus on integrating measurements from these two sources to complement each other. In this paper,  we further improve the prediction accuracy by using the observation that the AOD varies slowly in the spatial domain. Using a probabilistic approach, we impose this smoothness constraint by a Gaussian random field on the Earth's surface, which can be considered as a two-dimensional manifold. The proposed integration approach is computationally simple, and experimental results on both synthetic and real-world data sets show that it significantly outperforms the state-of-the-art. aerosol optical depth
manifold
Gaussian random field",accur integr aerosol predict smooth manifold accur measur aerosol optic depth aod essenti understand climat current aod measur satellit instrument oper global scale limit accuraci ii groundbas instrument accur wide avail recent approach focus integr measur two sourc complement paper improv predict accuraci use observ aod vari slowli spatial domain use probabilist approach impos smooth constraint gaussian random field earth surfac consid twodimension manifold propos integr approach comput simpl experiment result synthet realworld data set show signific outperform stateoftheart aerosol optic depth manifold gaussian random field,0,-1.1701901,-0.9690599
Leveraging Decomposed Trust in Probabilistic Matrix Factorization for Effective Recommendation,"Hui Fang, Yang Bao and Jie Zhang",AI and the Web (AIW),"multi-facet trust
trust theory
probabilistic matrix factorization
rating prediction","AIW: Representing, reasoning, and using provenance, trust, privacy, and security on the web
AIW: Web-based recommendation systems","Trust has been used to replace or complement rating-based similarity in recommender systems, to improve the accuracy of rating prediction. However, people trusting each other may not always share similar preferences. In this paper, we try to fill in this gap by decomposing the original single-aspect trust information into four general trust aspects, i.e. benevolence, integrity, competence, and predictability, and further employing the support vector regression technique to incorporate them into the probabilistic matrix factorization model for rating prediction in recommender systems. Experimental results on four datasets demonstrate the superiority of our method over the state-of-the-art approaches.","Leveraging Decomposed Trust in Probabilistic Matrix Factorization for Effective Recommendation Trust has been used to replace or complement rating-based similarity in recommender systems, to improve the accuracy of rating prediction. However, people trusting each other may not always share similar preferences. In this paper, we try to fill in this gap by decomposing the original single-aspect trust information into four general trust aspects, i.e. benevolence, integrity, competence, and predictability, and further employing the support vector regression technique to incorporate them into the probabilistic matrix factorization model for rating prediction in recommender systems. Experimental results on four datasets demonstrate the superiority of our method over the state-of-the-art approaches. multi-facet trust
trust theory
probabilistic matrix factorization
rating prediction",leverag decompos trust probabilist matrix factor effect recommend trust use replac complement ratingbas similar recommend system improv accuraci rate predict howev peopl trust may alway share similar prefer paper tri fill gap decompos origin singleaspect trust inform four general trust aspect ie benevol integr compet predict employ support vector regress techniqu incorpor probabilist matrix factor model rate predict recommend system experiment result four dataset demonstr superior method stateoftheart approach multifacet trust trust theori probabilist matrix factor rate predict,0,15.724367,-10.889024
Multi-Instance Learning with Distribution Change,Wei-Jia Zhang and Zhi-Hua Zhou,Novel Machine Learning Algorithms (NMLA),"multi-instance learning
covariate shift
distribution change
importance sampling",NMLA: Classification,"Multi-instance learning deals with tasks where each example is a bag of instances, and the bag labels of training data are known whereas instance labels are unknown. Most previous studies on multi-instance learning assumed that the training and testing data are from the same distribution; however, this assumption is often violated in real tasks. In this paper, we present possibly the first study on multi-instance learning with distribution change. We propose the MICS approach by considering both bag-level distribution change and instance-level distribution change. Experiments show that MICS is almost always significantly better than many state-of-the-art multi-instance learning approaches when distribution change occurs, and even when there is no distribution change, it is still highly competitive.","Multi-Instance Learning with Distribution Change Multi-instance learning deals with tasks where each example is a bag of instances, and the bag labels of training data are known whereas instance labels are unknown. Most previous studies on multi-instance learning assumed that the training and testing data are from the same distribution; however, this assumption is often violated in real tasks. In this paper, we present possibly the first study on multi-instance learning with distribution change. We propose the MICS approach by considering both bag-level distribution change and instance-level distribution change. Experiments show that MICS is almost always significantly better than many state-of-the-art multi-instance learning approaches when distribution change occurs, and even when there is no distribution change, it is still highly competitive. multi-instance learning
covariate shift
distribution change
importance sampling",multiinst learn distribut chang multiinst learn deal task exampl bag instanc bag label train data known wherea instanc label unknown previous studi multiinst learn assum train test data distribut howev assumpt often violat real task paper present possibl first studi multiinst learn distribut chang propos mic approach consid baglevel distribut chang instancelevel distribut chang experi show mic almost alway signific better mani stateoftheart multiinst learn approach distribut chang occur even distribut chang still high competit multiinst learn covari shift distribut chang import sampl,6,-6.3379593,-10.861876
Minimising Undesired Task Costs in Multi-robot Task Allocation Problems with In-Schedule Dependencies,Bradford Heap and Maurice Pagnucco,"Multiagent Systems (MAS)
Robotics (ROB)","Multi-Robot Task Allocation
Distributed Auctions
Multi-Agent Systems
Multi-Robot Systems
Autonomous Systems
Autonomous Robotics
Market-Based Systems","GTEP: Auctions and Market-Based Systems
MAS: Coordination and Collaboration
MAS: Distributed Problem Solving
ROB: Multi-Robot Systems","In multi-robot task allocation problems with in-schedule dependencies, tasks with high costs have a large influence on the overall time for a robot team to complete all tasks. 
We reduce this influence by calculating a novel task cost dispersion value which measures robots' collective preference for each task. 
By modifying the winner determination phase of sequential single-item auctions, our approach inspects the bids for every task to identify tasks which robots collectively consider to be high cost and we ensure these tasks are allocated before other tasks.
We show empirically this method reduces the overall time taken to complete all tasks.","Minimising Undesired Task Costs in Multi-robot Task Allocation Problems with In-Schedule Dependencies In multi-robot task allocation problems with in-schedule dependencies, tasks with high costs have a large influence on the overall time for a robot team to complete all tasks. 
We reduce this influence by calculating a novel task cost dispersion value which measures robots' collective preference for each task. 
By modifying the winner determination phase of sequential single-item auctions, our approach inspects the bids for every task to identify tasks which robots collectively consider to be high cost and we ensure these tasks are allocated before other tasks.
We show empirically this method reduces the overall time taken to complete all tasks. Multi-Robot Task Allocation
Distributed Auctions
Multi-Agent Systems
Multi-Robot Systems
Autonomous Systems
Autonomous Robotics
Market-Based Systems",minimis undesir task cost multirobot task alloc problem inschedul depend multirobot task alloc problem inschedul depend task high cost larg influenc overal time robot team complet task reduc influenc calcul novel task cost dispers valu measur robot collect prefer task modifi winner determin phase sequenti singleitem auction approach inspect bid everi task identifi task robot collect consid high cost ensur task alloc task show empir method reduc overal time taken complet task multirobot task alloc distribut auction multiag system multirobot system autonom system autonom robot marketbas system,3,-20.616722,9.9248495
A Machine Learning Approach to Musically Meaningful Homogeneous Style Classification,"William Herlands, Ricky Der, Yoel Greenberg and Simon Levin",Machine Learning Applications (MLA),"supervised machine learning
music
information retrieval
style classification
classical music","MLA: Humanities
MLA: Applications of Supervised Learning
MLA: Machine Learning Applications (General/other)","Recent literature has demonstrated the difficulty of classifying between composers who write in extremely similar styles (homogeneous style). Additionally, machine learning studies in this field have been exclusively of technical import with little musicological interpretability or significance. We present a supervised machine learning system which addresses the difficulty of differentiating between stylistically homogeneous composers using foundational elements of music, their complexity and interaction. Our work expands on previous style classification studies by developing more complex features as well as introducing a new class of musical features which focus on local irregularities within musical scores. We demonstrate the discriminative power of the system as applied to Haydn and Mozart's string quartets. Our results yield interpretable musicological conclusions about Haydn's and Mozart's stylistic differences while distinguishing between the composers with higher accuracy than previous studies in this domain.","A Machine Learning Approach to Musically Meaningful Homogeneous Style Classification Recent literature has demonstrated the difficulty of classifying between composers who write in extremely similar styles (homogeneous style). Additionally, machine learning studies in this field have been exclusively of technical import with little musicological interpretability or significance. We present a supervised machine learning system which addresses the difficulty of differentiating between stylistically homogeneous composers using foundational elements of music, their complexity and interaction. Our work expands on previous style classification studies by developing more complex features as well as introducing a new class of musical features which focus on local irregularities within musical scores. We demonstrate the discriminative power of the system as applied to Haydn and Mozart's string quartets. Our results yield interpretable musicological conclusions about Haydn's and Mozart's stylistic differences while distinguishing between the composers with higher accuracy than previous studies in this domain. supervised machine learning
music
information retrieval
style classification
classical music",machin learn approach music meaning homogen style classif recent literatur demonstr difficulti classifi compos write extrem similar style homogen style addit machin learn studi field exclus technic import littl musicolog interpret signific present supervis machin learn system address difficulti differenti stylist homogen compos use foundat element music complex interact work expand previous style classif studi develop complex featur well introduc new class music featur focus local irregular within music score demonstr discrimin power system appli haydn mozart string quartet result yield interpret musicolog conclus haydn mozart stylist differ distinguish compos higher accuraci previous studi domain supervis machin learn music inform retriev style classif classic music,6,-4.621823,-4.158504
Learning Deep Representations for Graph Clustering,"Fei Tian, Bin Gao, Enhong Chen and Tie-Yan Liu","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","deep representations
clustering on graph
neural networks","MLA: Applications of Unsupervised Learning
NMLA: Clustering
NMLA: Neural Networks/Deep Learning
NMLA: Relational/Graph-Based Learning","Recently deep learning has been successfully adopted in many applications such as speech recognition, image classification, and natural language processing. In this work, we explore the possibility of employing deep learning in graph clustering. In particular, we propose a simple method, which first learns a nonlinear embedding of the original graph by stacked autoencoder, and then runs a $k$-means algorithm on the embedding to obtain the clustering result. We show that this simple method has a solid theoretical foundation, due to the equivalence between autoencoder and spectral clustering in terms of what they actually optimize. Then, we demonstrate that the proposed method is more efficient and flexible than spectral clustering. First, the computational complexity of autoencoder is much lower than spectral clustering: the former can be linear to the number of nodes in a sparse graph while the latter is super quadratic due to its dependency on an eigenvalue decomposition. Second, when additional constraints like sparsity is imposed, we can simply employ the \emph{sparse} autoencoder developed in the literature of deep learning; however, it is non-straightforward to implement a sparse spectral method. We have conducted comprehensive experiments to test the performance of the proposed method. The results on various graph datasets show that it can significantly outperform the conventional spectral clustering method. This clearly indicates the effectiveness of deep learning in graph clustering, and enriches our understanding on the power of deep learning in general.","Learning Deep Representations for Graph Clustering Recently deep learning has been successfully adopted in many applications such as speech recognition, image classification, and natural language processing. In this work, we explore the possibility of employing deep learning in graph clustering. In particular, we propose a simple method, which first learns a nonlinear embedding of the original graph by stacked autoencoder, and then runs a $k$-means algorithm on the embedding to obtain the clustering result. We show that this simple method has a solid theoretical foundation, due to the equivalence between autoencoder and spectral clustering in terms of what they actually optimize. Then, we demonstrate that the proposed method is more efficient and flexible than spectral clustering. First, the computational complexity of autoencoder is much lower than spectral clustering: the former can be linear to the number of nodes in a sparse graph while the latter is super quadratic due to its dependency on an eigenvalue decomposition. Second, when additional constraints like sparsity is imposed, we can simply employ the \emph{sparse} autoencoder developed in the literature of deep learning; however, it is non-straightforward to implement a sparse spectral method. We have conducted comprehensive experiments to test the performance of the proposed method. The results on various graph datasets show that it can significantly outperform the conventional spectral clustering method. This clearly indicates the effectiveness of deep learning in graph clustering, and enriches our understanding on the power of deep learning in general. deep representations
clustering on graph
neural networks",learn deep represent graph cluster recent deep learn success adopt mani applic speech recognit imag classif natur languag process work explor possibl employ deep learn graph cluster particular propos simpl method first learn nonlinear embed origin graph stack autoencod run kmean algorithm embed obtain cluster result show simpl method solid theoret foundat due equival autoencod spectral cluster term actual optim demonstr propos method effici flexibl spectral cluster first comput complex autoencod much lower spectral cluster former linear number node spars graph latter super quadrat due depend eigenvalu decomposit second addit constraint like sparsiti impos simpli employ emphspars autoencod develop literatur deep learn howev nonstraightforward implement spars spectral method conduct comprehens experi test perform propos method result various graph dataset show signific outperform convent spectral cluster method clear indic effect deep learn graph cluster enrich understand power deep learn general deep represent cluster graph neural network,7,-9.108933,7.6504393
Fast Multi-Instance Multi-Label Learning,"Sheng-Jun Huang, Wei Gao and Zhi-Hua Zhou",Novel Machine Learning Algorithms (NMLA),"Multi-instance multi-label learning
Key instance
Fast",NMLA: Classification,"In multi-instance multi-label learning (MIML), one object is represented by multiple instances and simultaneously associated with multiple labels. Existing MIML approaches have been found useful in many applications; however, most of them can only handle moderate-sized data. To efficiently handle large data sets, we propose the MIMLfast approach, which first constructs a low-dimensional subspace shared by all labels, and then trains label specific linear models to optimize approximated ranking loss via stochastic gradient descent. Although the MIML problem is complicated, MIMLfast is able to achieve excellent performance by exploiting label relations with shared space and discovering sub-concepts for complicated labels. Experiments show that the performance of MIMLfast is highly competitive to state-of-the-art techniques, whereas its time cost is much less; particularly, on a data set with 30K bags and 270K instances, where none of existing approaches can return results in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is able to identify the most representative instance for each label, and thus providing a chance to understand the relation between input patterns and output semantics.","Fast Multi-Instance Multi-Label Learning In multi-instance multi-label learning (MIML), one object is represented by multiple instances and simultaneously associated with multiple labels. Existing MIML approaches have been found useful in many applications; however, most of them can only handle moderate-sized data. To efficiently handle large data sets, we propose the MIMLfast approach, which first constructs a low-dimensional subspace shared by all labels, and then trains label specific linear models to optimize approximated ranking loss via stochastic gradient descent. Although the MIML problem is complicated, MIMLfast is able to achieve excellent performance by exploiting label relations with shared space and discovering sub-concepts for complicated labels. Experiments show that the performance of MIMLfast is highly competitive to state-of-the-art techniques, whereas its time cost is much less; particularly, on a data set with 30K bags and 270K instances, where none of existing approaches can return results in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is able to identify the most representative instance for each label, and thus providing a chance to understand the relation between input patterns and output semantics. Multi-instance multi-label learning
Key instance
Fast",fast multiinst multilabel learn multiinst multilabel learn miml one object repres multipl instanc simultan associ multipl label exist miml approach found use mani applic howev handl moderates data effici handl larg data set propos mimlfast approach first construct lowdimension subspac share label train label specif linear model optim approxim rank loss via stochast gradient descent although miml problem complic mimlfast abl achiev excel perform exploit label relat share space discov subconcept complic label experi show perform mimlfast high competit stateoftheart techniqu wherea time cost much less particular data set 30k bag 270k instanc none exist approach return result 24 hour mimlfast take 12 minut moreov approach abl identifi repres instanc label thus provid chanc understand relat input pattern output semant multiinst multilabel learn key instanc fast,6,-7.0301385,-12.370427
Modeling and mining spatiotemporal patterns of infection risk from heterogeneous data for active surveillance planning,"Bo Yang, Hua Guo and Yi Yang","Computational Sustainability and AI (CSAI)
Novel Machine Learning Algorithms (NMLA)","active surveillance planning
spatiotemporal data mining
heterogeneous data mining","CSAI: Modeling and prediction of dynamic and spatiotemporal phenomena and systems
CSAI: Control and optimization of dynamic and spatiotemporal systems
NMLA: Data Mining and Knowledge Discovery","Active surveillance is a desirable way to prevent the
spread of infectious diseases in that it aims to timely
discover individual incidences through an active searching
for patients. However, in practice active surveillance
is difficult to implement especially when monitoring
space is large but the available resources are limited.
Therefore, it is extremely important for public health
authorities to know how to distribute their very sparse
resources to high-priority regions so as to maximize the
outcomes of active surveillance. In this paper, we raise
the problem of active surveillance planning and provide
an effective method to address it via modeling and mining
spatiotemporal patterns of infection risk from heterogeneous
data sources. Taking malaria as an example,
we perform an empirical study on real-world data
to validate our method and provide our new findings.","Modeling and mining spatiotemporal patterns of infection risk from heterogeneous data for active surveillance planning Active surveillance is a desirable way to prevent the
spread of infectious diseases in that it aims to timely
discover individual incidences through an active searching
for patients. However, in practice active surveillance
is difficult to implement especially when monitoring
space is large but the available resources are limited.
Therefore, it is extremely important for public health
authorities to know how to distribute their very sparse
resources to high-priority regions so as to maximize the
outcomes of active surveillance. In this paper, we raise
the problem of active surveillance planning and provide
an effective method to address it via modeling and mining
spatiotemporal patterns of infection risk from heterogeneous
data sources. Taking malaria as an example,
we perform an empirical study on real-world data
to validate our method and provide our new findings. active surveillance planning
spatiotemporal data mining
heterogeneous data mining",model mine spatiotempor pattern infect risk heterogen data activ surveil plan activ surveil desir way prevent spread infecti diseas aim time discov individu incid activ search patient howev practic activ surveil difficult implement especi monitor space larg avail resourc limit therefor extrem import public health author know distribut spars resourc highprior region maxim outcom activ surveil paper rais problem activ surveil plan provid effect method address via model mine spatiotempor pattern infect risk heterogen data sourc take malaria exampl perform empir studi realworld data valid method provid new find activ surveil plan spatiotempor data mine heterogen data mine,6,-2.27175,-2.2867858
Symbolic Domain Predictive Control,"Johannes Löhr, Martin Wehrle, Maria Fox and Bernhard Nebel",Planning and Scheduling (PS),"planning
control
hybrid domains
switched linear dynamic systems
domain predictive control","PS: Deterministic Planning
PS: Mixed Discrete/Continuous Planning
PS: Planning (General/Other)","Planning-based methods to guide switched hybrid systems from an initial state into a desired goal region opens an interesting field for control. The idea of the Domain Predictive Control (DPC) approach is to generate input signals affecting both the numerical states and the modes of the system by stringing together atomic actions to a logically consistent plan. However, the existing DPC approach is restricted in the sense that a discrete and pre-defined input signal is required for each action. In this paper, we extend the approach to deal with symbolic states. This allows for the propagation of reachable regions of the state space emerging from actions with inputs that can be arbitrarily chosen within specified input bounds. The symbolic extension enables the applicability of DPC to systems with bounded inputs sets and increases its robustness due to the implicitly reduced search space. Moreover, precise numeric goal states instead of goal regions become reachable.","Symbolic Domain Predictive Control Planning-based methods to guide switched hybrid systems from an initial state into a desired goal region opens an interesting field for control. The idea of the Domain Predictive Control (DPC) approach is to generate input signals affecting both the numerical states and the modes of the system by stringing together atomic actions to a logically consistent plan. However, the existing DPC approach is restricted in the sense that a discrete and pre-defined input signal is required for each action. In this paper, we extend the approach to deal with symbolic states. This allows for the propagation of reachable regions of the state space emerging from actions with inputs that can be arbitrarily chosen within specified input bounds. The symbolic extension enables the applicability of DPC to systems with bounded inputs sets and increases its robustness due to the implicitly reduced search space. Moreover, precise numeric goal states instead of goal regions become reachable. planning
control
hybrid domains
switched linear dynamic systems
domain predictive control",symbol domain predict control planningbas method guid switch hybrid system initi state desir goal region open interest field control idea domain predict control dpc approach generat input signal affect numer state mode system string togeth atom action logic consist plan howev exist dpc approach restrict sens discret predefin input signal requir action paper extend approach deal symbol state allow propag reachabl region state space emerg action input arbitrarili chosen within specifi input bound symbol extens enabl applic dpc system bound input set increas robust due implicit reduc search space moreov precis numer goal state instead goal region becom reachabl plan control hybrid domain switch linear dynam system domain predict control,3,-2.3962965,14.81058
Can Agent Development Affect Developer's Strategy?,"Avshalom Elmalech, David Sarne and Noa Agmon","Cognitive Modeling (CM)
Humans and AI (HAI)","Decision-making
Peer Designed Agents
The Doors Game
Simulating Humans","CM: Simulating Humans
HAI: Understanding People, Theories, Concepts and Methods","Peer Designed Agents (PDAs), computer agents developed by non-experts, is an emerging technology, widely advocated in recent literature for the purpose of replacing people in simulations and investigating human behavior.  Its main premise is that strategies programmed into these agents reliably reflect, to some extent, the behavior used by the programmer in real life.  In this paper we show that PDA development has an important side effect that has not been addressed to date --- the process that merely attempts to capture one's strategy is also likely to affect the developer's strategy.  The phenomenon is demonstrated experimentally using several performance measures.  This result has many implications concerning the appropriate design of PDA-based simulations, and the validness of using PDAs for studying individual decision making. 
Furthermore, we obtain that PDA development actually improved the developer's strategy according to all performance measures.  Therefore, PDA development can be suggested as a means for improving people's problem solving skills.","Can Agent Development Affect Developer's Strategy? Peer Designed Agents (PDAs), computer agents developed by non-experts, is an emerging technology, widely advocated in recent literature for the purpose of replacing people in simulations and investigating human behavior.  Its main premise is that strategies programmed into these agents reliably reflect, to some extent, the behavior used by the programmer in real life.  In this paper we show that PDA development has an important side effect that has not been addressed to date --- the process that merely attempts to capture one's strategy is also likely to affect the developer's strategy.  The phenomenon is demonstrated experimentally using several performance measures.  This result has many implications concerning the appropriate design of PDA-based simulations, and the validness of using PDAs for studying individual decision making. 
Furthermore, we obtain that PDA development actually improved the developer's strategy according to all performance measures.  Therefore, PDA development can be suggested as a means for improving people's problem solving skills. Decision-making
Peer Designed Agents
The Doors Game
Simulating Humans",agent develop affect develop strategi peer design agent pdas comput agent develop nonexpert emerg technolog wide advoc recent literatur purpos replac peopl simul investig human behavior main premis strategi program agent reliabl reflect extent behavior use programm real life paper show pda develop import side effect address date process mere attempt captur one strategi also like affect develop strategi phenomenon demonstr experiment use sever perform measur result mani implic concern appropri design pdabas simul valid use pdas studi individu decis make furthermor obtain pda develop actual improv develop strategi accord perform measur therefor pda develop suggest mean improv peopl problem solv skill decisionmak peer design agent door game simul human,2,4.663134,13.449745
Robust Distance Learning in the Presence of Label Noise,Dong Wang and Xiaoyang Tan,"Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","label noise
robust neighbourhood components analysis
distance learning","NMLA: Feature Construction/Reformulation
NMLA: Supervised Learning (Other)
NMLA: Machine Learning (General/other)","Many distance learning algorithms have been developed in recent years. However, few of them consider the problem when the class labels of training data are noisy, and this may lead to serious performance deterioration. In this paper, we present a robust distance learning method in the presence of label noise, by extending a previous non-parametric discriminative distance learning algorithm, i.e., Neighbourhood Components Analysis (NCA). Particularly, we model the conditional probability of each point’s true label over all the possible classe labels, and use it for a more robust estimation of intra-class scatter matrix. The model is then optimized within the EM framework. In addition, considering that the model tends to be complex under the situation of label noise, we propose to regularize its objective function to prevent overﬁtting. Our experiments on several UCI datasets and a real dataset with unknown noise patterns show that the proposed RNCA is more tolerant to class label noise compared to the original NCA method.","Robust Distance Learning in the Presence of Label Noise Many distance learning algorithms have been developed in recent years. However, few of them consider the problem when the class labels of training data are noisy, and this may lead to serious performance deterioration. In this paper, we present a robust distance learning method in the presence of label noise, by extending a previous non-parametric discriminative distance learning algorithm, i.e., Neighbourhood Components Analysis (NCA). Particularly, we model the conditional probability of each point’s true label over all the possible classe labels, and use it for a more robust estimation of intra-class scatter matrix. The model is then optimized within the EM framework. In addition, considering that the model tends to be complex under the situation of label noise, we propose to regularize its objective function to prevent overﬁtting. Our experiments on several UCI datasets and a real dataset with unknown noise patterns show that the proposed RNCA is more tolerant to class label noise compared to the original NCA method. label noise
robust neighbourhood components analysis
distance learning",robust distanc learn presenc label nois mani distanc learn algorithm develop recent year howev consid problem class label train data noisi may lead serious perform deterior paper present robust distanc learn method presenc label nois extend previous nonparametr discrimin distanc learn algorithm ie neighbourhood compon analysi nca particular model condit probabl point true label possibl class label use robust estim intraclass scatter matrix model optim within em framework addit consid model tend complex situat label nois propos regular object function prevent overﬁt experi sever uci dataset real dataset unknown nois pattern show propos rnca toler class label nois compar origin nca method label nois robust neighbourhood compon analysi distanc learn,6,-8.49694,-11.247983
Exponential Deepening A* for Real-Time Agent-Centered Search,"Guni Sharon, Ariel Felner and Nathan Sturtevant",Heuristic Search and Optimization (HSO),"Heuristic Search
Real-Time Search
Agent-Centered Search","HSO: Heuristic Search
HSO: Evaluation and Analysis (Search and Optimization)
HSO: Search (General/Other)","In the Real-Time Agent-Centered Search (RTACS) problem,
an agent has to arrive at a goal location while acting and
reasoning in the physical state space. Traditionally, RTACS
problems are solved by propagating and updating heuristic
values of states visited by the agent. In existing RTACS
algorithms (e.g., the LRTA* family) the agent may revisit
each state a many times causing the entire procedure to be
quadratic in the state space. In this paper we study the Iterative
Deepening (ID) approach for solving RTACS. We then
present Exponential Deepening A* (EDA*), a RTACS algorithm
where the threshold between successive Depth-First
calls is increased exponentially. We prove that EDA* results
in a linear worst case bound and support this experimentally
by demonstrating up to 10x reduction over existing RTACS
solvers WRT. states expanded and CPU runtime.","Exponential Deepening A* for Real-Time Agent-Centered Search In the Real-Time Agent-Centered Search (RTACS) problem,
an agent has to arrive at a goal location while acting and
reasoning in the physical state space. Traditionally, RTACS
problems are solved by propagating and updating heuristic
values of states visited by the agent. In existing RTACS
algorithms (e.g., the LRTA* family) the agent may revisit
each state a many times causing the entire procedure to be
quadratic in the state space. In this paper we study the Iterative
Deepening (ID) approach for solving RTACS. We then
present Exponential Deepening A* (EDA*), a RTACS algorithm
where the threshold between successive Depth-First
calls is increased exponentially. We prove that EDA* results
in a linear worst case bound and support this experimentally
by demonstrating up to 10x reduction over existing RTACS
solvers WRT. states expanded and CPU runtime. Heuristic Search
Real-Time Search
Agent-Centered Search",exponenti deepen realtim agentcent search realtim agentcent search rtac problem agent arriv goal locat act reason physic state space tradit rtac problem solv propag updat heurist valu state visit agent exist rtac algorithm eg lrta famili agent may revisit state mani time caus entir procedur quadrat state space paper studi iter deepen id approach solv rtac present exponenti deepen eda rtac algorithm threshold success depthfirst call increas exponenti prove eda result linear worst case bound support experiment demonstr 10x reduct exist rtac solver wrt state expand cpu runtim heurist search realtim search agentcent search,5,-12.614466,6.9083123
Learning Low-Rank Representations with Classwise Block-Diagonal Structure for Robust Face Recognition,"Yong Li, Jing Liu, Zechao Li, Yangmuzi Zhang, Hanqing Lu and Songde Ma",Vision (VIS),"Low-Rank Representations
Classwise Block-Diagonal Structure
Robust Face Recognition",CS: Structural learning and knowledge capture,"Face recognition has been widely studied due to its importance in various applications. However, the case that both training images and testing images are corrupted is not well addressed. Motivated by the success of low-rank matrix recovery, we propose a novel semi-supervised low-rank matrix recovery algorithm for robust face recognition. The proposed method can learn robust discriminative representations for both training images and testing images simultaneously by exploiting the classwise block-diagonal structure. Specifically, low-rank matrix approximation can handle the possible contamination of data and the sparse noises. Moreover, the classwise block-diagonal structure is exploited to promote discrimination between different classes. The above issues are formulated into a unified objective function and we design an efficient optimization procedure based on augmented Lagrange multiplier method to solve it. Extensive experiments on three public databases are performed to validate the effectiveness of our approach. The strong identification capability of representations with block-diagonal structure is verified.","Learning Low-Rank Representations with Classwise Block-Diagonal Structure for Robust Face Recognition Face recognition has been widely studied due to its importance in various applications. However, the case that both training images and testing images are corrupted is not well addressed. Motivated by the success of low-rank matrix recovery, we propose a novel semi-supervised low-rank matrix recovery algorithm for robust face recognition. The proposed method can learn robust discriminative representations for both training images and testing images simultaneously by exploiting the classwise block-diagonal structure. Specifically, low-rank matrix approximation can handle the possible contamination of data and the sparse noises. Moreover, the classwise block-diagonal structure is exploited to promote discrimination between different classes. The above issues are formulated into a unified objective function and we design an efficient optimization procedure based on augmented Lagrange multiplier method to solve it. Extensive experiments on three public databases are performed to validate the effectiveness of our approach. The strong identification capability of representations with block-diagonal structure is verified. Low-Rank Representations
Classwise Block-Diagonal Structure
Robust Face Recognition",learn lowrank represent classwis blockdiagon structur robust face recognit face recognit wide studi due import various applic howev case train imag test imag corrupt well address motiv success lowrank matrix recoveri propos novel semisupervis lowrank matrix recoveri algorithm robust face recognit propos method learn robust discrimin represent train imag test imag simultan exploit classwis blockdiagon structur specif lowrank matrix approxim handl possibl contamin data spars nois moreov classwis blockdiagon structur exploit promot discrimin differ class issu formul unifi object function design effici optim procedur base augment lagrang multipli method solv extens experi three public databas perform valid effect approach strong identif capabl represent blockdiagon structur verifi lowrank represent classwis blockdiagon structur robust face recognit,1,6.532084,-15.12385
Congestion Games for V2G-Enabled EV Charging,"Benny Lutati, Vadim Levit, Tal Grinshpoun and Amnon Meisels","Computational Sustainability and AI (CSAI)
Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Congestion games
Potential games
EV charging
V2G","CSAI: Modeling the interactions of agents with different and often conflicting interests
GTEP: Coordination and Collaboration
MAS: Distributed Problem Solving","A model of the problem of charging and discharging electrical vehicles as a congestion game is presented. A generalization of congestion games -- feedback congestion games (FCG) -- is introduced. The charging of grid-integrated vehicles, which can also discharge energy back to the grid, is a natural FCG application. FCGs are proven to be exact potential games and therefore converge to a pure-strategy Nash equilibrium by an iterated better-response process. A compact representation and an algorithm that enable efficient best-response search are presented. A detailed empirical evaluation assesses the performance of the iterated best-response process. The evaluation considers the quality of the resulting solutions and the rate of convergence to a stable state. The effect of allowing to also discharge batteries using FCG is compared to scenarios that only include charging and is found to dramatically improve the predictability of the achieved solutions as well as the balancing of load.","Congestion Games for V2G-Enabled EV Charging A model of the problem of charging and discharging electrical vehicles as a congestion game is presented. A generalization of congestion games -- feedback congestion games (FCG) -- is introduced. The charging of grid-integrated vehicles, which can also discharge energy back to the grid, is a natural FCG application. FCGs are proven to be exact potential games and therefore converge to a pure-strategy Nash equilibrium by an iterated better-response process. A compact representation and an algorithm that enable efficient best-response search are presented. A detailed empirical evaluation assesses the performance of the iterated best-response process. The evaluation considers the quality of the resulting solutions and the rate of convergence to a stable state. The effect of allowing to also discharge batteries using FCG is compared to scenarios that only include charging and is found to dramatically improve the predictability of the achieved solutions as well as the balancing of load. Congestion games
Potential games
EV charging
V2G",congest game v2genabl ev charg model problem charg discharg electr vehicl congest game present general congest game feedback congest game fcg introduc charg gridintegr vehicl also discharg energi back grid natur fcg applic fcgs proven exact potenti game therefor converg purestrategi nash equilibrium iter betterrespons process compact represent algorithm enabl effici bestrespons search present detail empir evalu assess perform iter bestrespons process evalu consid qualiti result solut rate converg stabl state effect allow also discharg batteri use fcg compar scenario includ charg found dramat improv predict achiev solut well balanc load congest game potenti game ev charg v2g,2,5.948492,17.526936
Accurate Household Occupant Behavior Modeling Based on Data Mining Techniques,"Márcia Baptista, Anjie Fang, Helmut Prendinger, Rui Prada and Yohei Yamaguchi","Machine Learning Applications (MLA)
Multiagent Systems (MAS)","Household occupant behavior modeling
Markov chain models
Nearest Neighbor algorithm","MLA: Applications of Supervised Learning
MLA: Machine Learning Applications (General/other)
MAS: Agent-based Simulation and Emergent Behavior","An important requirement of household energy simulation models is their accuracy in estimating energy demand and its fluctuations. Occupant behavior has a major impact upon energy demand. However, Markov chains, the traditional approach to model occupant behavior, (1) has limitations in accurately capturing the coordinated behavior of occupants and (2) is prone to over-fitting. To address these issues, we propose a novel approach that relies on a combination of data mining techniques. The core idea of our model is to determine the behavior of occupants based on nearest neighbor comparison over a database of sample data. Importantly, the model takes into account features related to the coordination of occupants' activities. We use a customized distance function suited for mixed categorical and numerical data. Further, association rule learning allows us to capture the coordination between occupants. Using real data from four households in Japan we are able to show that our model outperforms the traditional Markov chain model with respect to occupant coordination and generalization of behavior patterns.","Accurate Household Occupant Behavior Modeling Based on Data Mining Techniques An important requirement of household energy simulation models is their accuracy in estimating energy demand and its fluctuations. Occupant behavior has a major impact upon energy demand. However, Markov chains, the traditional approach to model occupant behavior, (1) has limitations in accurately capturing the coordinated behavior of occupants and (2) is prone to over-fitting. To address these issues, we propose a novel approach that relies on a combination of data mining techniques. The core idea of our model is to determine the behavior of occupants based on nearest neighbor comparison over a database of sample data. Importantly, the model takes into account features related to the coordination of occupants' activities. We use a customized distance function suited for mixed categorical and numerical data. Further, association rule learning allows us to capture the coordination between occupants. Using real data from four households in Japan we are able to show that our model outperforms the traditional Markov chain model with respect to occupant coordination and generalization of behavior patterns. Household occupant behavior modeling
Markov chain models
Nearest Neighbor algorithm",accur household occup behavior model base data mine techniqu import requir household energi simul model accuraci estim energi demand fluctuat occup behavior major impact upon energi demand howev markov chain tradit approach model occup behavior 1 limit accur captur coordin behavior occup 2 prone overfit address issu propos novel approach reli combin data mine techniqu core idea model determin behavior occup base nearest neighbor comparison databas sampl data import model take account featur relat coordin occup activ use custom distanc function suit mix categor numer data associ rule learn allow us captur coordin occup use real data four household japan abl show model outperform tradit markov chain model respect occup coordin general behavior pattern household occup behavior model markov chain model nearest neighbor algorithm,7,-10.97693,-15.150463
Generating Content for Scenario-Based Serious-Games using CrowdSourcing,"Sigal Sina, Avi Rosenfeld and Sarit Kraus",Game Playing and Interactive Entertainment (GPIE),"Scenario-Based Serious-Games
Generated Content
Crowd-Sourcing",GPIE: Procedural Content Generation,"Scenario-based serious-games have become a main tool for
learning new skills and capabilities. An important factor in
the development of such systems is reducing the time and cost
overhead in manually creating content for these scenarios. To do so, we present ScenarioGen, an automatic method for generating content about everyday activities through combining computer science techniques with the crowd. ScenarioGen uses the crowd in three different ways: to capture a database of scenarios of everyday activities, to generate a database of likely replacements for specific events within that scenario, and to evaluate the resulting scenarios. We evaluated ScenarioGen in 6 different content domains and found that it was consistently rated as coherent and consistent as the originally captured content. We also compared ScenarioGen’s content to that created by traditional planning techniques. We found that both methods were equally effective in generated coherent and consistent scenarios, yet ScenarioGen’s content was found to be more varied and easier to create.","Generating Content for Scenario-Based Serious-Games using CrowdSourcing Scenario-based serious-games have become a main tool for
learning new skills and capabilities. An important factor in
the development of such systems is reducing the time and cost
overhead in manually creating content for these scenarios. To do so, we present ScenarioGen, an automatic method for generating content about everyday activities through combining computer science techniques with the crowd. ScenarioGen uses the crowd in three different ways: to capture a database of scenarios of everyday activities, to generate a database of likely replacements for specific events within that scenario, and to evaluate the resulting scenarios. We evaluated ScenarioGen in 6 different content domains and found that it was consistently rated as coherent and consistent as the originally captured content. We also compared ScenarioGen’s content to that created by traditional planning techniques. We found that both methods were equally effective in generated coherent and consistent scenarios, yet ScenarioGen’s content was found to be more varied and easier to create. Scenario-Based Serious-Games
Generated Content
Crowd-Sourcing",generat content scenariobas seriousgam use crowdsourc scenariobas seriousgam becom main tool learn new skill capabl import factor develop system reduc time cost overhead manual creat content scenario present scenariogen automat method generat content everyday activ combin comput scienc techniqu crowd scenariogen use crowd three differ way captur databas scenario everyday activ generat databas like replac specif event within scenario evalu result scenario evalu scenariogen 6 differ content domain found consist rate coher consist origin captur content also compar scenariogen content creat tradit plan techniqu found method equal effect generat coher consist scenario yet scenariogen content found vari easier creat scenariobas seriousgam generat content crowdsourc,0,0.33844006,3.6179428
A Tractable Approach to ABox Abduction over Description Logic Ontologies,"Jianfeng Du, Kewen Wang and Yi-Dong Shen",Knowledge Representation and Reasoning (KRR),"ABox abduction
abductive reasoning
query abduction problem
description logics
first-order rewritable","KRR: Description Logics
KRR: Diagnosis and Abductive Reasoning","ABox abduction is an important reasoning mechanism for description logic ontologies. It computes all minimal explanations (sets of ABox assertions) whose appending to a consistent ontology enforces the entailment of an observation while keeps the ontology consistent. We focus on practical computation for a general problem of ABox abduction, called the query abduction problem, where an observation is a Boolean conjunctive query and the explanations may contain fresh individuals neither in the ontology nor in the observation. However, in this problem there can be infinitely many minimal explanations. Hence we first identify a class of TBoxes called first-order rewritable TBoxes. It guarantees the existence of finitely many minimal explanations and is sufficient for many ontology applications. To reduce the number of explanations that need to be computed, we introduce a special kind of minimal explanations called representative explanations from which all minimal explanations can be retrieved. We develop a tractable method (in data complexity) for computing all representative explanations in a consistent ontology whose TBox is first-order rewritable. Experimental results demonstrate that the method is efficient and scalable for ontologies with large ABoxes.","A Tractable Approach to ABox Abduction over Description Logic Ontologies ABox abduction is an important reasoning mechanism for description logic ontologies. It computes all minimal explanations (sets of ABox assertions) whose appending to a consistent ontology enforces the entailment of an observation while keeps the ontology consistent. We focus on practical computation for a general problem of ABox abduction, called the query abduction problem, where an observation is a Boolean conjunctive query and the explanations may contain fresh individuals neither in the ontology nor in the observation. However, in this problem there can be infinitely many minimal explanations. Hence we first identify a class of TBoxes called first-order rewritable TBoxes. It guarantees the existence of finitely many minimal explanations and is sufficient for many ontology applications. To reduce the number of explanations that need to be computed, we introduce a special kind of minimal explanations called representative explanations from which all minimal explanations can be retrieved. We develop a tractable method (in data complexity) for computing all representative explanations in a consistent ontology whose TBox is first-order rewritable. Experimental results demonstrate that the method is efficient and scalable for ontologies with large ABoxes. ABox abduction
abductive reasoning
query abduction problem
description logics
first-order rewritable",tractabl approach abox abduct descript logic ontolog abox abduct import reason mechan descript logic ontolog comput minim explan set abox assert whose append consist ontolog enforc entail observ keep ontolog consist focus practic comput general problem abox abduct call queri abduct problem observ boolean conjunct queri explan may contain fresh individu neither ontolog observ howev problem infinit mani minim explan henc first identifi class tbox call firstord rewrit tbox guarante exist finit mani minim explan suffici mani ontolog applic reduc number explan need comput introduc special kind minim explan call repres explan minim explan retriev develop tractabl method data complex comput repres explan consist ontolog whose tbox firstord rewrit experiment result demonstr method effici scalabl ontolog larg abox abox abduct abduct reason queri abduct problem descript logic firstord rewrit,8,24.17282,7.1148415
User Group Oriented Dynamics Exploration,"Zhiting Hu, Junjie Yao and Bin Cui","AI and the Web (AIW)
Machine Learning Applications (MLA)
NLP and Knowledge Representation (NLPKR)","Social media
temporal dynamics
topic modeling
user group","AIW: Human language technologies for web systems, including text summarization and machine translation
AIW: Languages, tools, and methodologies for representing, managing, and visualizing semantic web data
AIW: Machine learning and the web
AIW: Web-based opinion extraction and trend spotting
MLA: Networks
NLPKR: Semantics and Summarization
NLPKR: Natural Language Processing (General/Other)","Dynamic online content becomes the zeitgeist. Vibrant user groups are essential participants and promoters behind it. Studying temporal dynamics is a valuable means to uncover group characters. Since different user groups usually tend to have highly diverse interest and show distinct behavior patterns, they will exhibit varying temporal dynamics. However, most current work only use global trends of temporal topics and fail to distinguish such fine-grained patterns across groups.  

In this paper, we propose GrosToT (Group Specific Topics-over-Time), a unified probabilistic model which infers latent user groups and temporal topics. It can model group-specific temporal topic variation from social media stream. By leveraging the comprehensive group-specific temporal patterns, GrosToT significantly outperforms state-of-the-art dynamics modeling methods. Our proposed approach shows advantage not only in temporal dynamics modeling but also group content exploration. Based on GrosToT, we uncover the interplay between group interest and temporal dynamics. Specifically, we find that group's attention to their medium-interested topics are event-driven, showing rich bursts; while its engagement in group's dominating topics are interest-driven, remaining stable over time. The dynamics over different groups vary and reflect the groups' intention.","User Group Oriented Dynamics Exploration Dynamic online content becomes the zeitgeist. Vibrant user groups are essential participants and promoters behind it. Studying temporal dynamics is a valuable means to uncover group characters. Since different user groups usually tend to have highly diverse interest and show distinct behavior patterns, they will exhibit varying temporal dynamics. However, most current work only use global trends of temporal topics and fail to distinguish such fine-grained patterns across groups.  

In this paper, we propose GrosToT (Group Specific Topics-over-Time), a unified probabilistic model which infers latent user groups and temporal topics. It can model group-specific temporal topic variation from social media stream. By leveraging the comprehensive group-specific temporal patterns, GrosToT significantly outperforms state-of-the-art dynamics modeling methods. Our proposed approach shows advantage not only in temporal dynamics modeling but also group content exploration. Based on GrosToT, we uncover the interplay between group interest and temporal dynamics. Specifically, we find that group's attention to their medium-interested topics are event-driven, showing rich bursts; while its engagement in group's dominating topics are interest-driven, remaining stable over time. The dynamics over different groups vary and reflect the groups' intention. Social media
temporal dynamics
topic modeling
user group",user group orient dynam explor dynam onlin content becom zeitgeist vibrant user group essenti particip promot behind studi tempor dynam valuabl mean uncov group charact sinc differ user group usual tend high divers interest show distinct behavior pattern exhibit vari tempor dynam howev current work use global trend tempor topic fail distinguish finegrain pattern across group paper propos grostot group specif topicsovertim unifi probabilist model infer latent user group tempor topic model groupspecif tempor topic variat social media stream leverag comprehens groupspecif tempor pattern grostot signific outperform stateoftheart dynam model method propos approach show advantag tempor dynam model also group content explor base grostot uncov interplay group interest tempor dynam specif find group attent mediuminterest topic eventdriven show rich burst engag group domin topic interestdriven remain stabl time dynam differ group vari reflect group intent social media tempor dynam topic model user group,0,6.791088,0.25609133
Automatic Construction and Natural-Language Description of Nonparametric Regression Models,"James Lloyd, David Duvenaud, Roger Grosse, Josh Tenenbaum and Zoubin Ghahramani","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Automatic statistician
gaussian processes
regression
bayesian
summarization
model description","MLA: Applications of Supervised Learning
NMLA: Bayesian Learning
NMLA: Kernel Methods
NMLA: Time-Series/Data Streams","This paper presents the beginnings of an automatic statistician, focusing on regression problems.  Our system explores an open-ended space of possible statistical models to discover a good explanation of the data, and then produces a detailed report with figures and natural-language text.

Our approach treats unknown functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints).
Taken together with the compositional structure of our language of models, this allows us to automatically describe functions through a decomposition into additive parts.  Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains.","Automatic Construction and Natural-Language Description of Nonparametric Regression Models This paper presents the beginnings of an automatic statistician, focusing on regression problems.  Our system explores an open-ended space of possible statistical models to discover a good explanation of the data, and then produces a detailed report with figures and natural-language text.

Our approach treats unknown functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints).
Taken together with the compositional structure of our language of models, this allows us to automatically describe functions through a decomposition into additive parts.  Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains. Automatic statistician
gaussian processes
regression
bayesian
summarization
model description",automat construct naturallanguag descript nonparametr regress model paper present begin automat statistician focus regress problem system explor openend space possibl statist model discov good explan data produc detail report figur naturallanguag text approach treat unknown function nonparametr use gaussian process two import consequ first gaussian process model function term highlevel properti eg smooth trend period changepoint taken togeth composit structur languag model allow us automat describ function decomposit addit part second use flexibl nonparametr model rich languag compos openend manner also result stateoftheart extrapol perform evalu 13 real time seri data set various domain automat statistician gaussian process regress bayesian summar model descript,6,3.6632993,-0.9172953
Capturing Difficulty Expressions in Student Online Q&A Discussions,Jaebong Yoo and Jihie Kim,"AI and the Web (AIW)
Applications (APP)
NLP and Machine Learning (NLPML)
NLP and Text Mining (NLPTM)","Student online discussions
dialog analysis (emotional or information roles)
student performance prediction
computational linguistic features
educational data mining","AIW: Question answering on the web
APP: Computer-Aided Education
MLA: Machine Learning Applications (General/other)
NLPML: Discourse and Dialogue
NLPML: Text Classification","We introduce a new application of online dialogue analysis: supporting pedagogical assessment of online Q&A discussions. Extending the existing speech act framework, we capture common emotional expressions that often appear in student discussions, such as frustration and degree of certainty, and present a viable approach for the classification. We demonstrate how such dialogue information can be used in analyzing student discussions and identifying difficulties. In particular, the difficulty indicators are aligned to discussion patterns and student performance. We found that frustration occurs more frequently in longer discussions. The students who frequently express frustration tend to get lower grades than others. On the other hand, frequency of high certainty expressions is positively correlated with the performance. We believe emotional and informational dialogue roles are important factors in explaining discussion development and student performance. We expect such online dialogue analyses can become a powerful assessment tool for instructors and education researchers.","Capturing Difficulty Expressions in Student Online Q&A Discussions We introduce a new application of online dialogue analysis: supporting pedagogical assessment of online Q&A discussions. Extending the existing speech act framework, we capture common emotional expressions that often appear in student discussions, such as frustration and degree of certainty, and present a viable approach for the classification. We demonstrate how such dialogue information can be used in analyzing student discussions and identifying difficulties. In particular, the difficulty indicators are aligned to discussion patterns and student performance. We found that frustration occurs more frequently in longer discussions. The students who frequently express frustration tend to get lower grades than others. On the other hand, frequency of high certainty expressions is positively correlated with the performance. We believe emotional and informational dialogue roles are important factors in explaining discussion development and student performance. We expect such online dialogue analyses can become a powerful assessment tool for instructors and education researchers. Student online discussions
dialog analysis (emotional or information roles)
student performance prediction
computational linguistic features
educational data mining",captur difficulti express student onlin qa discuss introduc new applic onlin dialogu analysi support pedagog assess onlin qa discuss extend exist speech act framework captur common emot express often appear student discuss frustrat degre certainti present viabl approach classif demonstr dialogu inform use analyz student discuss identifi difficulti particular difficulti indic align discuss pattern student perform found frustrat occur frequent longer discuss student frequent express frustrat tend get lower grade other hand frequenc high certainti express posit correl perform believ emot inform dialogu role import factor explain discuss develop student perform expect onlin dialogu analys becom power assess tool instructor educ research student onlin discuss dialog analysi emot inform role student perform predict comput linguist featur educ data mine,0,9.91677,-2.3160553
Lifting Relational MAP-LP Relaxations using Permutation Constraint Graphs,"Udi Apsel, Kristian Kersting and Martin Mladenov",Reasoning under Uncertainty (RU),"Lifted Probabilistic Inference
Statistical Relational Learning
MAP Estimation
Linear Programming
Sherali-Adams Hierarchy","RU: Probabilistic Inference
RU: Relational Probabilistic Models","Inference in large scale graphical models is an important task in many domains, and in particular probabilistic relational models (e.g. Markov logic networks). Such models often exhibit considerable symmetry, and it is a challenge to devise algorithms that exploit this symmetry to speed up inference. Recently, the automorphism group has been proposed to formalize mathematically what ""exploiting symmetry"" means. However, obtaining symmetry derived from automorphism is GI-hard, and consequently only a small fraction of the symmetry is easily available for effective employment. In this paper, we improve upon efficiency in two ways. First, we introduce the Permutation Constraint Graph (PCG), a platform on which greater portions of the symmetries can be revealed and exploited. PCGs classify clusters of variables by projecting relations between cluster members onto a graph, allowing for the efficient pruning of symmetrical clusters even before their generation. Second, we introduce a novel framework based on PCGs for the Sherali-Adams hierarchy of linear program (LP) relaxations, dedicated to exploiting this symmetry for the benefit of tight Maximum A Posteriori (MAP) approximations. Combined with the pruning power of PCG, the framework quickly generates compact formulations for otherwise intractable LPs as demonstrated by several empirical results.","Lifting Relational MAP-LP Relaxations using Permutation Constraint Graphs Inference in large scale graphical models is an important task in many domains, and in particular probabilistic relational models (e.g. Markov logic networks). Such models often exhibit considerable symmetry, and it is a challenge to devise algorithms that exploit this symmetry to speed up inference. Recently, the automorphism group has been proposed to formalize mathematically what ""exploiting symmetry"" means. However, obtaining symmetry derived from automorphism is GI-hard, and consequently only a small fraction of the symmetry is easily available for effective employment. In this paper, we improve upon efficiency in two ways. First, we introduce the Permutation Constraint Graph (PCG), a platform on which greater portions of the symmetries can be revealed and exploited. PCGs classify clusters of variables by projecting relations between cluster members onto a graph, allowing for the efficient pruning of symmetrical clusters even before their generation. Second, we introduce a novel framework based on PCGs for the Sherali-Adams hierarchy of linear program (LP) relaxations, dedicated to exploiting this symmetry for the benefit of tight Maximum A Posteriori (MAP) approximations. Combined with the pruning power of PCG, the framework quickly generates compact formulations for otherwise intractable LPs as demonstrated by several empirical results. Lifted Probabilistic Inference
Statistical Relational Learning
MAP Estimation
Linear Programming
Sherali-Adams Hierarchy",lift relat maplp relax use permut constraint graph infer larg scale graphic model import task mani domain particular probabilist relat model eg markov logic network model often exhibit consider symmetri challeng devis algorithm exploit symmetri speed infer recent automorph group propos formal mathemat exploit symmetri mean howev obtain symmetri deriv automorph gihard consequ small fraction symmetri easili avail effect employ paper improv upon effici two way first introduc permut constraint graph pcg platform greater portion symmetri reveal exploit pcgs classifi cluster variabl project relat cluster member onto graph allow effici prune symmetr cluster even generat second introduc novel framework base pcgs sheraliadam hierarchi linear program lp relax dedic exploit symmetri benefit tight maximum posteriori map approxim combin prune power pcg framework quick generat compact formul otherwis intract lps demonstr sever empir result lift probabilist infer statist relat learn map estim linear program sheraliadam hierarchi,7,-7.2790923,3.3978205
Boosting SBDS for Partial Symmetry Breaking in Constraint Programming,Zichen Zhu and Jimmy Lee,Search and Constraint Satisfaction (SCS),"constraint satisfaction problems
symmetry breaking
SBDS","SCS: Constraint Satisfaction
SCS: SAT and CSP: Modeling/Formulations
SCS: Constraint Satisfaction (General/other)","The paper proposes a dynamic method, Recursive SBDS
(ReSBDS), for efficient partial symmetry breaking. We
first demonstrate how (partial) SBDS misses important
pruning opportunities when given only a subset of symmetries
to break. The investigation pinpoints the culprit
and in turn suggests rectification. The main idea is to
add extra conditional constraints during search recursively
to prune also symmetric nodes of some pruned
subtrees. Thus, ReSBDS can break extra symmetry
compositions, but is carefully designed to break only the
ones that are easy to identify and inexpensive to break.
We present theorems to guarantee the soundness and
termination of our approach, and compare our method
with popular static and dynamic methods. When the
variable (value) heuristic is static, ReSBDS is also complete
in eliminating all interchangeable variables (values)
given only the generator symmetries. Extensive
experimentations confirm the efficiency of ReSBDS,
when compared against state of the art methods.","Boosting SBDS for Partial Symmetry Breaking in Constraint Programming The paper proposes a dynamic method, Recursive SBDS
(ReSBDS), for efficient partial symmetry breaking. We
first demonstrate how (partial) SBDS misses important
pruning opportunities when given only a subset of symmetries
to break. The investigation pinpoints the culprit
and in turn suggests rectification. The main idea is to
add extra conditional constraints during search recursively
to prune also symmetric nodes of some pruned
subtrees. Thus, ReSBDS can break extra symmetry
compositions, but is carefully designed to break only the
ones that are easy to identify and inexpensive to break.
We present theorems to guarantee the soundness and
termination of our approach, and compare our method
with popular static and dynamic methods. When the
variable (value) heuristic is static, ReSBDS is also complete
in eliminating all interchangeable variables (values)
given only the generator symmetries. Extensive
experimentations confirm the efficiency of ReSBDS,
when compared against state of the art methods. constraint satisfaction problems
symmetry breaking
SBDS",boost sbds partial symmetri break constraint program paper propos dynam method recurs sbds resbd effici partial symmetri break first demonstr partial sbds miss import prune opportun given subset symmetri break investig pinpoint culprit turn suggest rectif main idea add extra condit constraint search recurs prune also symmetr node prune subtre thus resbd break extra symmetri composit care design break one easi identifi inexpens break present theorem guarante sound termin approach compar method popular static dynam method variabl valu heurist static resbd also complet elimin interchang variabl valu given generat symmetri extens experiment confirm effici resbd compar state art method constraint satisfact problem symmetri break sbds,5,-4.688123,3.2066612
Evolutionary dynamics of learning algorithms over the sequence form,"Fabio Panozzo, Nicola Gatti and Marcello Restelli","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Game Theory
Multiagent learning
Evolutionary game theory","GTEP: Adversarial Learning
GTEP: Equilibrium
MAS: Multiagent Learning","Multi-agent learning is a challenging open task in artificial intelligence. It is known an interesting connection between multi-agent learning algorithms and evolutionary game theory, showing that the learning dynamics of some algorithms can be modeled as replicator dynamics with a mutation term. Inspired by the recent sequence-form replicator dynamics, we develop a new version of the Q-learning algorithm working on the sequence form of an extensive-form game allowing thus an exponential reduction of the dynamics length w.r.t. those of the normal form. The dynamics of the proposed algorithm can be modeled by using the sequence-form replicator dynamics with a mutation term. We show that, although sequence-form and normal-form replicator dynamics are realization equivalent, the Q-learning algorithm applied to the two forms have non-realization equivalent dynamics. Originally from the previous works on evolutionary game theory models form multi-agent learning, we produce an experimental evaluation to show the accuracy of the model.","Evolutionary dynamics of learning algorithms over the sequence form Multi-agent learning is a challenging open task in artificial intelligence. It is known an interesting connection between multi-agent learning algorithms and evolutionary game theory, showing that the learning dynamics of some algorithms can be modeled as replicator dynamics with a mutation term. Inspired by the recent sequence-form replicator dynamics, we develop a new version of the Q-learning algorithm working on the sequence form of an extensive-form game allowing thus an exponential reduction of the dynamics length w.r.t. those of the normal form. The dynamics of the proposed algorithm can be modeled by using the sequence-form replicator dynamics with a mutation term. We show that, although sequence-form and normal-form replicator dynamics are realization equivalent, the Q-learning algorithm applied to the two forms have non-realization equivalent dynamics. Originally from the previous works on evolutionary game theory models form multi-agent learning, we produce an experimental evaluation to show the accuracy of the model. Game Theory
Multiagent learning
Evolutionary game theory",evolutionari dynam learn algorithm sequenc form multiag learn challeng open task artifici intellig known interest connect multiag learn algorithm evolutionari game theori show learn dynam algorithm model replic dynam mutat term inspir recent sequenceform replic dynam develop new version qlearn algorithm work sequenc form extensiveform game allow thus exponenti reduct dynam length wrt normal form dynam propos algorithm model use sequenceform replic dynam mutat term show although sequenceform normalform replic dynam realize equival qlearn algorithm appli two form nonreal equival dynam origin previous work evolutionari game theori model form multiag learn produc experiment evalu show accuraci model game theori multiag learn evolutionari game theori,2,4.1936183,7.4570303
Structured Possibilistic Planning using Decision Diagrams,"Nicolas Drougard, Florent Teichteil-Königsbuch and Jean-Loup Farges","Knowledge Representation and Reasoning (KRR)
Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","Qualitative Planning under Uncertainty
Symbolic Planning with Decision Diagrams
Possibilistic (PO)MDPs
Mixed Observability","KRR: Qualitative Reasoning
KRR: Reasoning with Beliefs
PS: Markov Models of Environments
PS: Probabilistic Planning
PS: Planning (General/Other)
RU: Graphical Models (Other)
RU: Decision/Utility Theory
RU: Sequential Decision Making
RU: Uncertainty Representations
RU: Uncertainty in AI (General/Other)","Qualitative Possibilistic Mixed-Observable MDPs ($\pi$-MOMDPs), generalizing
$\pi$-MDPs and $\pi$-POMDPs, are well-suited models to planning under
uncertainty with mixed-observability when transition, observation and reward
functions are not precisely known. Functions defining the model as well as
intermediate calculations are valued in a finite possibilistic scale
$\mathcal{L}$, which induces a \emph{finite} belief state space under partial
observability contrary to its probabilistic counterpart. In this paper, we
propose the first study of factored $\pi$-MOMDP models in order to solve large
structured planning problems under \emph{imprecise} uncertainty, or considered
as qualitative approximations of probabilistic problems. Building upon the SPUDD
algorithm for solving factored (probabilistic) MDPs, we conceived a symbolic
algorithm named PPUDD for solving factored $\pi$-MOMDPs. Whereas SPUDD's
decision diagrams' leaves may be as large as the state space since their values
are real numbers aggregated through additions and multiplications, PPUDD's ones
always remain in the finite scale $\echL$ via $\min$ and $\max$ operations only.
Our experiments show that PPUDD's computation time is much lower than SPUDD,
Symbolic-HSVI and APPL for possibilistic and probabilistic versions of the same
benchmarks under either total or mixed observability, while providing
high-quality policies.","Structured Possibilistic Planning using Decision Diagrams Qualitative Possibilistic Mixed-Observable MDPs ($\pi$-MOMDPs), generalizing
$\pi$-MDPs and $\pi$-POMDPs, are well-suited models to planning under
uncertainty with mixed-observability when transition, observation and reward
functions are not precisely known. Functions defining the model as well as
intermediate calculations are valued in a finite possibilistic scale
$\mathcal{L}$, which induces a \emph{finite} belief state space under partial
observability contrary to its probabilistic counterpart. In this paper, we
propose the first study of factored $\pi$-MOMDP models in order to solve large
structured planning problems under \emph{imprecise} uncertainty, or considered
as qualitative approximations of probabilistic problems. Building upon the SPUDD
algorithm for solving factored (probabilistic) MDPs, we conceived a symbolic
algorithm named PPUDD for solving factored $\pi$-MOMDPs. Whereas SPUDD's
decision diagrams' leaves may be as large as the state space since their values
are real numbers aggregated through additions and multiplications, PPUDD's ones
always remain in the finite scale $\echL$ via $\min$ and $\max$ operations only.
Our experiments show that PPUDD's computation time is much lower than SPUDD,
Symbolic-HSVI and APPL for possibilistic and probabilistic versions of the same
benchmarks under either total or mixed observability, while providing
high-quality policies. Qualitative Planning under Uncertainty
Symbolic Planning with Decision Diagrams
Possibilistic (PO)MDPs
Mixed Observability",structur possibilist plan use decis diagram qualit possibilist mixedobserv mdps pimomdp general pimdp pipomdp wellsuit model plan uncertainti mixedobserv transit observ reward function precis known function defin model well intermedi calcul valu finit possibilist scale mathcal induc emphfinit belief state space partial observ contrari probabilist counterpart paper propos first studi factor pimomdp model order solv larg structur plan problem emphimprecis uncertainti consid qualit approxim probabilist problem build upon spudd algorithm solv factor probabilist mdps conceiv symbol algorithm name ppudd solv factor pimomdp wherea spudd decis diagram leav may larg state space sinc valu real number aggreg addit multipl ppudd one alway remain finit scale echl via min max oper experi show ppudd comput time much lower spudd symbolichsvi appl possibilist probabilist version benchmark either total mix observ provid highqual polici qualit plan uncertainti symbol plan decis diagram possibilist pomdp mix observ,5,-1.744359,-6.7660656
Labeling Complicated Objects: Multi-View Multi-Instance Multi-Label Learning,"Cam-Tu Nguyen, Xiaoliang Wang and Zhi-Hua Zhou",Machine Learning Applications (MLA),"multi-view learning.
multi-instance multi-label learning.
partial examples.","AIW: AI for multimedia and multimodal web applications
MLA: Applications of Supervised Learning
MLA: Machine Learning Applications (General/other)","Multi-Instance Multi-Label (MIML) is a learning framework where an example is associated with multiple labels and represented by a set of feature vectors (multiple instances). In the original formalization of MIML learning, instances come from a single source (single view). To leverage multiple information sources (multi-view), we develop a multi-view MIML framework based on hierarchical Bayesian Network, and present an effective learning algorithm based on variational inference. The model can naturally deal with the examples in which some views could be absent (partial examples). On multi-view datasets, it is shown that our method is better than other multi-view and single-view approaches particularly in the presence of partial examples. On single-view benchmarks, extensive evaluation shows that our approach is highly comparable or better than other MIML approaches on labeling examples and instances. Moreover, our method can effectively handle datasets with a large number of labels.","Labeling Complicated Objects: Multi-View Multi-Instance Multi-Label Learning Multi-Instance Multi-Label (MIML) is a learning framework where an example is associated with multiple labels and represented by a set of feature vectors (multiple instances). In the original formalization of MIML learning, instances come from a single source (single view). To leverage multiple information sources (multi-view), we develop a multi-view MIML framework based on hierarchical Bayesian Network, and present an effective learning algorithm based on variational inference. The model can naturally deal with the examples in which some views could be absent (partial examples). On multi-view datasets, it is shown that our method is better than other multi-view and single-view approaches particularly in the presence of partial examples. On single-view benchmarks, extensive evaluation shows that our approach is highly comparable or better than other MIML approaches on labeling examples and instances. Moreover, our method can effectively handle datasets with a large number of labels. multi-view learning.
multi-instance multi-label learning.
partial examples.",label complic object multiview multiinst multilabel learn multiinst multilabel miml learn framework exampl associ multipl label repres set featur vector multipl instanc origin formal miml learn instanc come singl sourc singl view leverag multipl inform sourc multiview develop multiview miml framework base hierarch bayesian network present effect learn algorithm base variat infer model natur deal exampl view could absent partial exampl multiview dataset shown method better multiview singleview approach particular presenc partial exampl singleview benchmark extens evalu show approach high compar better miml approach label exampl instanc moreov method effect handl dataset larg number label multiview learn multiinst multilabel learn partial exampl,7,-5.8531547,-14.248434
Reasoning on LTL on Finite Traces: Insensitivity to Infiniteness,"Giuseppe De Giacomo, Riccardo De Masellis and Marco Montali",Knowledge Representation and Reasoning (KRR),"Linear Temporal Logic
Finite Traces
Reasoning about Actions
Trajectory Constraints in Planning
Logic-Based Process and Service Modelling","AIW: AI for web services: semantic descriptions, planning, matching, and coordination
KRR: Action, Change, and Causality
PS: Temporal Planning","In this paper we study when an LTL formula on finite traces (LTLf formula) is insensitive to infiniteness, that is, it can be correctly handled as a formula on infinite traces under the assumption that at a certain point the infinite trace starts repeating an end event forever, trivializing all other propositions to false. This intuition has been put forward and (wrongly) assumed to hold in general in the literature. We define a necessary and sufficient condition to characterize whether an LTLf formula is insensitive to infiniteness, which can be automatically checked by any LTL reasoner. Then, we show that typical LTLf specification patterns used in process and service modeling in CS, as well as trajectory constraints in Planning and transition-based LTLf specifications of action domains in KR, are indeed very often insensitive to infiniteness. This may help to explain why the assumption of interpreting LTL on finite and on infinite traces has been (wrongly) blurred. Possibly because of this blurring, virtually all literature detours to Buechi automata for constructing the NFA that accepts the traces satisfying an LTLf formula. As a further contribution, we give a simple direct algorithm for computing such NFA.","Reasoning on LTL on Finite Traces: Insensitivity to Infiniteness In this paper we study when an LTL formula on finite traces (LTLf formula) is insensitive to infiniteness, that is, it can be correctly handled as a formula on infinite traces under the assumption that at a certain point the infinite trace starts repeating an end event forever, trivializing all other propositions to false. This intuition has been put forward and (wrongly) assumed to hold in general in the literature. We define a necessary and sufficient condition to characterize whether an LTLf formula is insensitive to infiniteness, which can be automatically checked by any LTL reasoner. Then, we show that typical LTLf specification patterns used in process and service modeling in CS, as well as trajectory constraints in Planning and transition-based LTLf specifications of action domains in KR, are indeed very often insensitive to infiniteness. This may help to explain why the assumption of interpreting LTL on finite and on infinite traces has been (wrongly) blurred. Possibly because of this blurring, virtually all literature detours to Buechi automata for constructing the NFA that accepts the traces satisfying an LTLf formula. As a further contribution, we give a simple direct algorithm for computing such NFA. Linear Temporal Logic
Finite Traces
Reasoning about Actions
Trajectory Constraints in Planning
Logic-Based Process and Service Modelling",reason ltl finit trace insensit infinit paper studi ltl formula finit trace ltlf formula insensit infinit correct handl formula infinit trace assumpt certain point infinit trace start repeat end event forev trivial proposit fals intuit put forward wrong assum hold general literatur defin necessari suffici condit character whether ltlf formula insensit infinit automat check ltl reason show typic ltlf specif pattern use process servic model cs well trajectori constraint plan transitionbas ltlf specif action domain kr inde often insensit infinit may help explain assumpt interpret ltl finit infinit trace wrong blur possibl blur virtual literatur detour buechi automata construct nfa accept trace satisfi ltlf formula contribut give simpl direct algorithm comput nfa linear tempor logic finit trace reason action trajectori constraint plan logicbas process servic model,3,0.530807,1.3312746
A Game-theoretic Analysis of Catalog Optimization,"Joel Oren, Nina Narodytska and Craig Boutilier","Game Theory and Economic Paradigms (GTEP)
Knowledge Representation and Reasoning (KRR)","Competitive assortment optimization
Nash equilibrium
Price of Stability
Computational social choice","GTEP: Auctions and Market-Based Systems
GTEP: Game Theory
GTEP: Social Choice / Voting
GTEP: Equilibrium
GTEP: Imperfect Information
KRR: Preferences
KRR: Reasoning with Beliefs
MAS: E-Commerce","Vendors of all types face the problem of selecting a slate of product offerings---their assortment or catalog---that will maximize their profits.  The profitability of a catalog is determined by both customer preferences and the offerings of their competitors.

We develop a game-theoretic model for analyzing the vendor \emph{catalog optimization} problem in the face of competing vendors. We show that computing a best response is intractable in general, but can be solved by dynamic programming given certain informational or structural assumptions about consumer preferences.
We also analyze conditions under which pure Nash equilibria exist. We study the price of anarchy (and stability), where applicable.","A Game-theoretic Analysis of Catalog Optimization Vendors of all types face the problem of selecting a slate of product offerings---their assortment or catalog---that will maximize their profits.  The profitability of a catalog is determined by both customer preferences and the offerings of their competitors.

We develop a game-theoretic model for analyzing the vendor \emph{catalog optimization} problem in the face of competing vendors. We show that computing a best response is intractable in general, but can be solved by dynamic programming given certain informational or structural assumptions about consumer preferences.
We also analyze conditions under which pure Nash equilibria exist. We study the price of anarchy (and stability), where applicable. Competitive assortment optimization
Nash equilibrium
Price of Stability
Computational social choice",gametheoret analysi catalog optim vendor type face problem select slate product offeringstheir assort catalogthat maxim profit profit catalog determin custom prefer offer competitor develop gametheoret model analyz vendor emphcatalog optim problem face compet vendor show comput best respons intract general solv dynam program given certain inform structur assumpt consum prefer also analyz condit pure nash equilibria exist studi price anarchi stabil applic competit assort optim nash equilibrium price stabil comput social choic,9,7.150876,4.796926
Linear-Time Filtering Algorithms for the Disjunctive Constraint,Hamed Fahimi and Claude-Guy Quimper,"Knowledge Representation and Reasoning (KRR)
Planning and Scheduling (PS)","Constraint programming
Scheduling
Global constraints
Filtering algorithms
Disjunctive constraint","PS: Scheduling
SCS: Constraint Satisfaction
SCS: Global Constraints
SCS: Constraint Satisfaction (General/other)","We present three new filtering algorithms for the disjunctive constraint that all have a linear running time
complexity in the number of tasks. The first algorithm filters the tasks according to the rules of the time tabling. The second algorithm performs an overload check that could also be used for the cumulative constraint. The third algorithm enforces the rules of detectable precedences. The two last algorithms use a new data structure that we introduce and that we call the time line. This data structure provides many constant time operations that were previously implemented in logarithmic time by the Theta-tree data structure. Experiments show that these new algorithms are competitive even for a small number of tasks and outperforms existing algorithms as the number of tasks increases.","Linear-Time Filtering Algorithms for the Disjunctive Constraint We present three new filtering algorithms for the disjunctive constraint that all have a linear running time
complexity in the number of tasks. The first algorithm filters the tasks according to the rules of the time tabling. The second algorithm performs an overload check that could also be used for the cumulative constraint. The third algorithm enforces the rules of detectable precedences. The two last algorithms use a new data structure that we introduce and that we call the time line. This data structure provides many constant time operations that were previously implemented in logarithmic time by the Theta-tree data structure. Experiments show that these new algorithms are competitive even for a small number of tasks and outperforms existing algorithms as the number of tasks increases. Constraint programming
Scheduling
Global constraints
Filtering algorithms
Disjunctive constraint",lineartim filter algorithm disjunct constraint present three new filter algorithm disjunct constraint linear run time complex number task first algorithm filter task accord rule time tabl second algorithm perform overload check could also use cumul constraint third algorithm enforc rule detect preced two last algorithm use new data structur introduc call time line data structur provid mani constant time oper previous implement logarithm time thetatre data structur experi show new algorithm competit even small number task outperform exist algorithm number task increas constraint program schedul global constraint filter algorithm disjunct constraint,3,-4.9349017,6.2546234
Low-Rank Tensor Completion with Discriminant Analysis for Action Classification,"Chengcheng Jia, Guoqiang Zhong and Yun Fu","Machine Learning Applications (MLA)
Vision (VIS)","low-rank
tensor
action recognition
discriminant analysis","MLA: Applications of Supervised Learning
VIS: Face and Gesture Recognition
VIS: Videos","Tensor completion is an important topic in the area of image processing and computer vision research, which is generally built on extraction of the intrinsic structure of tensor data. However, tensor completion techniques are rarely used for action classification, which heavily relies on the extracted features of high-dimensional tensors as well. In this paper, we proposed a method for video based action classification via low-rank tensor completion. Since there could be distortion and corruption in the tensor, we projected the tensor into the subspace, which contains the invariant structure of the tensor and be used as the input of the classifier. The key point is that we aim to calculate the optimal projection matrices, which have the low-rank structure and are used to obtain the subspace. In order to integrate the useful supervisory information of data, we adopt the discriminant analysis criterion to learn the projection matrices and the augmented Lagrange multiplier (ALM) algorithm to solve the multi-variate optimization problem. We explained the properties of the projection matrices, which indicate the different meanings in the row space, column space and frame space, respectively. Experiments demonstrate that our method has obtained better accuracy compared with some other state-of-the-art low-rank tensor methods on MSR Hand Gesture 3D database and MSR Action 3D database.","Low-Rank Tensor Completion with Discriminant Analysis for Action Classification Tensor completion is an important topic in the area of image processing and computer vision research, which is generally built on extraction of the intrinsic structure of tensor data. However, tensor completion techniques are rarely used for action classification, which heavily relies on the extracted features of high-dimensional tensors as well. In this paper, we proposed a method for video based action classification via low-rank tensor completion. Since there could be distortion and corruption in the tensor, we projected the tensor into the subspace, which contains the invariant structure of the tensor and be used as the input of the classifier. The key point is that we aim to calculate the optimal projection matrices, which have the low-rank structure and are used to obtain the subspace. In order to integrate the useful supervisory information of data, we adopt the discriminant analysis criterion to learn the projection matrices and the augmented Lagrange multiplier (ALM) algorithm to solve the multi-variate optimization problem. We explained the properties of the projection matrices, which indicate the different meanings in the row space, column space and frame space, respectively. Experiments demonstrate that our method has obtained better accuracy compared with some other state-of-the-art low-rank tensor methods on MSR Hand Gesture 3D database and MSR Action 3D database. low-rank
tensor
action recognition
discriminant analysis",lowrank tensor complet discrimin analysi action classif tensor complet import topic area imag process comput vision research general built extract intrins structur tensor data howev tensor complet techniqu rare use action classif heavili reli extract featur highdimension tensor well paper propos method video base action classif via lowrank tensor complet sinc could distort corrupt tensor project tensor subspac contain invari structur tensor use input classifi key point aim calcul optim project matric lowrank structur use obtain subspac order integr use supervisori inform data adopt discrimin analysi criterion learn project matric augment lagrang multipli alm algorithm solv multivari optim problem explain properti project matric indic differ mean row space column space frame space respect experi demonstr method obtain better accuraci compar stateoftheart lowrank tensor method msr hand gestur 3d databas msr action 3d databas lowrank tensor action recognit discrimin analysi,1,-22.654737,2.1748445
Programming by Example using Least General Generalizations,"Mohammad Raza, Sumit Gulwani and Natasa Milic-Frayling","Applications (APP)
Heuristic Search and Optimization (HSO)
Knowledge Representation and Reasoning (KRR)","Programming by example
Inductive inference
XML transformations
Program synthesis
Document editing interfaces","APP: Intelligent User Interfaces
APP: Other Applications
HSO: Search (General/Other)
KRR: Knowledge Representation Languages
KRR: Knowledge Representation (General/Other)","Programming-by-example (PBE) has recently seen important advances in the domain of text editing,  but existing technology is restricted to transformations on relatively small unstructured text strings. In this paper we address structural/formatting transformations in richly formatted documents, using an approach based on the idea of least general generalizations from inductive inference, which avoids the scalability issues faced by state-of-the-art PBE methods.  We describe a novel domain specific language (DSL) that can succinctly describe expressive transformations over XML structures (used for describing richly formatted content) and is equipped with a natural partial ordering between programs based on a subsumption relation. We then describe a synthesis algorithm that, given a set of input-output examples of XML structures, identifies a minimal DSL program that is consistent with the examples. We present experimental results over a benchmark of formatting tasks that we collected from online help forums, which show an average of 4.1 examples required for task completion in a few seconds.","Programming by Example using Least General Generalizations Programming-by-example (PBE) has recently seen important advances in the domain of text editing,  but existing technology is restricted to transformations on relatively small unstructured text strings. In this paper we address structural/formatting transformations in richly formatted documents, using an approach based on the idea of least general generalizations from inductive inference, which avoids the scalability issues faced by state-of-the-art PBE methods.  We describe a novel domain specific language (DSL) that can succinctly describe expressive transformations over XML structures (used for describing richly formatted content) and is equipped with a natural partial ordering between programs based on a subsumption relation. We then describe a synthesis algorithm that, given a set of input-output examples of XML structures, identifies a minimal DSL program that is consistent with the examples. We present experimental results over a benchmark of formatting tasks that we collected from online help forums, which show an average of 4.1 examples required for task completion in a few seconds. Programming by example
Inductive inference
XML transformations
Program synthesis
Document editing interfaces",program exampl use least general general programmingbyexampl pbe recent seen import advanc domain text edit exist technolog restrict transform relat small unstructur text string paper address structuralformat transform rich format document use approach base idea least general general induct infer avoid scalabl issu face stateoftheart pbe method describ novel domain specif languag dsl succinct describ express transform xml structur use describ rich format content equip natur partial order program base subsumpt relat describ synthesi algorithm given set inputoutput exampl xml structur identifi minim dsl program consist exampl present experiment result benchmark format task collect onlin help forum show averag 41 exampl requir task complet second program exampl induct infer xml transform program synthesi document edit interfac,1,-12.833272,1.5673772
Give a Hard Problem to a Diverse Team: Exploring Large Action Spaces,"Leandro Soriano Marcolino, Haifeng Xu, Albert Xin Jiang, Milind Tambe and Emma Bowring",Multiagent Systems (MAS),"Team formation
Coordination & Collaboration
Distributed AI",MAS: Coordination and Collaboration,"Recent work has shown that diverse teams can outperform a uniform team made of copies of the best agent. However, there are fundamental questions that were not asked before. When should we use diverse or uniform teams? How does the performance change as the action space or the teams get larger? Hence, we present a new model of diversity for teams, that is more general than previous models. We prove that the performance of a diverse team improves as the size of the action space gets larger. Concerning the size of the diverse team, we show that the performance converges exponentially fast to the optimal one as we increase the number of agents. We present synthetic experiments that allow us to gain further insights: even though a diverse team outperforms a uniform team when the size of the action space increases, the uniform team will eventually again play better than the diverse team for a large enough action space. We verify our predictions in a system of Go playing agents, where we show a diverse team that improves in performance as the board size increases, and eventually overcomes a uniform team.","Give a Hard Problem to a Diverse Team: Exploring Large Action Spaces Recent work has shown that diverse teams can outperform a uniform team made of copies of the best agent. However, there are fundamental questions that were not asked before. When should we use diverse or uniform teams? How does the performance change as the action space or the teams get larger? Hence, we present a new model of diversity for teams, that is more general than previous models. We prove that the performance of a diverse team improves as the size of the action space gets larger. Concerning the size of the diverse team, we show that the performance converges exponentially fast to the optimal one as we increase the number of agents. We present synthetic experiments that allow us to gain further insights: even though a diverse team outperforms a uniform team when the size of the action space increases, the uniform team will eventually again play better than the diverse team for a large enough action space. We verify our predictions in a system of Go playing agents, where we show a diverse team that improves in performance as the board size increases, and eventually overcomes a uniform team. Team formation
Coordination & Collaboration
Distributed AI",give hard problem divers team explor larg action space recent work shown divers team outperform uniform team made copi best agent howev fundament question ask use divers uniform team perform chang action space team get larger henc present new model divers team general previous model prove perform divers team improv size action space get larger concern size divers team show perform converg exponenti fast optim one increas number agent present synthet experi allow us gain insight even though divers team outperform uniform team size action space increas uniform team eventu play better divers team larg enough action space verifi predict system go play agent show divers team improv perform board size increas eventu overcom uniform team team format coordin collabor distribut ai,0,1.807224,3.4519172
A Simple Polynomial-Time Randomized Distributed Algorithm for Connected Row Convex Constraints,"Nguyen Duc Thien, T. K. Satish Kumar, William Yeoh and Sven Koenig","Multiagent Systems (MAS)
Search and Constraint Satisfaction (SCS)","Connected Row Convex Constraints
Distributed CSP
Randomized Algorithms","MAS: Distributed Problem Solving
SCS: Distributed CSP/Optimization","In this paper, we describe a simple randomized algorithm that runs in polynomial time and solves connected row convex (CRC) constraints in a distributed setting. CRC constraints generalize many known tractable classes of constraints like 2-SAT and implicational constraints. They can model problems in many domains including temporal reasoning and geometric reasoning; and generally speaking, play the role of ``Gaussians'' in the logical world. Our simple randomized algorithm for solving them in distributed settings, therefore, has a number of important applications. We support our claims through empirical results. We also generalize our algorithm to tractable classes of tree convex constraints.","A Simple Polynomial-Time Randomized Distributed Algorithm for Connected Row Convex Constraints In this paper, we describe a simple randomized algorithm that runs in polynomial time and solves connected row convex (CRC) constraints in a distributed setting. CRC constraints generalize many known tractable classes of constraints like 2-SAT and implicational constraints. They can model problems in many domains including temporal reasoning and geometric reasoning; and generally speaking, play the role of ``Gaussians'' in the logical world. Our simple randomized algorithm for solving them in distributed settings, therefore, has a number of important applications. We support our claims through empirical results. We also generalize our algorithm to tractable classes of tree convex constraints. Connected Row Convex Constraints
Distributed CSP
Randomized Algorithms",simpl polynomialtim random distribut algorithm connect row convex constraint paper describ simpl random algorithm run polynomi time solv connect row convex crc constraint distribut set crc constraint general mani known tractabl class constraint like 2sat implic constraint model problem mani domain includ tempor reason geometr reason general speak play role gaussian logic world simpl random algorithm solv distribut set therefor number import applic support claim empir result also general algorithm tractabl class tree convex constraint connect row convex constraint distribut csp random algorithm,3,-4.4598556,5.5468097
SUIT: A Supervised User-Item based Topic model for Sentiment Analysis,"Fangtao Li, Sheng Wang, Shenghua Liu and Ming Zhang",NLP and Text Mining (NLPTM),"topic model
sentiment analysis
review rating","AIW: Web-based opinion extraction and trend spotting
NLPTM: Information Extraction","Topic models have been widely used for sentiment analysis. Previous studies shows that supervised topic model can better model the sentiment expressions. However most of existing topic methods only model the sentiment text information, but do not consider the user, who expressed the sentiment, and the item, which the sentiment is expressed on. Since different users may use different sentiment expressions for different items, we argue that it is better to incorporate the user and item information into the topic model for sentiment analysis. In this paper, we propose a novel Supervised User-Item based Topic model, called SUIT model, for sentiment analysis. It can simultaneously utilize the textual topic and latent user-item factors. Our proposed method uses the tensor outer product of text topic proportion vector, user latent factor and item latent factor to model the sentiment label generalization. Extensive experiments are conducted on two datasets: review dataset and microblog dataset. The results demonstrate the advantages of our model. It shows significant improvement compared with supervised topic models and collaborative filtering methods.","SUIT: A Supervised User-Item based Topic model for Sentiment Analysis Topic models have been widely used for sentiment analysis. Previous studies shows that supervised topic model can better model the sentiment expressions. However most of existing topic methods only model the sentiment text information, but do not consider the user, who expressed the sentiment, and the item, which the sentiment is expressed on. Since different users may use different sentiment expressions for different items, we argue that it is better to incorporate the user and item information into the topic model for sentiment analysis. In this paper, we propose a novel Supervised User-Item based Topic model, called SUIT model, for sentiment analysis. It can simultaneously utilize the textual topic and latent user-item factors. Our proposed method uses the tensor outer product of text topic proportion vector, user latent factor and item latent factor to model the sentiment label generalization. Extensive experiments are conducted on two datasets: review dataset and microblog dataset. The results demonstrate the advantages of our model. It shows significant improvement compared with supervised topic models and collaborative filtering methods. topic model
sentiment analysis
review rating",suit supervis useritem base topic model sentiment analysi topic model wide use sentiment analysi previous studi show supervis topic model better model sentiment express howev exist topic method model sentiment text inform consid user express sentiment item sentiment express sinc differ user may use differ sentiment express differ item argu better incorpor user item inform topic model sentiment analysi paper propos novel supervis useritem base topic model call suit model sentiment analysi simultan util textual topic latent useritem factor propos method use tensor outer product text topic proport vector user latent factor item latent factor model sentiment label general extens experi conduct two dataset review dataset microblog dataset result demonstr advantag model show signific improv compar supervis topic model collabor filter method topic model sentiment analysi review rate,0,8.417775,-0.6300296
Automatic Game Design via Mechanic Generation,Alexander Zook and Mark Riedl,Game Playing and Interactive Entertainment (GPIE),"procedural content generation
game generation
game mechanics","GPIE: AI in Game Design
GPIE: Procedural Content Generation","Game designs often center on the game mechanics - rules governing the logical evolution of the game. We seek to develop an intelligent system that generates computer games. As first steps towards this goal we present a composable and cross-domain representation for game mechanics that draws from AI planning action representations. We use a constraint solver to generate mechanics subject to design requirements on the form of those mechanics - what they do in the game. A planner takes a set of generated mechanics and tests whether those mechanics meet playability requirements - controlling how mechanics function in a game to affect player behavior. We demonstrate our system by modeling and generating mechanics in a role-playing game, platformer game, and combined role-playing-platformer game.","Automatic Game Design via Mechanic Generation Game designs often center on the game mechanics - rules governing the logical evolution of the game. We seek to develop an intelligent system that generates computer games. As first steps towards this goal we present a composable and cross-domain representation for game mechanics that draws from AI planning action representations. We use a constraint solver to generate mechanics subject to design requirements on the form of those mechanics - what they do in the game. A planner takes a set of generated mechanics and tests whether those mechanics meet playability requirements - controlling how mechanics function in a game to affect player behavior. We demonstrate our system by modeling and generating mechanics in a role-playing game, platformer game, and combined role-playing-platformer game. procedural content generation
game generation
game mechanics",automat game design via mechan generat game design often center game mechan rule govern logic evolut game seek develop intellig system generat comput game first step toward goal present compos crossdomain represent game mechan draw ai plan action represent use constraint solver generat mechan subject design requir form mechan game planner take set generat mechan test whether mechan meet playabl requir control mechan function game affect player behavior demonstr system model generat mechan roleplay game platform game combin roleplayingplatform game procedur content generat game generat game mechan,2,8.878917,20.137354
Learning the Structure of Probabilistic Graphical Models with an Extended Cascading Indian Buffet Process,"Patrick Dallaire, Philippe Giguère and Brahim Chaib-Draa",Novel Machine Learning Algorithms (NMLA),"Structure learning
Bayesian Learning
Bayesian nonparametrics
Graphical models
Deep belief networks
Infinite directed acyclic graphs
MCMC inference","NMLA: Bayesian Learning
NMLA: Data Mining and Knowledge Discovery
NMLA: Graphical Model Learning
RU: Bayesian Networks
RU: Graphical Models (Other)","In this paper, we present an extension of the cascading Indian buffet process (CIBP) intended to learning arbitrary directed acyclic graph structures as opposed to the CIBP, which is limited to purely layered structures. The extended cascading Indian buffet process (eCIBP) essentially consists in adding an extra sampling step to the CIBP to generate connections between non-consecutive layers. In the context of graphical model structure learning, the proposed approach allows learning structures having an unbounded number of hidden random variables and automatically selecting the model complexity. We evaluated the extended process on multivariate density estimation and structure identification tasks by measuring the structure complexity and predictive performance. The results suggest the extension leads to extracting simpler graphs without scarifying predictive precision.","Learning the Structure of Probabilistic Graphical Models with an Extended Cascading Indian Buffet Process In this paper, we present an extension of the cascading Indian buffet process (CIBP) intended to learning arbitrary directed acyclic graph structures as opposed to the CIBP, which is limited to purely layered structures. The extended cascading Indian buffet process (eCIBP) essentially consists in adding an extra sampling step to the CIBP to generate connections between non-consecutive layers. In the context of graphical model structure learning, the proposed approach allows learning structures having an unbounded number of hidden random variables and automatically selecting the model complexity. We evaluated the extended process on multivariate density estimation and structure identification tasks by measuring the structure complexity and predictive performance. The results suggest the extension leads to extracting simpler graphs without scarifying predictive precision. Structure learning
Bayesian Learning
Bayesian nonparametrics
Graphical models
Deep belief networks
Infinite directed acyclic graphs
MCMC inference",learn structur probabilist graphic model extend cascad indian buffet process paper present extens cascad indian buffet process cibp intend learn arbitrari direct acycl graph structur oppos cibp limit pure layer structur extend cascad indian buffet process ecibp essenti consist ad extra sampl step cibp generat connect nonconsecut layer context graphic model structur learn propos approach allow learn structur unbound number hidden random variabl automat select model complex evalu extend process multivari densiti estim structur identif task measur structur complex predict perform result suggest extens lead extract simpler graph without scarifi predict precis structur learn bayesian learn bayesian nonparametr graphic model deep belief network infinit direct acycl graph mcmc infer,1,3.9227633,-2.3806956
Learning Compositional Sparse Models of Bimodal Percepts,"Suren Kumar, Vikas Dhiman and Jason J. Corso","Cognitive Systems (CS)
Novel Machine Learning Algorithms (NMLA)
Vision (VIS)","compositional model
bimodal sparse representation
vision and audio","CS: Structural learning and knowledge capture
NMLA: Dimension Reduction/Feature Selection
VIS: Language and Vision","Various perceptual domains have underlying compositional semantics that are rarely captured in current models.  We suspect this is because directly learning the compositional structure has evaded these models.  Yet, the compositional structure of a given domain can be grounded in a separate domain thereby simplifying its learning.  To that end, we propose a new approach to modeling bimodal percepts that explicitly relates distinct projections across each modality and then jointly learns a bimodal sparse representation.  The resulting model enables compositionality across these distinct projections and hence can generalize to unobserved percepts spanned by this compositional basis.  For example, our model can be trained on ""red triangles"" and ""blue squares""; yet, implicitly will also have learned ""red squares"" and ""blue triangles"".  The structure of the projections and hence the compositional basis is learned automatically for a given language model.  To test our model, we have acquired a new bimodal dataset comprising images and spoken utterances of colored shapes in a tabletop setup.  Our experiments demonstrate the benefits of explicitly leveraging compositionality in both quantitative and human evaluation studies.","Learning Compositional Sparse Models of Bimodal Percepts Various perceptual domains have underlying compositional semantics that are rarely captured in current models.  We suspect this is because directly learning the compositional structure has evaded these models.  Yet, the compositional structure of a given domain can be grounded in a separate domain thereby simplifying its learning.  To that end, we propose a new approach to modeling bimodal percepts that explicitly relates distinct projections across each modality and then jointly learns a bimodal sparse representation.  The resulting model enables compositionality across these distinct projections and hence can generalize to unobserved percepts spanned by this compositional basis.  For example, our model can be trained on ""red triangles"" and ""blue squares""; yet, implicitly will also have learned ""red squares"" and ""blue triangles"".  The structure of the projections and hence the compositional basis is learned automatically for a given language model.  To test our model, we have acquired a new bimodal dataset comprising images and spoken utterances of colored shapes in a tabletop setup.  Our experiments demonstrate the benefits of explicitly leveraging compositionality in both quantitative and human evaluation studies. compositional model
bimodal sparse representation
vision and audio",learn composit spars model bimod percept various perceptu domain under composit semant rare captur current model suspect direct learn composit structur evad model yet composit structur given domain ground separ domain therebi simplifi learn end propos new approach model bimod percept explicit relat distinct project across modal joint learn bimod spars represent result model enabl composit across distinct project henc general unobserv percept span composit basi exampl model train red triangl blue squar yet implicit also learn red squar blue triangl structur project henc composit basi learn automat given languag model test model acquir new bimod dataset compris imag spoken utter color shape tabletop setup experi demonstr benefit explicit leverag composit quantit human evalu studi composit model bimod spars represent vision audio,4,9.668982,-6.5336123
Dramatis: A Computational Model of Suspense,Brian O'Neill and Mark Riedl,"Applications (APP)
Game Playing and Interactive Entertainment (GPIE)","computational creativity
psychological models
computational aesthetics","APP: Art and Music
GPIE: AI Storytelling
GPIE: Computational Creativity and Generative Art","We introduce Dramatis, a computational model of suspense based on a reformulation of a psychological definition of the suspense phenomenon. In this reformulation, suspense is correlated with the audience’s ability to generate a plan for the protagonist to avoid an impending negative outcome. Dramatis measures the suspense level by generating such a plan and determining its perceived likelihood of success. We report on three evaluations of Dramatis, including a comparison of Dramatis output to the suspense reported by human readers, as well as ablative tests of Dramatis components. In these studies, we found that Dramatis output corresponded to the suspense ratings given by human readers for stories in three separate domains.","Dramatis: A Computational Model of Suspense We introduce Dramatis, a computational model of suspense based on a reformulation of a psychological definition of the suspense phenomenon. In this reformulation, suspense is correlated with the audience’s ability to generate a plan for the protagonist to avoid an impending negative outcome. Dramatis measures the suspense level by generating such a plan and determining its perceived likelihood of success. We report on three evaluations of Dramatis, including a comparison of Dramatis output to the suspense reported by human readers, as well as ablative tests of Dramatis components. In these studies, we found that Dramatis output corresponded to the suspense ratings given by human readers for stories in three separate domains. computational creativity
psychological models
computational aesthetics",dramati comput model suspens introduc dramati comput model suspens base reformul psycholog definit suspens phenomenon reformul suspens correl audienc abil generat plan protagonist avoid impend negat outcom dramati measur suspens level generat plan determin perceiv likelihood success report three evalu dramati includ comparison dramati output suspens report human reader well ablat test dramati compon studi found dramati output correspond suspens rate given human reader stori three separ domain comput creativ psycholog model comput aesthet,0,1.8176073,1.0195299
LASS: A Simple Assignment Model with Laplacian Smoothing,Miguel Carreira-Perpinan and Weiran Wang,Novel Machine Learning Algorithms (NMLA),"semi-supervised learning
convex optimization
ADMM
soft assignment
Laplacian smoothing","HSO: Optimization
NMLA: Semisupervised Learning
RU: Uncertainty in AI (General/Other)","We consider the problem of learning soft assignments of N items to K categories given two sources of information: an item-category similarity matrix, which encourages items to be assigned to categories they are similar to (and to not be assigned to categories they are dissimilar to), and an item-item similarity matrix, which encourages similar items to have similar assignments. We propose a simple quadratic programming model that captures this intuition. We give necessary conditions for its solution to be unique, define an out-of-sample mapping, and derive a simple, effective training algorithm based on the alternating direction method of multipliers. The model predicts reasonable assignments from even a few similarity values, and can be seen as a generalization of semisupervised learning. It is particularly useful when items naturally belong to multiple categories, as for example when annotating documents with keywords or pictures with tags, with partially tagged items, or when the categories have complex interrelations (e.g. hierarchical) that are unknown.","LASS: A Simple Assignment Model with Laplacian Smoothing We consider the problem of learning soft assignments of N items to K categories given two sources of information: an item-category similarity matrix, which encourages items to be assigned to categories they are similar to (and to not be assigned to categories they are dissimilar to), and an item-item similarity matrix, which encourages similar items to have similar assignments. We propose a simple quadratic programming model that captures this intuition. We give necessary conditions for its solution to be unique, define an out-of-sample mapping, and derive a simple, effective training algorithm based on the alternating direction method of multipliers. The model predicts reasonable assignments from even a few similarity values, and can be seen as a generalization of semisupervised learning. It is particularly useful when items naturally belong to multiple categories, as for example when annotating documents with keywords or pictures with tags, with partially tagged items, or when the categories have complex interrelations (e.g. hierarchical) that are unknown. semi-supervised learning
convex optimization
ADMM
soft assignment
Laplacian smoothing",lass simpl assign model laplacian smooth consid problem learn soft assign n item k categori given two sourc inform itemcategori similar matrix encourag item assign categori similar assign categori dissimilar itemitem similar matrix encourag similar item similar assign propos simpl quadrat program model captur intuit give necessari condit solut uniqu defin outofsampl map deriv simpl effect train algorithm base altern direct method multipli model predict reason assign even similar valu seen general semisupervis learn particular use item natur belong multipl categori exampl annot document keyword pictur tag partial tag item categori complex interrel eg hierarch unknown semisupervis learn convex optim admm soft assign laplacian smooth,0,-3.694205,-2.2498276
Learning Goal-Oriented Hierarchical Tasks from Situated Interactive Instruction,Shiwali Mohan and John Laird,Cognitive Systems (CS),"task learning
interactive learning
explanation-based learning
situated interactive instruction
learning for robots
learning from dialog
Soar
cognitive architecture","CM: Agent Architectures
CM: Symbolic AI
CS: Social cognition and interaction
CS: Structural learning and knowledge capture","Our research aims at building interactive robots and agent that can expand their knowledge by interacting with human users. In this paper, we focus on learning goal-oriented tasks from situated interactive instructions. Learning novel tasks is a challenging computational problem requiring the agent to acquire a variety of knowledge including goal definitions and hierarchical control information. We frame acquisition of hierarchical tasks as an explanation-based learning (EBL) problem and propose an interactive learning variant of EBL for a robotic agent. We show that our approach can exploit information in situated instructions along with the domain knowledge to demonstrate fast generalization on several tasks.The knowledge acquired transfers across structurally similar tasks. Finally, we show that our approach seamlessly combines agent-driven exploration with instructions for mixed-initiative learning.","Learning Goal-Oriented Hierarchical Tasks from Situated Interactive Instruction Our research aims at building interactive robots and agent that can expand their knowledge by interacting with human users. In this paper, we focus on learning goal-oriented tasks from situated interactive instructions. Learning novel tasks is a challenging computational problem requiring the agent to acquire a variety of knowledge including goal definitions and hierarchical control information. We frame acquisition of hierarchical tasks as an explanation-based learning (EBL) problem and propose an interactive learning variant of EBL for a robotic agent. We show that our approach can exploit information in situated instructions along with the domain knowledge to demonstrate fast generalization on several tasks.The knowledge acquired transfers across structurally similar tasks. Finally, we show that our approach seamlessly combines agent-driven exploration with instructions for mixed-initiative learning. task learning
interactive learning
explanation-based learning
situated interactive instruction
learning for robots
learning from dialog
Soar
cognitive architecture",learn goalori hierarch task situat interact instruct research aim build interact robot agent expand knowledg interact human user paper focus learn goalori task situat interact instruct learn novel task challeng comput problem requir agent acquir varieti knowledg includ goal definit hierarch control inform frame acquisit hierarch task explanationbas learn ebl problem propos interact learn variant ebl robot agent show approach exploit inform situat instruct along domain knowledg demonstr fast general sever tasksth knowledg acquir transfer across structur similar task final show approach seamless combin agentdriven explor instruct mixedin learn task learn interact learn explanationbas learn situat interact instruct learn robot learn dialog soar cognit architectur,6,-0.53882533,-11.331674
Chance-constrained Probabilistic Simple Temporal Problems,"Cheng Fang, Peng Yu and Brian Williams","Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)
Search and Constraint Satisfaction (SCS)","scheduling
probabilistic
STNU","PS: Probabilistic Planning
PS: Scheduling
PS: Temporal Planning
SCS: Constraint Optimization","Robust scheduling is essential to many autonomous systems and logistics tasks. Probabilistic formalisms quantify the risk of schedule failure, which is essential for mission critical applications. Probabilistic methods for solving temporal problems exist that attempt to minimize the probability of schedule failure. These methods are overly conservative, resulting in a loss in schedule utility. Chance constrained formalism address this problem by imposing bounds on risk, while maximizing utility subject to these risk bounds. 

In this paper we present the probabilistic Simple Temporal Network (pSTN), a probabilistic formalism for representing temporal problems with bounded risk and a utility over event timing. We introduce a constrained optimisation algorithm for pSTNs that achieves compactness and efficiency through a problem encoding in terms of a parameterised STNU and its reformulation as a parameterised STN. We demonstrate through a car sharing application that our chance-constrained approach runs in the same time as the previous probabilistic approach, yields solutions with utility improvements of at least 5% over previous arts, while guaranteeing operation within the specified risk bound.","Chance-constrained Probabilistic Simple Temporal Problems Robust scheduling is essential to many autonomous systems and logistics tasks. Probabilistic formalisms quantify the risk of schedule failure, which is essential for mission critical applications. Probabilistic methods for solving temporal problems exist that attempt to minimize the probability of schedule failure. These methods are overly conservative, resulting in a loss in schedule utility. Chance constrained formalism address this problem by imposing bounds on risk, while maximizing utility subject to these risk bounds. 

In this paper we present the probabilistic Simple Temporal Network (pSTN), a probabilistic formalism for representing temporal problems with bounded risk and a utility over event timing. We introduce a constrained optimisation algorithm for pSTNs that achieves compactness and efficiency through a problem encoding in terms of a parameterised STNU and its reformulation as a parameterised STN. We demonstrate through a car sharing application that our chance-constrained approach runs in the same time as the previous probabilistic approach, yields solutions with utility improvements of at least 5% over previous arts, while guaranteeing operation within the specified risk bound. scheduling
probabilistic
STNU",chanceconstrain probabilist simpl tempor problem robust schedul essenti mani autonom system logist task probabilist formal quantifi risk schedul failur essenti mission critic applic probabilist method solv tempor problem exist attempt minim probabl schedul failur method over conserv result loss schedul util chanc constrain formal address problem impos bound risk maxim util subject risk bound paper present probabilist simpl tempor network pstn probabilist formal repres tempor problem bound risk util event time introduc constrain optimis algorithm pstns achiev compact effici problem encod term parameteris stnu reformul parameteris stn demonstr car share applic chanceconstrain approach run time previous probabilist approach yield solut util improv least 5 previous art guarante oper within specifi risk bound schedul probabilist stnu,3,-13.279324,-1.7060161
False-Name Bidding and Economic Efficiency in Combinatorial Auctions,Adrian Vetta and Colleen Alkalay-Houlihan,"Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)
Reasoning under Uncertainty (RU)","- GTEP: Auctions and Market-Based Systems
- GTEP: Equilibrium
- MAS: E-Commerce
- MAS: Mechanism Design","GTEP: Auctions and Market-Based Systems
GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information
MAS: E-Commerce
MAS: Evaluation and Analysis (Multiagent Systems)
RU: Decision/Utility Theory","Combinatorial auctions are multiple-item auctions in which bidders may place bids on any package (subset) of goods. This additional expressibility produces benefits that have led to combinatorial auctions becoming extremely important both in practice and in theory. In the computer science community, research has focused primarily on computation and incentive compatibility. The latter concerns a specific form of bidder misrepresentation. However, with modern forms of bid submission, such as electronic bidding, new types of cheating become feasible. For combinatorial auctions, prominent amongst them is false-name bidding; that is, bidding under pseudonyms. The ubiquitous Vickrey-Clarke-Groves (VCG) mechanism is incentive compatible and produces optimal allocations, but it is not false-name-proof; bidders can increase their utility by submitting bids under multiple identifiers. Consequently, there has recently been much interest in the design and analysis of false-name-proof auction mechanisms. 
 
Such false-name-proof mechanisms, however, can produce allocations with very low economic efficiency/social welfare. In contrast, we show that, provided the degree to which different goods are complementary is bounded (as is the case in many important, practical auctions), the VCG mechanism gives a constant efficiency guarantee. Such efficiency guarantees hold even at equilibria where the agents bid in a manner that is not individually rational. Thus, while an individual bidder may 
personally benefit greatly from making false-name bids, this will have only a small detrimental effect on the objective of the auctioneer: maximizing economic efficiency. Thus, from the auctioneer's viewpoint the VCG mechanism remains preferable to false-name-proof mechanisms.","False-Name Bidding and Economic Efficiency in Combinatorial Auctions Combinatorial auctions are multiple-item auctions in which bidders may place bids on any package (subset) of goods. This additional expressibility produces benefits that have led to combinatorial auctions becoming extremely important both in practice and in theory. In the computer science community, research has focused primarily on computation and incentive compatibility. The latter concerns a specific form of bidder misrepresentation. However, with modern forms of bid submission, such as electronic bidding, new types of cheating become feasible. For combinatorial auctions, prominent amongst them is false-name bidding; that is, bidding under pseudonyms. The ubiquitous Vickrey-Clarke-Groves (VCG) mechanism is incentive compatible and produces optimal allocations, but it is not false-name-proof; bidders can increase their utility by submitting bids under multiple identifiers. Consequently, there has recently been much interest in the design and analysis of false-name-proof auction mechanisms. 
 
Such false-name-proof mechanisms, however, can produce allocations with very low economic efficiency/social welfare. In contrast, we show that, provided the degree to which different goods are complementary is bounded (as is the case in many important, practical auctions), the VCG mechanism gives a constant efficiency guarantee. Such efficiency guarantees hold even at equilibria where the agents bid in a manner that is not individually rational. Thus, while an individual bidder may 
personally benefit greatly from making false-name bids, this will have only a small detrimental effect on the objective of the auctioneer: maximizing economic efficiency. Thus, from the auctioneer's viewpoint the VCG mechanism remains preferable to false-name-proof mechanisms. - GTEP: Auctions and Market-Based Systems
- GTEP: Equilibrium
- MAS: E-Commerce
- MAS: Mechanism Design",falsenam bid econom effici combinatori auction combinatori auction multipleitem auction bidder may place bid packag subset good addit express produc benefit led combinatori auction becom extrem import practic theori comput scienc communiti research focus primarili comput incent compat latter concern specif form bidder misrepresent howev modern form bid submiss electron bid new type cheat becom feasibl combinatori auction promin amongst falsenam bid bid pseudonym ubiquit vickreyclarkegrov vcg mechan incent compat produc optim alloc falsenameproof bidder increas util submit bid multipl identifi consequ recent much interest design analysi falsenameproof auction mechan falsenameproof mechan howev produc alloc low econom efficiencysoci welfar contrast show provid degre differ good complementari bound case mani import practic auction vcg mechan give constant effici guarante effici guarante hold even equilibria agent bid manner individu ration thus individu bidder may person benefit great make falsenam bid small detriment effect object auction maxim econom effici thus auction viewpoint vcg mechan remain prefer falsenameproof mechan gtep auction marketbas system gtep equilibrium mas ecommerc mas mechan design,9,11.789021,13.872609
Exploiting Competition Relationship for Robust Visual Recognition,Liang Du and Haibin Ling,"Applications (APP)
Machine Learning Applications (MLA)
Vision (VIS)","Competition relationships
jointly learning
visual recognition","APP: Other Applications
MLA: Applications of Supervised Learning
VIS: Categorization","Joint learning of similar tasks has been a popular trend in visual recognition and proven to be beneficial. The between-task similarity typically provides useful cues, such as feature sharing, for learning visual classifiers. By contrast, the competition relationship between visual recognition tasks (\eg, content independent writer identification and handwriting recognition) remains largely under-explored.
Intuitively, the between-task competition can be used to guide the feature selection process that plays a key role when learning a visual classifier.
Motivated by this intuition, we propose a novel algorithm to exploit competition relationship for improving visual recognition tasks. More specifically, given a target task and its competing tasks, we jointly model them by a generalized additive regression model with  competition constraints. This constraint effectively discourages choosing of irrelevant features (weak learners) that are good for the 
 tasks with competition relationships . The proposed algorithm is named \emph{CompBoost} since it can be viewed extended from the RealAdaboost algorithm. We apply CompBoost to two visual recognition applications: (1) content-independent writer identification from handwriting scripts by exploiting competing tasks of handwriting recognition, and (2) actor-independent facial expression recognition by exploiting competing tasks of face recognition. In both experiments our approach demonstrates promising performance gains by exploiting the between-task competition relationships.","Exploiting Competition Relationship for Robust Visual Recognition Joint learning of similar tasks has been a popular trend in visual recognition and proven to be beneficial. The between-task similarity typically provides useful cues, such as feature sharing, for learning visual classifiers. By contrast, the competition relationship between visual recognition tasks (\eg, content independent writer identification and handwriting recognition) remains largely under-explored.
Intuitively, the between-task competition can be used to guide the feature selection process that plays a key role when learning a visual classifier.
Motivated by this intuition, we propose a novel algorithm to exploit competition relationship for improving visual recognition tasks. More specifically, given a target task and its competing tasks, we jointly model them by a generalized additive regression model with  competition constraints. This constraint effectively discourages choosing of irrelevant features (weak learners) that are good for the 
 tasks with competition relationships . The proposed algorithm is named \emph{CompBoost} since it can be viewed extended from the RealAdaboost algorithm. We apply CompBoost to two visual recognition applications: (1) content-independent writer identification from handwriting scripts by exploiting competing tasks of handwriting recognition, and (2) actor-independent facial expression recognition by exploiting competing tasks of face recognition. In both experiments our approach demonstrates promising performance gains by exploiting the between-task competition relationships. Competition relationships
jointly learning
visual recognition",exploit competit relationship robust visual recognit joint learn similar task popular trend visual recognit proven benefici betweentask similar typic provid use cue featur share learn visual classifi contrast competit relationship visual recognit task eg content independ writer identif handwrit recognit remain larg underexplor intuit betweentask competit use guid featur select process play key role learn visual classifi motiv intuit propos novel algorithm exploit competit relationship improv visual recognit task specif given target task compet task joint model general addit regress model competit constraint constraint effect discourag choos irrelev featur weak learner good task competit relationship propos algorithm name emphcompboost sinc view extend realadaboost algorithm appli compboost two visual recognit applic 1 contentindepend writer identif handwrit script exploit compet task handwrit recognit 2 actorindepend facial express recognit exploit compet task face recognit experi approach demonstr promis perform gain exploit betweentask competit relationship competit relationship joint learn visual recognit,1,5.030856,-6.2889643
Power Iterated Color Refinement,"Kristian Kersting, Martin Mladenov, Roman Garnet and Martin Grohe","Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)","Lifted Belief Propagation
Color Refinement
Fractional Automorphism
Conditional Gradient
Power Iteration
Continuous Optimization","NMLA: Relational/Graph-Based Learning
RU: Relational Probabilistic Models","Color refinement is a basic algorithmic routine for graph isomorphism
testing and has recently been used for computing graph kernels as well as for lifting belief propagation and linear programming.  
So far, color refinement has been treated as a combinatorial 
problem. Instead, we treat it as a nonlinear continuous optimization problem and prove that
it implements a conditional gradient optimizer that can be turned into 
graph clustering approaches using hashing and truncated power iterations. This shows that color refinement 
is easy to understand in terms of (local) random walks, easy to implement (matrix-vector multiplications) and is readily parallelizable. 
We support our theoretical results with experimental evidence on real-world graphs with millions of edges.","Power Iterated Color Refinement Color refinement is a basic algorithmic routine for graph isomorphism
testing and has recently been used for computing graph kernels as well as for lifting belief propagation and linear programming.  
So far, color refinement has been treated as a combinatorial 
problem. Instead, we treat it as a nonlinear continuous optimization problem and prove that
it implements a conditional gradient optimizer that can be turned into 
graph clustering approaches using hashing and truncated power iterations. This shows that color refinement 
is easy to understand in terms of (local) random walks, easy to implement (matrix-vector multiplications) and is readily parallelizable. 
We support our theoretical results with experimental evidence on real-world graphs with millions of edges. Lifted Belief Propagation
Color Refinement
Fractional Automorphism
Conditional Gradient
Power Iteration
Continuous Optimization",power iter color refin color refin basic algorithm routin graph isomorph test recent use comput graph kernel well lift belief propag linear program far color refin treat combinatori problem instead treat nonlinear continu optim problem prove implement condit gradient optim turn graph cluster approach use hash truncat power iter show color refin easi understand term local random walk easi implement matrixvector multipl readili paralleliz support theoret result experiment evid realworld graph million edg lift belief propag color refin fraction automorph condit gradient power iter continu optim,4,-7.4430633,5.8854585
A Framework for Task Planning in Heterogeneous Multi Robot Systems Based on Robot Capabilities,Jennifer Buehler and Maurice Pagnucco,"Planning and Scheduling (PS)
Robotics (ROB)","Robot Task Planning
Temporal Planning
Heterogeneous Multi Robot Systems
Robot capabilities","PS: Temporal Planning
PS: Planning (General/Other)
ROB: Multi-Robot Systems
ROB: Robotics (General/Other)","In heterogeneous multi-robot teams, robustness and flexibility are increased by the diversity of the robots, each contributing different capabilities. Yet platform-
independence is desirable when planning actions for the various robots. This work develops a framework for task planning based on a platform-independent model
of robots capabilities, building on a temporal planner. Generating new data objects during planning is essential to reflect data flow between actions in a robotic system. This requires online action instantiation, for which we present a novel approach. Required concurrency of actions is an essential part of robotic systems and therefore is incorporated in the framework. We evaluate the planner on benchmark domains and present results on an example object transportation task in simulation.","A Framework for Task Planning in Heterogeneous Multi Robot Systems Based on Robot Capabilities In heterogeneous multi-robot teams, robustness and flexibility are increased by the diversity of the robots, each contributing different capabilities. Yet platform-
independence is desirable when planning actions for the various robots. This work develops a framework for task planning based on a platform-independent model
of robots capabilities, building on a temporal planner. Generating new data objects during planning is essential to reflect data flow between actions in a robotic system. This requires online action instantiation, for which we present a novel approach. Required concurrency of actions is an essential part of robotic systems and therefore is incorporated in the framework. We evaluate the planner on benchmark domains and present results on an example object transportation task in simulation. Robot Task Planning
Temporal Planning
Heterogeneous Multi Robot Systems
Robot capabilities",framework task plan heterogen multi robot system base robot capabl heterogen multirobot team robust flexibl increas divers robot contribut differ capabl yet platform independ desir plan action various robot work develop framework task plan base platformindepend model robot capabl build tempor planner generat new data object plan essenti reflect data flow action robot system requir onlin action instanti present novel approach requir concurr action essenti part robot system therefor incorpor framework evalu planner benchmark domain present result exampl object transport task simul robot task plan tempor plan heterogen multi robot system robot capabl,3,-0.6306254,11.529665
Active Learning Using Knowledge Transfer from Unlabeled Data,"Meng Fang, Jie Yin and Dacheng Tao",Novel Machine Learning Algorithms (NMLA),"Active learning
Multiple labelers
Knowledge transfer",NMLA: Active Learning,"This paper studies the problems associated with active learning, in which multiple users with varying levels of expertise are available for labeling data. Annotations collected from different users may be noisy and unreliable, and the quality of labeled data needs to be maintained for data mining tasks. Previous solutions have included estimating individual user reliability based on existing knowledge in each task, but for this to be effective each task requires a large quantity of labeled data to provide accurate estimates. In practice, annotation budgets for a given target task are limited, so each example can be presented to only a few users, each of whom can only label a few examples. To overcome data scarcity we propose a new probabilistic model that transfers knowledge from abundant unlabeled data in auxiliary domains to help estimate labelers' expertise. Based on this model we present a novel active learning algorithm that: a) simultaneously selects the most informative example and b) queries its label from the labeler with the best expertise. Experiments on both text and image datasets demonstrate that our proposed method outperforms other state-of-the-art active learning methods.","Active Learning Using Knowledge Transfer from Unlabeled Data This paper studies the problems associated with active learning, in which multiple users with varying levels of expertise are available for labeling data. Annotations collected from different users may be noisy and unreliable, and the quality of labeled data needs to be maintained for data mining tasks. Previous solutions have included estimating individual user reliability based on existing knowledge in each task, but for this to be effective each task requires a large quantity of labeled data to provide accurate estimates. In practice, annotation budgets for a given target task are limited, so each example can be presented to only a few users, each of whom can only label a few examples. To overcome data scarcity we propose a new probabilistic model that transfers knowledge from abundant unlabeled data in auxiliary domains to help estimate labelers' expertise. Based on this model we present a novel active learning algorithm that: a) simultaneously selects the most informative example and b) queries its label from the labeler with the best expertise. Experiments on both text and image datasets demonstrate that our proposed method outperforms other state-of-the-art active learning methods. Active learning
Multiple labelers
Knowledge transfer",activ learn use knowledg transfer unlabel data paper studi problem associ activ learn multipl user vari level expertis avail label data annot collect differ user may noisi unreli qualiti label data need maintain data mine task previous solut includ estim individu user reliabl base exist knowledg task effect task requir larg quantiti label data provid accur estim practic annot budget given target task limit exampl present user label exampl overcom data scarciti propos new probabilist model transfer knowledg abund unlabel data auxiliari domain help estim label expertis base model present novel activ learn algorithm simultan select inform exampl b queri label label best expertis experi text imag dataset demonstr propos method outperform stateoftheart activ learn method activ learn multipl label knowledg transfer,6,-12.043411,-10.1266365
Using Model-Based Diagnosis to Improve Software Testing,"Tom Zamir, Roni Stern and Meir Kalech","Applications (APP)
Knowledge Representation and Reasoning (KRR)
Planning and Scheduling (PS)","Model based diagnosis
Software engineering
Planning
Testing","APP: Other Applications
KRR: Automated Reasoning and Theorem Proving
KRR: Diagnosis and Abductive Reasoning
PS: Model-Based Reasoning","We propose a combination of AI techniques to improve software testing. When a test fails, a model-based diagnosis (MBD) algorithm is used to propose a set of possible explanations. We call these explanations ""diagnoses"". Then, a planning algorithm is used to suggest further tests to identify the correct diagnosis. A tester preforms these tests and reports their outcome back to the MBD algorithm, which uses this information to prune incorrect diagnoses. This iterative process continues until the correct diagnosis is returned. We call this testing paradigm Test, Diagnose and Plan (TDP). Several test planning algorithms are proposed to minimize the number of TDP iterations, and consequently the number of tests required until the correct diagnosis is found. Experimental results show that benefits of using an MDP-based planning algorithms over greedy test planning.","Using Model-Based Diagnosis to Improve Software Testing We propose a combination of AI techniques to improve software testing. When a test fails, a model-based diagnosis (MBD) algorithm is used to propose a set of possible explanations. We call these explanations ""diagnoses"". Then, a planning algorithm is used to suggest further tests to identify the correct diagnosis. A tester preforms these tests and reports their outcome back to the MBD algorithm, which uses this information to prune incorrect diagnoses. This iterative process continues until the correct diagnosis is returned. We call this testing paradigm Test, Diagnose and Plan (TDP). Several test planning algorithms are proposed to minimize the number of TDP iterations, and consequently the number of tests required until the correct diagnosis is found. Experimental results show that benefits of using an MDP-based planning algorithms over greedy test planning. Model based diagnosis
Software engineering
Planning
Testing",use modelbas diagnosi improv softwar test propos combin ai techniqu improv softwar test test fail modelbas diagnosi mbd algorithm use propos set possibl explan call explan diagnos plan algorithm use suggest test identifi correct diagnosi tester preform test report outcom back mbd algorithm use inform prune incorrect diagnos iter process continu correct diagnosi return call test paradigm test diagnos plan tdp sever test plan algorithm propos minim number tdp iter consequ number test requir correct diagnosi found experiment result show benefit use mdpbase plan algorithm greedi test plan model base diagnosi softwar engin plan test,1,2.073975,5.499566
Wormhole Hamiltonian Monte Carlo,"Shiwei Lan, Jeffrey Streets and Babak Shahbaba",Novel Machine Learning Algorithms (NMLA),"Bayesian Inference
Computational Statistics
Markov Chain Monte Carlo
Multimodal Distributions
Geometric Methods","NMLA: Bayesian Learning
NMLA: Machine Learning (General/other)","In machine learning and statistics, probabilistic inference involving multimodal distributions is quite difficult. This is especially true in high dimensional problems, where most existing algorithms cannot easily move from one mode to another. To address this issue, we propose a novel Bayesian inference approach based on Markov Chain Monte Carlo. Our method can effectively sample from multimodal distributions, especially when the dimension is high and the modes are isolated. To this end, it exploits and modifies the Riemannian geometric properties of the target distribution to create wormholes connecting modes in order to facilitate moving between them. Further, our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution. To find new modes, as opposed to rediscovering those previously identified, we employ a novel mode searching algorithm that explores a residual energy function obtained by subtracting an approximate Gaussian mixture density (based on previously discovered modes) from the target density function.","Wormhole Hamiltonian Monte Carlo In machine learning and statistics, probabilistic inference involving multimodal distributions is quite difficult. This is especially true in high dimensional problems, where most existing algorithms cannot easily move from one mode to another. To address this issue, we propose a novel Bayesian inference approach based on Markov Chain Monte Carlo. Our method can effectively sample from multimodal distributions, especially when the dimension is high and the modes are isolated. To this end, it exploits and modifies the Riemannian geometric properties of the target distribution to create wormholes connecting modes in order to facilitate moving between them. Further, our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution. To find new modes, as opposed to rediscovering those previously identified, we employ a novel mode searching algorithm that explores a residual energy function obtained by subtracting an approximate Gaussian mixture density (based on previously discovered modes) from the target density function. Bayesian Inference
Computational Statistics
Markov Chain Monte Carlo
Multimodal Distributions
Geometric Methods",wormhol hamiltonian mont carlo machin learn statist probabilist infer involv multimod distribut quit difficult especi true high dimension problem exist algorithm cannot easili move one mode anoth address issu propos novel bayesian infer approach base markov chain mont carlo method effect sampl multimod distribut especi dimens high mode isol end exploit modifi riemannian geometr properti target distribut creat wormhol connect mode order facilit move propos method use regener techniqu order adapt algorithm identifi new mode updat network wormhol without affect stationari distribut find new mode oppos rediscov previous identifi employ novel mode search algorithm explor residu energi function obtain subtract approxim gaussian mixtur densiti base previous discov mode target densiti function bayesian infer comput statist markov chain mont carlo multimod distribut geometr method,5,-6.2055063,-0.1009776
Pay-as-you-go OWL Query Answering Using a Triple Store,"Yujiao Zhou, Yavor Nenov, Bernardo Cuenca Grau and Ian Horrocks","AI and the Web (AIW)
Knowledge Representation and Reasoning (KRR)","OWL
ontologies
triple store","AIW: Searching, querying, visualizing, and interpreting the semantic web
KRR: Ontologies
KRR: Automated Reasoning and Theorem Proving
KRR: Description Logics","We present an enhanced hybrid approach to OWL query answering that combines an RDF triple-store with a fully-fledged OWL reasoner in order to provide scalable ``pay as you go'' performance. The enhancements presented here include an extension to deal with arbitrary OWL ontologies, and several optimisations that significantly improve scalability. We have implemented these techniques in a prototype system, a preliminary evaluation of which has produced very encouraging results.","Pay-as-you-go OWL Query Answering Using a Triple Store We present an enhanced hybrid approach to OWL query answering that combines an RDF triple-store with a fully-fledged OWL reasoner in order to provide scalable ``pay as you go'' performance. The enhancements presented here include an extension to deal with arbitrary OWL ontologies, and several optimisations that significantly improve scalability. We have implemented these techniques in a prototype system, a preliminary evaluation of which has produced very encouraging results. OWL
ontologies
triple store",payasyougo owl queri answer use tripl store present enhanc hybrid approach owl queri answer combin rdf triplestor fullyfledg owl reason order provid scalabl pay go perform enhanc present includ extens deal arbitrari owl ontolog sever optimis signific improv scalabl implement techniqu prototyp system preliminari evalu produc encourag result owl ontolog tripl store,8,22.68213,5.1844497
Imitation Learning with Demonstrations and Shaping Rewards,"Kshitij Judah, Alan Fern, Prasad Tadepalli and Robby Goetschalckx",Reasoning under Uncertainty (RU),"Imitation Learning
Reinforcement Learning
Reward Shaping",RU: Sequential Decision Making,"Imitation Learning (IL) is a popular approach for teaching behavior policies to agents by demonstrating the desired target policy. While the approach has lead to many successes, IL often requires a large set of demonstrations to achieve robust learning, which can be expensive for the teacher. In this paper, we consider a novel approach to improve the learning efficiency of IL by providing a shaping reward function in addition to the usual demonstrations. Shaping rewards are numeric functions of states (and possibly actions) that are generally easily specified and capture general principles of desired behavior, without necessarily completely specifying the behavior. Shaping rewards have been used extensively in reinforcement learning, but have been seldom considered for IL, though they are often easy to specify. Our main contribution is to propose an IL approach that learns from both shaping rewards and demonstrations. We demonstrate the effectiveness of the approach across several IL problems, even when the shaping reward is not fully consistent with the demonstrations.","Imitation Learning with Demonstrations and Shaping Rewards Imitation Learning (IL) is a popular approach for teaching behavior policies to agents by demonstrating the desired target policy. While the approach has lead to many successes, IL often requires a large set of demonstrations to achieve robust learning, which can be expensive for the teacher. In this paper, we consider a novel approach to improve the learning efficiency of IL by providing a shaping reward function in addition to the usual demonstrations. Shaping rewards are numeric functions of states (and possibly actions) that are generally easily specified and capture general principles of desired behavior, without necessarily completely specifying the behavior. Shaping rewards have been used extensively in reinforcement learning, but have been seldom considered for IL, though they are often easy to specify. Our main contribution is to propose an IL approach that learns from both shaping rewards and demonstrations. We demonstrate the effectiveness of the approach across several IL problems, even when the shaping reward is not fully consistent with the demonstrations. Imitation Learning
Reinforcement Learning
Reward Shaping",imit learn demonstr shape reward imit learn il popular approach teach behavior polici agent demonstr desir target polici approach lead mani success il often requir larg set demonstr achiev robust learn expens teacher paper consid novel approach improv learn effici il provid shape reward function addit usual demonstr shape reward numer function state possibl action general easili specifi captur general principl desir behavior without necessarili complet specifi behavior shape reward use extens reinforc learn seldom consid il though often easi specifi main contribut propos il approach learn shape reward demonstr demonstr effect approach across sever il problem even shape reward fulli consist demonstr imit learn reinforc learn reward shape,4,-2.4337335,-12.153019
Spectral Thompson Sampling,"Tomáš Kocák, Michal Valko, Remi Munos and Shipra Agrawal","Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)","Spectral bandits
Thompson Sampling
Smooth functions on graphs","NMLA: Online Learning
NMLA: Recommender Systems
RU: Sequential Decision Making","Thompson Sampling (TS) has  surged a lot of interest due to its good
empirical
performance, in particular in the computational advertising. Though successful,
the tools for its performance analysis appeared only recently.  In this paper,
we describe and analyze SpectralTS for a bandit problem, where
the payoffs of the choices are smooth given an underlying graph. In
this
setting, each choice is a node of a graph and the expected payoffs of the
neighboring nodes are assumed to be similar.  Although the setting has
application both in recommender systems and advertising, the traditional
algorithms would scale poorly with the number of choices. For that purpose we
consider an effective dimension $d$, which is small in real-world graphs.
Building on prior work, we deliver the analysis showing that the regret of
SpectralTS scales with $d\sqrt{T \ln N}$, where $T$ is the time
horizon and $N$ is the number of choices. Since a $d\sqrt{T \ln N}$ regret is
comparable to the known results, SpectralTS offers a computationally more
efficient alternative. We also show that our algorithm is competitive on both
synthetic and real-world data.","Spectral Thompson Sampling Thompson Sampling (TS) has  surged a lot of interest due to its good
empirical
performance, in particular in the computational advertising. Though successful,
the tools for its performance analysis appeared only recently.  In this paper,
we describe and analyze SpectralTS for a bandit problem, where
the payoffs of the choices are smooth given an underlying graph. In
this
setting, each choice is a node of a graph and the expected payoffs of the
neighboring nodes are assumed to be similar.  Although the setting has
application both in recommender systems and advertising, the traditional
algorithms would scale poorly with the number of choices. For that purpose we
consider an effective dimension $d$, which is small in real-world graphs.
Building on prior work, we deliver the analysis showing that the regret of
SpectralTS scales with $d\sqrt{T \ln N}$, where $T$ is the time
horizon and $N$ is the number of choices. Since a $d\sqrt{T \ln N}$ regret is
comparable to the known results, SpectralTS offers a computationally more
efficient alternative. We also show that our algorithm is competitive on both
synthetic and real-world data. Spectral bandits
Thompson Sampling
Smooth functions on graphs",spectral thompson sampl thompson sampl ts surg lot interest due good empir perform particular comput advertis though success tool perform analysi appear recent paper describ analyz spectralt bandit problem payoff choic smooth given under graph set choic node graph expect payoff neighbor node assum similar although set applic recommend system advertis tradit algorithm would scale poor number choic purpos consid effect dimens small realworld graph build prior work deliv analysi show regret spectralt scale dsqrtt ln n time horizon n number choic sinc dsqrtt ln n regret compar known result spectralt offer comput effici altern also show algorithm competit synthet realworld data spectral bandit thompson sampl smooth function graph,9,-2.275998,3.0015554
Mechanism Design for Scheduling with Uncertain Execution Time.,Vincent Conitzer and Angelina Vidali,"Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","mechanism design
scheduling mechanisms
game theory
uncertainity","GTEP: Auctions and Market-Based Systems
GTEP: Game Theory
MAS: Mechanism Design","We study the problem where a task (or multiple unrelated tasks) must be executed, there are multiple
machines/agents that can potentially perform the task, and our
objective is to minimize the expected sum of the agents' processing
times.  Each agent does not know exactly how long it will take him to
finish the task; he only knows the distribution from which this time
is drawn.  These times are independent across agents and the
distributions fulfill the monotone hazard rate condition.  Agents are
selfish and will lie about their distributions if this increases their
expected utility.

We study different variations of the Vickrey mechanism that take as input the agents' reported distributions and the players' realized running times and that output a schedule that minimizes the expected sum of processing times, as well as payments that make it an ex-post equilibrium for the agents to both truthfully report their
distributions and exert full effort to complete the task. We devise the ChPE mechanism, which is uniquely tailored to our problem, and has many desirable properties including: not rewarding agents that fail to finish the task and having non-negative payments.","Mechanism Design for Scheduling with Uncertain Execution Time. We study the problem where a task (or multiple unrelated tasks) must be executed, there are multiple
machines/agents that can potentially perform the task, and our
objective is to minimize the expected sum of the agents' processing
times.  Each agent does not know exactly how long it will take him to
finish the task; he only knows the distribution from which this time
is drawn.  These times are independent across agents and the
distributions fulfill the monotone hazard rate condition.  Agents are
selfish and will lie about their distributions if this increases their
expected utility.

We study different variations of the Vickrey mechanism that take as input the agents' reported distributions and the players' realized running times and that output a schedule that minimizes the expected sum of processing times, as well as payments that make it an ex-post equilibrium for the agents to both truthfully report their
distributions and exert full effort to complete the task. We devise the ChPE mechanism, which is uniquely tailored to our problem, and has many desirable properties including: not rewarding agents that fail to finish the task and having non-negative payments. mechanism design
scheduling mechanisms
game theory
uncertainity",mechan design schedul uncertain execut time studi problem task multipl unrel task must execut multipl machinesag potenti perform task object minim expect sum agent process time agent know exact long take finish task know distribut time drawn time independ across agent distribut fulfil monoton hazard rate condit agent selfish lie distribut increas expect util studi differ variat vickrey mechan take input agent report distribut player realiz run time output schedul minim expect sum process time well payment make expost equilibrium agent truth report distribut exert full effort complet task devis chpe mechan uniqu tailor problem mani desir properti includ reward agent fail finish task nonneg payment mechan design schedul mechan game theori uncertain,3,9.666946,15.778387
Robust Winners and Winner Determination Policies under Candidate Uncertainty,"Craig Boutilier, Jérôme Lang, Joel Oren and Hector Palacios","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","voting
candidate availability
query policies
robust winner determination","APP: Computational Social Science
GTEP: Social Choice / Voting","We consider voting situations in which some candidates may turn out to be unavailable. When determining availability is costly (e.g., in terms of money, time, or computation), voting prior to determining candidate availability and testing the winner's availability *after* the vote may be beneficial.  However, since few voting rules are robust to candidate deletion, winner determination requires a number of such availability tests. We outline a model for analyzing such problems, defining *robust winners* relative to potential candidate unavailability. We assess the complexity of computing robust winners for several voting rules.  Assuming a distribution over availability, and costs for availability tests/queries, we describe algorithms for *optimal query policies*, which minimize the expected cost of determining true winners.","Robust Winners and Winner Determination Policies under Candidate Uncertainty We consider voting situations in which some candidates may turn out to be unavailable. When determining availability is costly (e.g., in terms of money, time, or computation), voting prior to determining candidate availability and testing the winner's availability *after* the vote may be beneficial.  However, since few voting rules are robust to candidate deletion, winner determination requires a number of such availability tests. We outline a model for analyzing such problems, defining *robust winners* relative to potential candidate unavailability. We assess the complexity of computing robust winners for several voting rules.  Assuming a distribution over availability, and costs for availability tests/queries, we describe algorithms for *optimal query policies*, which minimize the expected cost of determining true winners. voting
candidate availability
query policies
robust winner determination",robust winner winner determin polici candid uncertainti consid vote situat candid may turn unavail determin avail cost eg term money time comput vote prior determin candid avail test winner avail vote may benefici howev sinc vote rule robust candid delet winner determin requir number avail test outlin model analyz problem defin robust winner relat potenti candid unavail assess complex comput robust winner sever vote rule assum distribut avail cost avail testsqueri describ algorithm optim queri polici minim expect cost determin true winner vote candid avail queri polici robust winner determin,9,1.9005818,-1.9734358
Learning Latent Engagement Patterns of Students in Online Courses,"Arti Ramesh, Dan Goldwasser, Bert Huang, Hal Daume Iii and Lise Getoor","Applications (APP)
Machine Learning Applications (MLA)","probabilistic modeling
structured prediction
data-driven methods in education
MOOC
online education","APP: Computer-Aided Education
HAI: Understanding People, Theories, Concepts and Methods
MLA: Machine Learning Applications (General/other)","Maintaining and cultivating student engagement is a critical component of education. In various teaching settings, communication via online forums, electronic quizzes, and interaction with multimedia can include valuable information for assessing and understanding student engagement. Massive open online courses (MOOCs) measure large-scale data of this nature and provide the opportunity for data-driven study. Characterizing student engagement as a course progresses helps identify student learning patterns and can aid in minimizing dropout rates, initiating instructor intervention. In this paper, we construct a probabilistic model connecting student behavior and class completion, formulating student engagement types as latent variables. We show that our model accurately identifies course success indicators, which can be used by instructors to initiate interventions and assist students.","Learning Latent Engagement Patterns of Students in Online Courses Maintaining and cultivating student engagement is a critical component of education. In various teaching settings, communication via online forums, electronic quizzes, and interaction with multimedia can include valuable information for assessing and understanding student engagement. Massive open online courses (MOOCs) measure large-scale data of this nature and provide the opportunity for data-driven study. Characterizing student engagement as a course progresses helps identify student learning patterns and can aid in minimizing dropout rates, initiating instructor intervention. In this paper, we construct a probabilistic model connecting student behavior and class completion, formulating student engagement types as latent variables. We show that our model accurately identifies course success indicators, which can be used by instructors to initiate interventions and assist students. probabilistic modeling
structured prediction
data-driven methods in education
MOOC
online education",learn latent engag pattern student onlin cours maintain cultiv student engag critic compon educ various teach set communic via onlin forum electron quizz interact multimedia includ valuabl inform assess understand student engag massiv open onlin cours mooc measur largescal data natur provid opportun datadriven studi character student engag cours progress help identifi student learn pattern aid minim dropout rate initi instructor intervent paper construct probabilist model connect student behavior class complet formul student engag type latent variabl show model accur identifi cours success indic use instructor initi intervent assist student probabilist model structur predict datadriven method educ mooc onlin educ,0,9.895704,-2.3587458
Semantic Segmentation Using Multiple Graphs with Block-Diagonal Constraints,"Ke Zhang, Wei Zhang, Sheng Zeng and Xiangyang Xue","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)
Vision (VIS)","Image Semantic Segmentation
Block-Diagonal Constraints
MultiView Affinity Graph","MLA: Applications of Supervised Learning
NMLA: Classification
NMLA: Relational/Graph-Based Learning
VIS: Categorization
VIS: Object Detection
VIS: Object Recognition
VIS: Statistical Methods and Learning","In this paper we propose a novel method for image semantic segmentation
using multiple graphs. The
multi-view affinity graph is constructed by leveraging the consistency between semantic space and multiple visual spaces.
With
block-diagonal constraints, we enforce the affinity matrix to be sparse such that the pairwise
potential for  dissimilar superpixels is close to zero. By a divide-and-conquer strategy, the optimization for learning affinity matrix is  decomposed into several subproblems that can be solved in parallel. Using the $neighborhood$ $relationship$ between superpixels and the $consistency$ between
affinity matrix  and label-confidence matrix, we infer the semantic label for each superpixel of unlabeled images by  minimizing an objective whose closed form solution can be easily obtained.
 Experimental results on two real-world
image datasets demonstrate the effectiveness of our method.","Semantic Segmentation Using Multiple Graphs with Block-Diagonal Constraints In this paper we propose a novel method for image semantic segmentation
using multiple graphs. The
multi-view affinity graph is constructed by leveraging the consistency between semantic space and multiple visual spaces.
With
block-diagonal constraints, we enforce the affinity matrix to be sparse such that the pairwise
potential for  dissimilar superpixels is close to zero. By a divide-and-conquer strategy, the optimization for learning affinity matrix is  decomposed into several subproblems that can be solved in parallel. Using the $neighborhood$ $relationship$ between superpixels and the $consistency$ between
affinity matrix  and label-confidence matrix, we infer the semantic label for each superpixel of unlabeled images by  minimizing an objective whose closed form solution can be easily obtained.
 Experimental results on two real-world
image datasets demonstrate the effectiveness of our method. Image Semantic Segmentation
Block-Diagonal Constraints
MultiView Affinity Graph",semant segment use multipl graph blockdiagon constraint paper propos novel method imag semant segment use multipl graph multiview affin graph construct leverag consist semant space multipl visual space blockdiagon constraint enforc affin matrix spars pairwis potenti dissimilar superpixel close zero divideandconqu strategi optim learn affin matrix decompos sever subproblem solv parallel use neighborhood relationship superpixel consist affin matrix labelconfid matrix infer semant label superpixel unlabel imag minim object whose close form solut easili obtain experiment result two realworld imag dataset demonstr effect method imag semant segment blockdiagon constraint multiview affin graph,1,-9.479008,10.564973
Parametrized Families of Hard Planning Problems from Phase Transitions,"Eleanor Rieffel, Davide Venturelli, Minh Do, Itay Hen and Jeremy Frank",Planning and Scheduling (PS),"parametrized families
scaling analysis
phase transition
benchmark planning problems","PS: Scheduling
PS: Planning (General/Other)","There are two complementary ways to evaluate planning algorithms: performance on benchmark problems derived from real applications and analysis of performance on parametrized families of problems with known properties. Prior to this work, few means of generating parametrized families of hard planning problems were known. We generate hard planning problems from the solvable/unsolvable phase transition region of well-studied NP-complete problems that map naturally to navigation and scheduling, aspects common to many planning domains. Our results confirm exponential scaling of hardness with problem size, even at very small problem sizes. We observe significant differences between state-of-the-art planners on these problem families, enabling us to gain insight into the relative strengths and weaknesses of these planners. These families provide complementary test sets exhibiting properties not found in existing benchmarks.","Parametrized Families of Hard Planning Problems from Phase Transitions There are two complementary ways to evaluate planning algorithms: performance on benchmark problems derived from real applications and analysis of performance on parametrized families of problems with known properties. Prior to this work, few means of generating parametrized families of hard planning problems were known. We generate hard planning problems from the solvable/unsolvable phase transition region of well-studied NP-complete problems that map naturally to navigation and scheduling, aspects common to many planning domains. Our results confirm exponential scaling of hardness with problem size, even at very small problem sizes. We observe significant differences between state-of-the-art planners on these problem families, enabling us to gain insight into the relative strengths and weaknesses of these planners. These families provide complementary test sets exhibiting properties not found in existing benchmarks. parametrized families
scaling analysis
phase transition
benchmark planning problems",parametr famili hard plan problem phase transit two complementari way evalu plan algorithm perform benchmark problem deriv real applic analysi perform parametr famili problem known properti prior work mean generat parametr famili hard plan problem known generat hard plan problem solvableunsolv phase transit region wellstudi npcomplet problem map natur navig schedul aspect common mani plan domain result confirm exponenti scale hard problem size even small problem size observ signific differ stateoftheart planner problem famili enabl us gain insight relat strength weak planner famili provid complementari test set exhibit properti found exist benchmark parametr famili scale analysi phase transit benchmark plan problem,3,-3.0628939,17.559408
On Dataless Hierarchical Text Classification,Yangqiu Song and Dan Roth,NLP and Machine Learning (NLPML),"Hierarchical Text Classification
Dataless Text Classification
Semantic Representation",NLPML: Text Classification,"In this paper, we systematically study the problem of dataless hierarchical text classification. Unlike standard text classification schemes that rely on supervised training, dataless classification depends on understanding the labels of the sought after categories and requires no labeled data. Given a collection of text documents and a set of labels, we show that understanding the labels can be used to categorize the documents to the corresponding categories. This is done by embedding both labels and documents in a semantic space that allows one to compute meaningful semantic similarity between a document and a potential label. We show that this scheme can be used to support accurate multiclass classification without any supervision. We study several semantic representations and show how to improve the classification using bootstrapping methods.  Our results show that bootstrapped dataless classification is competitive with supervised classification with thousands of labeled examples.","On Dataless Hierarchical Text Classification In this paper, we systematically study the problem of dataless hierarchical text classification. Unlike standard text classification schemes that rely on supervised training, dataless classification depends on understanding the labels of the sought after categories and requires no labeled data. Given a collection of text documents and a set of labels, we show that understanding the labels can be used to categorize the documents to the corresponding categories. This is done by embedding both labels and documents in a semantic space that allows one to compute meaningful semantic similarity between a document and a potential label. We show that this scheme can be used to support accurate multiclass classification without any supervision. We study several semantic representations and show how to improve the classification using bootstrapping methods.  Our results show that bootstrapped dataless classification is competitive with supervised classification with thousands of labeled examples. Hierarchical Text Classification
Dataless Text Classification
Semantic Representation",dataless hierarch text classif paper systemat studi problem dataless hierarch text classif unlik standard text classif scheme reli supervis train dataless classif depend understand label sought categori requir label data given collect text document set label show understand label use categor document correspond categori done embed label document semant space allow one comput meaning semant similar document potenti label show scheme use support accur multiclass classif without supervis studi sever semant represent show improv classif use bootstrap method result show bootstrap dataless classif competit supervis classif thousand label exampl hierarch text classif dataless text classif semant represent,6,-9.499338,-8.0421295
Confident Reasoning on Raven’s Progressive Matrices Tests,Keith McGreggor and Ashok Goel,Knowledge Representation and Reasoning (KRR),"Visual Representations
Reasoning
Analogy","KRR: Geometric, Spatial, and Temporal Reasoning
KRR: Knowledge Representation (General/Other)","We report a novel approach to addressing the Raven’s Progressive Matrices (RPM) tests, one based upon purely visual representations. Our technique introduces the calculation of confidence in an answer and the automatic adjustment of level of resolution if that confidence is insufficient. We first describe the nature of the visual analogies found on the RPM.  We then exhibit our algorithm and work through a detailed example.  Finally, we present the performance of our algorithm on the four major variants of the RPM tests, illustrating the impact of confidence.  This is the first such account of any computational model against the entirety of the Raven’s.","Confident Reasoning on Raven’s Progressive Matrices Tests We report a novel approach to addressing the Raven’s Progressive Matrices (RPM) tests, one based upon purely visual representations. Our technique introduces the calculation of confidence in an answer and the automatic adjustment of level of resolution if that confidence is insufficient. We first describe the nature of the visual analogies found on the RPM.  We then exhibit our algorithm and work through a detailed example.  Finally, we present the performance of our algorithm on the four major variants of the RPM tests, illustrating the impact of confidence.  This is the first such account of any computational model against the entirety of the Raven’s. Visual Representations
Reasoning
Analogy",confid reason raven progress matric test report novel approach address raven progress matric rpm test one base upon pure visual represent techniqu introduc calcul confid answer automat adjust level resolut confid insuffici first describ natur visual analog found rpm exhibit algorithm work detail exampl final present perform algorithm four major variant rpm test illustr impact confid first account comput model entireti raven visual represent reason analog,1,5.70226,-4.1625404
The Role of Dimensionality Reduction in Linear Classification,Weiran Wang and Miguel Carreira-Perpinan,Novel Machine Learning Algorithms (NMLA),"dimensionality reduction
nonlinear classification
optimization","HSO: Optimization
NMLA: Classification
NMLA: Dimension Reduction/Feature Selection","Dimensionality reduction (DR) is often used as a preprocessing step in classification, but usually one first fixes the DR mapping, possibly using label information, and then learns a classifier (a filter approach). Best performance would be obtained by optimizing the classification error jointly over DR mapping and classifier (a wrapper approach), but this is a difficult nonconvex problem, particularly with nonlinear DR. Using the method of auxiliary coordinates, we give a simple, efficient algorithm to train a combination of nonlinear DR and a classifier, and apply it to a RBF mapping with a linear SVM. This alternates steps where we train the RBF mapping and a linear SVM as usual regression and classification, respectively, with a closed-form step that coordinates both. The resulting nonlinear low-dimensional classifier achieves classification errors competitive with the state-of-the-art but is fast at training and testing, and allows the user to trade off runtime for classification accuracy easily. We then study the role of nonlinear DR in linear classification, and the interplay between the DR mapping, the number of latent dimensions and the number of classes. When trained jointly, the DR mapping takes an extreme role in eliminating variation: it tends to collapse classes in latent space, erasing all manifold structure, and lay out class centroids so they are linearly separable with maximum margin.","The Role of Dimensionality Reduction in Linear Classification Dimensionality reduction (DR) is often used as a preprocessing step in classification, but usually one first fixes the DR mapping, possibly using label information, and then learns a classifier (a filter approach). Best performance would be obtained by optimizing the classification error jointly over DR mapping and classifier (a wrapper approach), but this is a difficult nonconvex problem, particularly with nonlinear DR. Using the method of auxiliary coordinates, we give a simple, efficient algorithm to train a combination of nonlinear DR and a classifier, and apply it to a RBF mapping with a linear SVM. This alternates steps where we train the RBF mapping and a linear SVM as usual regression and classification, respectively, with a closed-form step that coordinates both. The resulting nonlinear low-dimensional classifier achieves classification errors competitive with the state-of-the-art but is fast at training and testing, and allows the user to trade off runtime for classification accuracy easily. We then study the role of nonlinear DR in linear classification, and the interplay between the DR mapping, the number of latent dimensions and the number of classes. When trained jointly, the DR mapping takes an extreme role in eliminating variation: it tends to collapse classes in latent space, erasing all manifold structure, and lay out class centroids so they are linearly separable with maximum margin. dimensionality reduction
nonlinear classification
optimization",role dimension reduct linear classif dimension reduct dr often use preprocess step classif usual one first fix dr map possibl use label inform learn classifi filter approach best perform would obtain optim classif error joint dr map classifi wrapper approach difficult nonconvex problem particular nonlinear dr use method auxiliari coordin give simpl effici algorithm train combin nonlinear dr classifi appli rbf map linear svm altern step train rbf map linear svm usual regress classif respect closedform step coordin result nonlinear lowdimension classifi achiev classif error competit stateoftheart fast train test allow user trade runtim classif accuraci easili studi role nonlinear dr linear classif interplay dr map number latent dimens number class train joint dr map take extrem role elimin variat tend collaps class latent space eras manifold structur lay class centroid linear separ maximum margin dimension reduct nonlinear classif optim,6,-6.150245,-5.821837
Generalizing Policy Advice with Gaussian Process Bandits for Dynamic Skill Improvement,Jared Glover and Charlotte Zhu,"Heuristic Search and Optimization (HSO)
Humans and AI (HAI)
Machine Learning Applications (MLA)
Reasoning under Uncertainty (RU)
Robotics (ROB)","robot table tennis
gaussian process bandits
human advice
coaching robots","HSO: Heuristic Search
HAI: Human-Computer Interaction
MLA: Machine Learning Applications (General/other)
RU: Decision/Utility Theory
RU: Sequential Decision Making
ROB: Robotics (General/Other)","We present a ping-pong-playing robot that learns to improve its swings with human advice.  Our method learns a reward function over the joint space of task and policy parameters.  This allows the robot to explore policy space more intelligently by leveraging active learning techniques to explore the reward surface
in a way that trades off exploration vs. exploitation to maximize the total cumulative reward over time.  Multimodal stochastic polices can also easily be learned with this approach when the reward function is multimodal in the policy parameters.  We extend the recently-developed Gaussian Process Bandit
Optimization framework to include advice from human domain experts.","Generalizing Policy Advice with Gaussian Process Bandits for Dynamic Skill Improvement We present a ping-pong-playing robot that learns to improve its swings with human advice.  Our method learns a reward function over the joint space of task and policy parameters.  This allows the robot to explore policy space more intelligently by leveraging active learning techniques to explore the reward surface
in a way that trades off exploration vs. exploitation to maximize the total cumulative reward over time.  Multimodal stochastic polices can also easily be learned with this approach when the reward function is multimodal in the policy parameters.  We extend the recently-developed Gaussian Process Bandit
Optimization framework to include advice from human domain experts. robot table tennis
gaussian process bandits
human advice
coaching robots",general polici advic gaussian process bandit dynam skill improv present pingpongplay robot learn improv swing human advic method learn reward function joint space task polici paramet allow robot explor polici space intellig leverag activ learn techniqu explor reward surfac way trade explor vs exploit maxim total cumul reward time multimod stochast polic also easili learn approach reward function multimod polici paramet extend recentlydevelop gaussian process bandit optim framework includ advic human domain expert robot tabl tenni gaussian process bandit human advic coach robot,4,-0.97578454,-10.188856
Deep Modeling of Group Preferences for Group-based Recommendation,"Liang Hu, Jian Cao, Guandong Xu, Longbing Cao, Zhiping Gu and Wei Cao","AI and the Web (AIW)
Novel Machine Learning Algorithms (NMLA)","Group Recommender System
Deep Learning
Feature Learning
Deep Belief Network
Restricted Boltzmann Machine","AIW: Web-based recommendation systems
NMLA: Preferences/Ranking Learning
NMLA: Recommender Systems","Nowadays, most recommender systems (RSs) mainly aim to suggest appropriate items for individuals. Due to the social nature of human beings, group activities have become an integral part of our daily life, thus motivating the study on group RS (GRS). However, most existing methods used by GRS make recommendations through aggregating individual ratings or individual predictive results rather than considering the collective features that govern user choices made within a group. As a result, such methods are heavily sensitive to data, hence they often fail to learn group preferences when the data are slightly inconsistent with predefined aggregation assumptions. To this end, we devise a novel GRS approach which accommodates both individual choices and group decisions in a joint model. More specifically, we propose a deep-architecture model built with a collective deep belief network and dual-wing restricted Boltzmann machine. With such a deep model, we can use high-level features, which are induced from lower-level features, to represent group preference so as to relieve the vulnerability of data. Finally, the experiments conducted on a real-world dataset prove the superiority of our deep model over other state-of-the-art methods.","Deep Modeling of Group Preferences for Group-based Recommendation Nowadays, most recommender systems (RSs) mainly aim to suggest appropriate items for individuals. Due to the social nature of human beings, group activities have become an integral part of our daily life, thus motivating the study on group RS (GRS). However, most existing methods used by GRS make recommendations through aggregating individual ratings or individual predictive results rather than considering the collective features that govern user choices made within a group. As a result, such methods are heavily sensitive to data, hence they often fail to learn group preferences when the data are slightly inconsistent with predefined aggregation assumptions. To this end, we devise a novel GRS approach which accommodates both individual choices and group decisions in a joint model. More specifically, we propose a deep-architecture model built with a collective deep belief network and dual-wing restricted Boltzmann machine. With such a deep model, we can use high-level features, which are induced from lower-level features, to represent group preference so as to relieve the vulnerability of data. Finally, the experiments conducted on a real-world dataset prove the superiority of our deep model over other state-of-the-art methods. Group Recommender System
Deep Learning
Feature Learning
Deep Belief Network
Restricted Boltzmann Machine",deep model group prefer groupbas recommend nowaday recommend system rss main aim suggest appropri item individu due social natur human be group activ becom integr part daili life thus motiv studi group rs grs howev exist method use grs make recommend aggreg individu rate individu predict result rather consid collect featur govern user choic made within group result method heavili sensit data henc often fail learn group prefer data slight inconsist predefin aggreg assumpt end devis novel grs approach accommod individu choic group decis joint model specif propos deeparchitectur model built collect deep belief network dualw restrict boltzmann machin deep model use highlevel featur induc lowerlevel featur repres group prefer reliev vulner data final experi conduct realworld dataset prove superior deep model stateoftheart method group recommend system deep learn featur learn deep belief network restrict boltzmann machin,0,14.943422,-0.6690102
Cross-Domain Metric Learning Based on Information Theory,Wei Wang,Novel Machine Learning Algorithms (NMLA),"Mahalanobis distance
metric learning
transfer learning
relative entropy","NMLA: Transfer, Adaptation, Multitask Learning","Supervised metric learning plays a substantial role in statistical classification. Conventional metric learning algorithms have limited utility when the training data and the testing data are drawn from related but different domains (i.e., source domain and target domain). Although this issue has got some progress in feature-based transfer learning, most of the work in this area suffers from non-trivial optimization and pays little attention to preserving the discriminating information. In this paper, we propose a novel metric learning algorithm to transfer knowledge from the source domain to the target domain in an information-theoretic setting, where a shared Mahalanobis distance across two domains is learnt by combining  three goals together: 1) reducing the distribution difference between different domains; 2) preserving the geometry of target domain data; 3) aligning the geometry of source domain data with its label information. Based on this combination, the learnt Mahalanobis distance   effectively transfers the discriminating power and propagates  standard classifiers  across two domains. More importantly, our proposed method has  closed-form solution and can be efficiently optimized. Experiments on two real-world applications (i.e., face recognition and text classification) demonstrate the effectiveness and efficiency of our proposed method.","Cross-Domain Metric Learning Based on Information Theory Supervised metric learning plays a substantial role in statistical classification. Conventional metric learning algorithms have limited utility when the training data and the testing data are drawn from related but different domains (i.e., source domain and target domain). Although this issue has got some progress in feature-based transfer learning, most of the work in this area suffers from non-trivial optimization and pays little attention to preserving the discriminating information. In this paper, we propose a novel metric learning algorithm to transfer knowledge from the source domain to the target domain in an information-theoretic setting, where a shared Mahalanobis distance across two domains is learnt by combining  three goals together: 1) reducing the distribution difference between different domains; 2) preserving the geometry of target domain data; 3) aligning the geometry of source domain data with its label information. Based on this combination, the learnt Mahalanobis distance   effectively transfers the discriminating power and propagates  standard classifiers  across two domains. More importantly, our proposed method has  closed-form solution and can be efficiently optimized. Experiments on two real-world applications (i.e., face recognition and text classification) demonstrate the effectiveness and efficiency of our proposed method. Mahalanobis distance
metric learning
transfer learning
relative entropy",crossdomain metric learn base inform theori supervis metric learn play substanti role statist classif convent metric learn algorithm limit util train data test data drawn relat differ domain ie sourc domain target domain although issu got progress featurebas transfer learn work area suffer nontrivi optim pay littl attent preserv discrimin inform paper propos novel metric learn algorithm transfer knowledg sourc domain target domain informationtheoret set share mahalanobi distanc across two domain learnt combin three goal togeth 1 reduc distribut differ differ domain 2 preserv geometri target domain data 3 align geometri sourc domain data label inform base combin learnt mahalanobi distanc effect transfer discrimin power propag standard classifi across two domain import propos method closedform solut effici optim experi two realworld applic ie face recognit text classif demonstr effect effici propos method mahalanobi distanc metric learn transfer learn relat entropi,6,-15.364147,-11.321024
Spatial Scan for Disease Mapping on a Mobile Population,"Liang Lan, Vuk Malbasa and Slobodan Vucetic","Applications (APP)
Computational Sustainability and AI (CSAI)
Machine Learning Applications (MLA)","disease mapping
spatial scan
mobile data","APP: AI and Natural Sciences
CSAI: Modeling and prediction of dynamic and spatiotemporal phenomena and systems
CSAI: Control and optimization of dynamic and spatiotemporal systems
MLA: Machine Learning Applications (General/other)
NMLA: Machine Learning (General/other)","Spatial scan statistics is used for discovery of spatial regions with significantly higher scores according to some density measure. In disease surveillance, spatial scan is a standard tool to detect spatial regions whose population has significantly higher disease risk than the overall population. In this important application, called the disease mapping, current residence is typically used to define the location of individuals from the population. Considering the mobility of humans at various temporal and spatial scales, using only information about the current residence can be insufficient because it ignores a multitude of exposures that occur away from home or which had occurred at previous residences. In this paper, we propose a novel spatial scan statistic that allows disease mapping in a mobile population. We also propose a computationally efficient disease mapping algorithm that uses the proposed statistic to find the significant high-risk spatial regions. The experimental results demonstrate that the proposed algorithm is superior to the traditional disease mapping algorithms in discovering high-risk regions in mobile populations. Moreover, the algorithm is applicable on large populations and over dense spatial grids.","Spatial Scan for Disease Mapping on a Mobile Population Spatial scan statistics is used for discovery of spatial regions with significantly higher scores according to some density measure. In disease surveillance, spatial scan is a standard tool to detect spatial regions whose population has significantly higher disease risk than the overall population. In this important application, called the disease mapping, current residence is typically used to define the location of individuals from the population. Considering the mobility of humans at various temporal and spatial scales, using only information about the current residence can be insufficient because it ignores a multitude of exposures that occur away from home or which had occurred at previous residences. In this paper, we propose a novel spatial scan statistic that allows disease mapping in a mobile population. We also propose a computationally efficient disease mapping algorithm that uses the proposed statistic to find the significant high-risk spatial regions. The experimental results demonstrate that the proposed algorithm is superior to the traditional disease mapping algorithms in discovering high-risk regions in mobile populations. Moreover, the algorithm is applicable on large populations and over dense spatial grids. disease mapping
spatial scan
mobile data",spatial scan diseas map mobil popul spatial scan statist use discoveri spatial region signific higher score accord densiti measur diseas surveil spatial scan standard tool detect spatial region whose popul signific higher diseas risk overal popul import applic call diseas map current resid typic use defin locat individu popul consid mobil human various tempor spatial scale use inform current resid insuffici ignor multitud exposur occur away home occur previous resid paper propos novel spatial scan statist allow diseas map mobil popul also propos comput effici diseas map algorithm use propos statist find signific highrisk spatial region experiment result demonstr propos algorithm superior tradit diseas map algorithm discov highrisk region mobil popul moreov algorithm applic larg popul dens spatial grid diseas map spatial scan mobil data,4,-1.4715364,-0.28141367
Identifying Hierarchies for Fast Optimal Search,Tansel Uras and Sven Koenig,Heuristic Search and Optimization (HSO),"Hierarchical search
Path planning
Subgoals",HSO: Heuristic Search,"Search with Subgoal Graphs (Uras, Koenig, and Hernandez 2013) was a non-dominated optimal path-planning algorithm in the Grid-Based Path Planning Competitions 2012 and 2013. During a preprocessing phase, it computes a Simple Subgoal Graph, which is analogous to a visibility graph for continuous terrain, and then partitions the subgoals into global and local subgoals to obtain a Two-Level Subgoal Graph. During the path-planning phase, it performs an A* search that ignores local subgoals that are not relevant to the search, which significantly reduces the size of the graph being searched.

In this paper, we generalize this partitioning process to any undirected graph and show that it can be recursively applied to generate more than two levels, which reduces the size of the graph being searched even further. We distinguish between basic partitioning, which only partitions the vertices into different levels, and advanced partitioning, which can also add new edges. We show that the construction of Simple Subgoal Graphs from grids and the construction of Two-Level Subgoal Graphs from Simple Subgoal Graphs are instances of generalized partitioning. We then report on experiments on Subgoal Graphs that demonstrate the effects of different types and levels of partitioning. We also report on experiments that demonstrate that our new N-Level Subgoal Graphs with several additional improvements achieve a better performance compared to Two-Level Subgoal graphs from (Uras, Koenig, and Hernandez 2013).","Identifying Hierarchies for Fast Optimal Search Search with Subgoal Graphs (Uras, Koenig, and Hernandez 2013) was a non-dominated optimal path-planning algorithm in the Grid-Based Path Planning Competitions 2012 and 2013. During a preprocessing phase, it computes a Simple Subgoal Graph, which is analogous to a visibility graph for continuous terrain, and then partitions the subgoals into global and local subgoals to obtain a Two-Level Subgoal Graph. During the path-planning phase, it performs an A* search that ignores local subgoals that are not relevant to the search, which significantly reduces the size of the graph being searched.

In this paper, we generalize this partitioning process to any undirected graph and show that it can be recursively applied to generate more than two levels, which reduces the size of the graph being searched even further. We distinguish between basic partitioning, which only partitions the vertices into different levels, and advanced partitioning, which can also add new edges. We show that the construction of Simple Subgoal Graphs from grids and the construction of Two-Level Subgoal Graphs from Simple Subgoal Graphs are instances of generalized partitioning. We then report on experiments on Subgoal Graphs that demonstrate the effects of different types and levels of partitioning. We also report on experiments that demonstrate that our new N-Level Subgoal Graphs with several additional improvements achieve a better performance compared to Two-Level Subgoal graphs from (Uras, Koenig, and Hernandez 2013). Hierarchical search
Path planning
Subgoals",identifi hierarchi fast optim search search subgoal graph ura koenig hernandez 2013 nondomin optim pathplan algorithm gridbas path plan competit 2012 2013 preprocess phase comput simpl subgoal graph analog visibl graph continu terrain partit subgoal global local subgoal obtain twolevel subgoal graph pathplan phase perform search ignor local subgoal relev search signific reduc size graph search paper general partit process undirect graph show recurs appli generat two level reduc size graph search even distinguish basic partit partit vertic differ level advanc partit also add new edg show construct simpl subgoal graph grid construct twolevel subgoal graph simpl subgoal graph instanc general partit report experi subgoal graph demonstr effect differ type level partit also report experi demonstr new nlevel subgoal graph sever addit improv achiev better perform compar twolevel subgoal graph ura koenig hernandez 2013 hierarch search path plan subgoal,5,-7.5712276,7.5322433
On Boosting Sparse Parities,Lev Reyzin,Novel Machine Learning Algorithms (NMLA),"choosing weak learners
boosting
parity functions","NMLA: Ensemble Methods
NMLA: Machine Learning (General/other)","While the ensemble method of boosting has been extensively studied, considerably less attention has been devoted to the task of designing good weak learning algorithms. In this paper we consider the problem of designing weak learners that are especially adept to the boosting procedure and  specifically the AdaBoost algorithm.

First we describe conditions desirable for a weak learning algorithm.  We then propose using sparse parity functions as weak learners, which have many of our desired properties, as weak learners in boosting.  Our experimental tests show the proposed weak learners to be competitive with the most widely used ones: decision stumps and pruned decision trees.","On Boosting Sparse Parities While the ensemble method of boosting has been extensively studied, considerably less attention has been devoted to the task of designing good weak learning algorithms. In this paper we consider the problem of designing weak learners that are especially adept to the boosting procedure and  specifically the AdaBoost algorithm.

First we describe conditions desirable for a weak learning algorithm.  We then propose using sparse parity functions as weak learners, which have many of our desired properties, as weak learners in boosting.  Our experimental tests show the proposed weak learners to be competitive with the most widely used ones: decision stumps and pruned decision trees. choosing weak learners
boosting
parity functions",boost spars pariti ensembl method boost extens studi consider less attent devot task design good weak learn algorithm paper consid problem design weak learner especi adept boost procedur specif adaboost algorithm first describ condit desir weak learn algorithm propos use spars pariti function weak learner mani desir properti weak learner boost experiment test show propos weak learner competit wide use one decis stump prune decis tree choos weak learner boost pariti function,4,0.07557226,-5.44
Decentralized Stochastic Planning with Anonymity in Interactions,"Pradeep Varakantham, Yossiri Adulyasak and Patrick Jaillet","Multiagent Systems (MAS)
Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","Planning under uncertainty
Multiagent Systems
DEC-MDP
Optimization","PS: Markov Models of Environments
RU: Decision/Utility Theory
RU: Uncertainty in AI (General/Other)","In this paper, we solve cooperative decentralized stochastic planning problems, where the interactions between agents (specified using transition and reward functions) are dependent on the number of agents (and not on the identity of the individual agents) involved in the interaction. A collision of robots in a narrow corridor, defender teams coordinating patrol activities to secure a target, etc. are examples of such anonymous interactions.  Formally, we consider problems that are a subset of the well known Decentralized MDP (DEC-MDP) model, where the anonymity in interactions is specified within the joint reward and transition functions. In this paper, we make the following key contributions:\\
(a) A generic model, Decentralized Stochastic Planning with Anonymous InteracTions (D-SPAIT) to represent stochastic planning problems in cooperative domains with anonymity in interactions.\\
(b) An optimization based formulation along with theoretical results that establish scalability properties for general D-SPAIT problems.  \\
(c) Optimization formulations, whose scalability has little or no dependence on the number of agents, for solving certain classes of D-SPAIT problems optimally. \\
(d) Finally, we demonstrate the performance of our optimization approaches on randomly generated benchmark problems from the literature.","Decentralized Stochastic Planning with Anonymity in Interactions In this paper, we solve cooperative decentralized stochastic planning problems, where the interactions between agents (specified using transition and reward functions) are dependent on the number of agents (and not on the identity of the individual agents) involved in the interaction. A collision of robots in a narrow corridor, defender teams coordinating patrol activities to secure a target, etc. are examples of such anonymous interactions.  Formally, we consider problems that are a subset of the well known Decentralized MDP (DEC-MDP) model, where the anonymity in interactions is specified within the joint reward and transition functions. In this paper, we make the following key contributions:\\
(a) A generic model, Decentralized Stochastic Planning with Anonymous InteracTions (D-SPAIT) to represent stochastic planning problems in cooperative domains with anonymity in interactions.\\
(b) An optimization based formulation along with theoretical results that establish scalability properties for general D-SPAIT problems.  \\
(c) Optimization formulations, whose scalability has little or no dependence on the number of agents, for solving certain classes of D-SPAIT problems optimally. \\
(d) Finally, we demonstrate the performance of our optimization approaches on randomly generated benchmark problems from the literature. Planning under uncertainty
Multiagent Systems
DEC-MDP
Optimization",decentr stochast plan anonym interact paper solv cooper decentr stochast plan problem interact agent specifi use transit reward function depend number agent ident individu agent involv interact collis robot narrow corridor defend team coordin patrol activ secur target etc exampl anonym interact formal consid problem subset well known decentr mdp decmdp model anonym interact specifi within joint reward transit function paper make follow key contribut generic model decentr stochast plan anonym interact dspait repres stochast plan problem cooper domain anonym interact b optim base formul along theoret result establish scalabl properti general dspait problem c optim formul whose scalabl littl depend number agent solv certain class dspait problem optim final demonstr perform optim approach random generat benchmark problem literatur plan uncertainti multiag system decmdp optim,3,1.4436873,13.488938
Regret Transfer and Parameter Optimization,Noam Brown and Tuomas Sandholm,Game Theory and Economic Paradigms (GTEP),"Large Incomplete-Information Games
Poker
Game Solving
No-Regret Learning
Counterfactual Regret Minimization
Regret Matching
Regret Minimization","GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information","Regret matching is a widely-used algorithm for learning how to act.

We begin by proving that regrets on actions in one setting (game) can be transferred to warm start the regrets for solving a different setting with same structure but different payoffs that can be written as a function of parameters.  We prove how this can be done by carefully discounting the prior regrets. This provides, to our knowledge, the first warm-starting method for no-regret learning.  It also extends to warm-starting the widely-adopted counterfactual regret minimization (CFR) algorithm for large incomplete-information games; we show this experimentally as well.

We then study optimizing a parameter vector for a player in a two-player zero-sum game (e.g., optimizing bet sizes to use in poker). We propose a custom gradient descent algorithm that provably finds a locally optimal parameter vector while leveraging our warm-start theory to significantly save regret-matching iterations at each step. It optimizes the parameter vector while simultaneously finding an equilibrium. We present experiments in no-limit Leduc Hold'em and no-limit Texas Hold'em  to optimize bet sizing.  This amounts to the first action abstraction algorithm (algorithm for selecting a small number of discrete actions to use from a continuum of actions---a key preprocessing step for solving large games using current equilibrium-finding algorithms) with convergence guarantees for extensive-form games.","Regret Transfer and Parameter Optimization Regret matching is a widely-used algorithm for learning how to act.

We begin by proving that regrets on actions in one setting (game) can be transferred to warm start the regrets for solving a different setting with same structure but different payoffs that can be written as a function of parameters.  We prove how this can be done by carefully discounting the prior regrets. This provides, to our knowledge, the first warm-starting method for no-regret learning.  It also extends to warm-starting the widely-adopted counterfactual regret minimization (CFR) algorithm for large incomplete-information games; we show this experimentally as well.

We then study optimizing a parameter vector for a player in a two-player zero-sum game (e.g., optimizing bet sizes to use in poker). We propose a custom gradient descent algorithm that provably finds a locally optimal parameter vector while leveraging our warm-start theory to significantly save regret-matching iterations at each step. It optimizes the parameter vector while simultaneously finding an equilibrium. We present experiments in no-limit Leduc Hold'em and no-limit Texas Hold'em  to optimize bet sizing.  This amounts to the first action abstraction algorithm (algorithm for selecting a small number of discrete actions to use from a continuum of actions---a key preprocessing step for solving large games using current equilibrium-finding algorithms) with convergence guarantees for extensive-form games. Large Incomplete-Information Games
Poker
Game Solving
No-Regret Learning
Counterfactual Regret Minimization
Regret Matching
Regret Minimization",regret transfer paramet optim regret match widelyus algorithm learn act begin prove regret action one set game transfer warm start regret solv differ set structur differ payoff written function paramet prove done care discount prior regret provid knowledg first warmstart method noregret learn also extend warmstart widelyadopt counterfactu regret minim cfr algorithm larg incompleteinform game show experiment well studi optim paramet vector player twoplay zerosum game eg optim bet size use poker propos custom gradient descent algorithm provabl find local optim paramet vector leverag warmstart theori signific save regretmatch iter step optim paramet vector simultan find equilibrium present experi nolimit leduc holdem nolimit texa holdem optim bet size amount first action abstract algorithm algorithm select small number discret action use continuum actionsa key preprocess step solv larg game use current equilibriumfind algorithm converg guarante extensiveform game larg incompleteinform game poker game solv noregret learn counterfactu regret minim regret match regret minim,2,5.602191,20.534786
Learning with Augmented Class by Exploiting Unlabeled Data,"Qing Da, Yang Yu and Zhi-Hua Zhou",Novel Machine Learning Algorithms (NMLA),"open set classification
unlabeled data
support vector machines",NMLA: Classification,"In many real-world applications of learning, the environment is open and changes gradually, which requires the system to have the ability of detecting and adapting to the changes. Class-incremental learning (C-IL) is an important and practical problem where data from unseen augmented classes are fed, but has not been studied well in the past. In C-IL, the system should beware of predicting instances from augmented classes as a seen class, and thus faces the challenge that no such instances were shown in training. In this paper, we investigate tackling the challenge by using unlabeled data, which can be cheaply collected in many real-world applications. We propose the LACU framework as well as the LACU-SVM approach to learn the concept of seen classes while incorporating the structure presented in the unlabeled data, so that the misclassification risks among the seen classes as well as between the augmented and the seen classes are minimized simultaneously. Experiments on diverse datasets show the effectiveness of the proposed approach.","Learning with Augmented Class by Exploiting Unlabeled Data In many real-world applications of learning, the environment is open and changes gradually, which requires the system to have the ability of detecting and adapting to the changes. Class-incremental learning (C-IL) is an important and practical problem where data from unseen augmented classes are fed, but has not been studied well in the past. In C-IL, the system should beware of predicting instances from augmented classes as a seen class, and thus faces the challenge that no such instances were shown in training. In this paper, we investigate tackling the challenge by using unlabeled data, which can be cheaply collected in many real-world applications. We propose the LACU framework as well as the LACU-SVM approach to learn the concept of seen classes while incorporating the structure presented in the unlabeled data, so that the misclassification risks among the seen classes as well as between the augmented and the seen classes are minimized simultaneously. Experiments on diverse datasets show the effectiveness of the proposed approach. open set classification
unlabeled data
support vector machines",learn augment class exploit unlabel data mani realworld applic learn environ open chang gradual requir system abil detect adapt chang classincrement learn cil import practic problem data unseen augment class fed studi well past cil system bewar predict instanc augment class seen class thus face challeng instanc shown train paper investig tackl challeng use unlabel data cheapli collect mani realworld applic propos lacu framework well lacusvm approach learn concept seen class incorpor structur present unlabel data misclassif risk among seen class well augment seen class minim simultan experi divers dataset show effect propos approach open set classif unlabel data support vector machin,6,-6.12672,-8.914493
Locality-constrained Low-rank Coding for Image Classification,"Ziheng Jiang, Ping Guo and Lihong Peng",Vision (VIS),"Bag-of-words Model
Mid-level Representations
Locality Coding
Low-rank Coding
Inexact Augmented Lagrange Multiplier","VIS: Categorization
VIS: Face and Gesture Recognition
VIS: Object Recognition","Low-rank coding (LRC), originated from matrix decomposition, is recently introduced into image classification. Following the standard bag-of-words (BOW) pipeline, coding the data matrix in the sense of low-rankness incorporates contextual information into the traditional BOW model and it can capture the dependency relationship among neighbor patches. This differs from the traditional sparse coding paradigms which encode patches independently. Current LRC-based methods use l1 norm to increase the discrimination and sparsity of the learned codes. However, such methods fail to consider the local manifold structure between data space and dictionary space. To solve this problem, we propose a locality-constrained low-rank coding (LCLR) algorithm for image representations. By using the geometric structure information as a regularization term, the obtained representations are more discriminative. In addition, we present a fast and stable online algorithm to solve the optimization problem. In the experiments, we evaluate LCLR on four benchmarks, including one face recognition dataset (extended Yale B), one handwritten digit recognition dataset (USPS), and two image datasets (Scene13 for scene recognition and Caltech101 for object recognition). Experimental results show that our approach outperforms many state-of-the-art algorithms even with a linear classifier.","Locality-constrained Low-rank Coding for Image Classification Low-rank coding (LRC), originated from matrix decomposition, is recently introduced into image classification. Following the standard bag-of-words (BOW) pipeline, coding the data matrix in the sense of low-rankness incorporates contextual information into the traditional BOW model and it can capture the dependency relationship among neighbor patches. This differs from the traditional sparse coding paradigms which encode patches independently. Current LRC-based methods use l1 norm to increase the discrimination and sparsity of the learned codes. However, such methods fail to consider the local manifold structure between data space and dictionary space. To solve this problem, we propose a locality-constrained low-rank coding (LCLR) algorithm for image representations. By using the geometric structure information as a regularization term, the obtained representations are more discriminative. In addition, we present a fast and stable online algorithm to solve the optimization problem. In the experiments, we evaluate LCLR on four benchmarks, including one face recognition dataset (extended Yale B), one handwritten digit recognition dataset (USPS), and two image datasets (Scene13 for scene recognition and Caltech101 for object recognition). Experimental results show that our approach outperforms many state-of-the-art algorithms even with a linear classifier. Bag-of-words Model
Mid-level Representations
Locality Coding
Low-rank Coding
Inexact Augmented Lagrange Multiplier",localityconstrain lowrank code imag classif lowrank code lrc origin matrix decomposit recent introduc imag classif follow standard bagofword bow pipelin code data matrix sens lowrank incorpor contextu inform tradit bow model captur depend relationship among neighbor patch differ tradit spars code paradigm encod patch independ current lrcbase method use l1 norm increas discrimin sparsiti learn code howev method fail consid local manifold structur data space dictionari space solv problem propos localityconstrain lowrank code lclr algorithm imag represent use geometr structur inform regular term obtain represent discrimin addit present fast stabl onlin algorithm solv optim problem experi evalu lclr four benchmark includ one face recognit dataset extend yale b one handwritten digit recognit dataset usp two imag dataset scene13 scene recognit caltech101 object recognit experiment result show approach outperform mani stateoftheart algorithm even linear classifi bagofword model midlevel represent local code lowrank code inexact augment lagrang multipli,1,6.537094,-15.119452
Qualitative Planning with Quantitative Constraints for Online Learning of Robotic Behaviours,"Timothy Wiley, Claude Sammut and Ivan Bratko","Cognitive Systems (CS)
Robotics (ROB)","Robotics
Online Machine Learning
Multi-Strategy Architecture
Qualitative Reasoning
Qualitative Planning","CS: Problem solving and decision making
ROB: Behavior and Control
ROB: Motion and Path Planning
ROB: Robotics (General/Other)","This paper resolves previous problems in the Multi-Strategy architecture for online learning of robotic behaviours.
The hybrid method includes a symbolic qualitative planner that constructs an approximate solution to a control problem.
The approximate solution provides constraints for a numerical optimisation algorithm, which is used to refine the qualitative plan into an operational policy.
Introducing quantitative constraints into the planner gives previously unachievable domain independent reasoning.
The method is demonstrated on a multi-tracked robot intended for urban search and rescue.","Qualitative Planning with Quantitative Constraints for Online Learning of Robotic Behaviours This paper resolves previous problems in the Multi-Strategy architecture for online learning of robotic behaviours.
The hybrid method includes a symbolic qualitative planner that constructs an approximate solution to a control problem.
The approximate solution provides constraints for a numerical optimisation algorithm, which is used to refine the qualitative plan into an operational policy.
Introducing quantitative constraints into the planner gives previously unachievable domain independent reasoning.
The method is demonstrated on a multi-tracked robot intended for urban search and rescue. Robotics
Online Machine Learning
Multi-Strategy Architecture
Qualitative Reasoning
Qualitative Planning",qualit plan quantit constraint onlin learn robot behaviour paper resolv previous problem multistrategi architectur onlin learn robot behaviour hybrid method includ symbol qualit planner construct approxim solut control problem approxim solut provid constraint numer optimis algorithm use refin qualit plan oper polici introduc quantit constraint planner give previous unachiev domain independ reason method demonstr multitrack robot intend urban search rescu robot onlin machin learn multistrategi architectur qualit reason qualit plan,3,-3.5828912,11.91445
Regret-Based Multi-Agent Coordination with Uncertain Task Rewards,Feng Wu,Multiagent Systems (MAS),"Multi-Agent Coordination
Uncertain Task Rewards
DCOP","MAS: Coordination and Collaboration
MAS: Distributed Problem Solving","Many multi-agent coordination problems can be represented
as DCOPs. Motivated by task allocation in disaster
response, we extend standard DCOP models to consider
uncertain task rewards where the outcome of completing
a task depends on its current state, which is randomly
drawn from unknown distributions. The goal of
solving this problem is to find a solution for all agents
that minimizes the overall worst-case loss. This is a
challenging problem for centralized algorithms because
the search space grows exponentially with the number
of agents and is nontrivial for existing algorithms for
standard DCOPs. To address this, we propose a novel
decentralized algorithm that incorporates Max-Sum
with iterative constraint generation to solve the problem
by passing messages among agents. By so doing, our
approach scales well and can solve instances of the task
allocation problem with hundreds of agents and tasks.","Regret-Based Multi-Agent Coordination with Uncertain Task Rewards Many multi-agent coordination problems can be represented
as DCOPs. Motivated by task allocation in disaster
response, we extend standard DCOP models to consider
uncertain task rewards where the outcome of completing
a task depends on its current state, which is randomly
drawn from unknown distributions. The goal of
solving this problem is to find a solution for all agents
that minimizes the overall worst-case loss. This is a
challenging problem for centralized algorithms because
the search space grows exponentially with the number
of agents and is nontrivial for existing algorithms for
standard DCOPs. To address this, we propose a novel
decentralized algorithm that incorporates Max-Sum
with iterative constraint generation to solve the problem
by passing messages among agents. By so doing, our
approach scales well and can solve instances of the task
allocation problem with hundreds of agents and tasks. Multi-Agent Coordination
Uncertain Task Rewards
DCOP",regretbas multiag coordin uncertain task reward mani multiag coordin problem repres dcop motiv task alloc disast respons extend standard dcop model consid uncertain task reward outcom complet task depend current state random drawn unknown distribut goal solv problem find solut agent minim overal worstcas loss challeng problem central algorithm search space grow exponenti number agent nontrivi exist algorithm standard dcop address propos novel decentr algorithm incorpor maxsum iter constraint generat solv problem pass messag among agent approach scale well solv instanc task alloc problem hundr agent task multiag coordin uncertain task reward dcop,3,-19.146917,9.839155
Convex Co-embedding,"Farzaneh Mirzazadeh, Yuhong Guo and Dale Schuurmans",Novel Machine Learning Algorithms (NMLA),"convex relaxation
matrix norm regularization
relation learning
representation learning","NMLA: Dimension Reduction/Feature Selection
NMLA: Relational/Graph-Based Learning
NMLA: Supervised Learning (Other)","We present a general framework for association learning where entities are embedded in a common latent semantic space to allow relatedness to be expressed by geometry---an approach that underlies the state of the art for link prediction, relation learning, multi-label tagging, relevance retrieval and ranking.  Although current approaches rely on local training algorithms applied to non-convex formulations, we demonstrate how general convex relaxations can be easily achieved for entity embedding, both for the standard multi-linear and prototype-distance response models.  We propose an incremental optimization strategy that exploits decomposition to allow scaling.  An experimental evaluation reveals the advantages of tractable and repeatable global training in different case studies.","Convex Co-embedding We present a general framework for association learning where entities are embedded in a common latent semantic space to allow relatedness to be expressed by geometry---an approach that underlies the state of the art for link prediction, relation learning, multi-label tagging, relevance retrieval and ranking.  Although current approaches rely on local training algorithms applied to non-convex formulations, we demonstrate how general convex relaxations can be easily achieved for entity embedding, both for the standard multi-linear and prototype-distance response models.  We propose an incremental optimization strategy that exploits decomposition to allow scaling.  An experimental evaluation reveals the advantages of tractable and repeatable global training in different case studies. convex relaxation
matrix norm regularization
relation learning
representation learning",convex coembed present general framework associ learn entiti embed common latent semant space allow related express geometryan approach under state art link predict relat learn multilabel tag relev retriev rank although current approach reli local train algorithm appli nonconvex formul demonstr general convex relax easili achiev entiti embed standard multilinear prototypedist respons model propos increment optim strategi exploit decomposit allow scale experiment evalu reveal advantag tractabl repeat global train differ case studi convex relax matrix norm regular relat learn represent learn,6,3.182407,-10.749179
Optimal Decoupling in Linear Constraint Systems,"Cees Witteveen, Michel Wilson and Tomas Klos","Heuristic Search and Optimization (HSO)
Multiagent Systems (MAS)
Planning and Scheduling (PS)
Search and Constraint Satisfaction (SCS)","temporal decoupling
constraint solving
linear programming
flexibility","HSO: Optimization
MAS: Distributed Problem Solving
MAS: Multiagent Planning
PS: Scheduling
PS: Temporal Planning
PS: Planning (General/Other)
SCS: Constraint Satisfaction
SCS: Constraint Optimization
SCS: Global Constraints
SCS: Constraint Satisfaction (General/other)","Decomposition can be defined as a technique to obtain complete solutions by easy composition of partial solutions. Typically, these partial solutions are obtained by distributed and concurrent local problem solving without communication between the individual problem solvers. Constraint decomposition plays an important role in distributed databases, distributed scheduling and violation detection: Here, it enables conflict-free local decision making, while avoiding communication overloading. One of the main issues in decomposition is the loss of flexibility due to the composition technique used. Here, flexibility roughly refers to the freedom in choosing suitable values for the variables in order to satisfy the constraints. In this paper we concentrate on linear constraint systems and efficient decomposition techniques for these systems. Using a generalization of a flexibility metric developed for STNs, we show how  an efficient decomposition technique for linear constraints can be derived that minimizes the loss of flexibility due to decomposition.  As a by-product of our decomposition technique, we show that an intuitively attractive flexibility metric for linear constraint systems can be developed where decomposition does not incur any loss of flexibility.","Optimal Decoupling in Linear Constraint Systems Decomposition can be defined as a technique to obtain complete solutions by easy composition of partial solutions. Typically, these partial solutions are obtained by distributed and concurrent local problem solving without communication between the individual problem solvers. Constraint decomposition plays an important role in distributed databases, distributed scheduling and violation detection: Here, it enables conflict-free local decision making, while avoiding communication overloading. One of the main issues in decomposition is the loss of flexibility due to the composition technique used. Here, flexibility roughly refers to the freedom in choosing suitable values for the variables in order to satisfy the constraints. In this paper we concentrate on linear constraint systems and efficient decomposition techniques for these systems. Using a generalization of a flexibility metric developed for STNs, we show how  an efficient decomposition technique for linear constraints can be derived that minimizes the loss of flexibility due to decomposition.  As a by-product of our decomposition technique, we show that an intuitively attractive flexibility metric for linear constraint systems can be developed where decomposition does not incur any loss of flexibility. temporal decoupling
constraint solving
linear programming
flexibility",optim decoupl linear constraint system decomposit defin techniqu obtain complet solut easi composit partial solut typic partial solut obtain distribut concurr local problem solv without communic individu problem solver constraint decomposit play import role distribut databas distribut schedul violat detect enabl conflictfre local decis make avoid communic overload one main issu decomposit loss flexibl due composit techniqu use flexibl rough refer freedom choos suitabl valu variabl order satisfi constraint paper concentr linear constraint system effici decomposit techniqu system use general flexibl metric develop stns show effici decomposit techniqu linear constraint deriv minim loss flexibl due decomposit byproduct decomposit techniqu show intuit attract flexibl metric linear constraint system develop decomposit incur loss flexibl tempor decoupl constraint solv linear program flexibl,7,-4.492119,4.3081064
Who also likes it? Generating the most Persuasive Social Explanations in Recommender Systems,Beidou Wang and Martin Ester,AI and the Web (AIW),"Recommendation Explanation
Social Explanation
Social Network","AIW: Social networking and community identification
AIW: Web-based recommendation systems","Social explanation, the statement with the form of ”A and B also like the item”, is widely used in almost all the major recommender systems in the web and effectively improves the persuasiveness of the recommendation results by convincing more users to try. This paper presents the first algorithm to generate the most persuasive social explanation by recommending the optimal set of users to be put in the explanation. New challenges like modeling persuasiveness of multiple users, different types of users in social network, sparsity of likes, are discussed in depth and solved in our algorithm. The extensive evaluation demonstrates the advantage of our proposed algorithm compared with traditional methods.","Who also likes it? Generating the most Persuasive Social Explanations in Recommender Systems Social explanation, the statement with the form of ”A and B also like the item”, is widely used in almost all the major recommender systems in the web and effectively improves the persuasiveness of the recommendation results by convincing more users to try. This paper presents the first algorithm to generate the most persuasive social explanation by recommending the optimal set of users to be put in the explanation. New challenges like modeling persuasiveness of multiple users, different types of users in social network, sparsity of likes, are discussed in depth and solved in our algorithm. The extensive evaluation demonstrates the advantage of our proposed algorithm compared with traditional methods. Recommendation Explanation
Social Explanation
Social Network",also like generat persuas social explan recommend system social explan statement form ”a b also like item” wide use almost major recommend system web effect improv persuas recommend result convinc user tri paper present first algorithm generat persuas social explan recommend optim set user put explan new challeng like model persuas multipl user differ type user social network sparsiti like discuss depth solv algorithm extens evalu demonstr advantag propos algorithm compar tradit method recommend explan social explan social network,0,15.864354,6.6690073
Computing Contingent Plans via Fully Observable Non-Deterministic Planning,"Christian Muise, Vaishak Belle and Sheila Mcilraith",Planning and Scheduling (PS),"contingent planning
conditional planning
partial observability
planning and sensing
offline planning
FOND","PS: Deterministic Planning
PS: Planning (General/Other)","Planning with sensing actions under partial observability is a computationally challenging problem that is fundamental to the realization of AI tasks in areas as diverse as robotics, game playing, and diagnostic problem solving.  In this paper we explore a particular class of planning problems where the initial state specification includes a set of state constraints or so-called state invariants and where uncertainty about the state monotonically decreases.  Recent work on generating plans for partially observable domains has advocated for online planning, claiming that offline plans are often too large to generate.  Unfortunately, planning online can lead to avoidable deadends, and the generated plan only addresses the particular sequence of observations realized during the execution.  Here we push the envelope on this challenging problem, proposing a technique for generating conditional (aka contingent) plans offline.  The conditional plans we produce will eventually achieve the goal for all consistent sequences of observations for which a solution exists.  The key to our planner's success is the reliance on state-of-the-art techniques for fully observable non-deterministic (FOND) planning. In particular, we use an existing compilation for converting a planning problem under partial observability and sensing to a FOND planning problem. With a modified FOND planner in hand, we are able to scale beyond previous techniques for contingent planning and compute solutions that are orders of magnitude smaller than previously possible in some domains.","Computing Contingent Plans via Fully Observable Non-Deterministic Planning Planning with sensing actions under partial observability is a computationally challenging problem that is fundamental to the realization of AI tasks in areas as diverse as robotics, game playing, and diagnostic problem solving.  In this paper we explore a particular class of planning problems where the initial state specification includes a set of state constraints or so-called state invariants and where uncertainty about the state monotonically decreases.  Recent work on generating plans for partially observable domains has advocated for online planning, claiming that offline plans are often too large to generate.  Unfortunately, planning online can lead to avoidable deadends, and the generated plan only addresses the particular sequence of observations realized during the execution.  Here we push the envelope on this challenging problem, proposing a technique for generating conditional (aka contingent) plans offline.  The conditional plans we produce will eventually achieve the goal for all consistent sequences of observations for which a solution exists.  The key to our planner's success is the reliance on state-of-the-art techniques for fully observable non-deterministic (FOND) planning. In particular, we use an existing compilation for converting a planning problem under partial observability and sensing to a FOND planning problem. With a modified FOND planner in hand, we are able to scale beyond previous techniques for contingent planning and compute solutions that are orders of magnitude smaller than previously possible in some domains. contingent planning
conditional planning
partial observability
planning and sensing
offline planning
FOND",comput conting plan via fulli observ nondeterminist plan plan sens action partial observ comput challeng problem fundament realize ai task area divers robot game play diagnost problem solv paper explor particular class plan problem initi state specif includ set state constraint socal state invari uncertainti state monoton decreas recent work generat plan partial observ domain advoc onlin plan claim offlin plan often larg generat unfortun plan onlin lead avoid deadend generat plan address particular sequenc observ realiz execut push envelop challeng problem propos techniqu generat condit aka conting plan offlin condit plan produc eventu achiev goal consist sequenc observ solut exist key planner success relianc stateoftheart techniqu fulli observ nondeterminist fond plan particular use exist compil convert plan problem partial observ sens fond plan problem modifi fond planner hand abl scale beyond previous techniqu conting plan comput solut order magnitud smaller previous possibl domain conting plan condit plan partial observ plan sens offlin plan fond,3,-3.0361362,18.61099
Instance-based Domain Adaptation in NLP via In-target-domain Logistic Approximation,"Rui Xia, Jianfei Yu, Feng Xu and Shumei Wang","NLP and Machine Learning (NLPML)
Novel Machine Learning Algorithms (NMLA)","domain adaptation
instance adaptation
instance-based adaptation
density-ratio estimation
text categorization
sentiment classification","NLPML: Text Classification
NLPML: Natural Language Processing (General/Other)
NMLA: Transfer, Adaptation, Multitask Learning","In the field of NLP, most of the existing domain adaptation studies belong to the feature-based adaptation, while the research of instance-based adaptation is very scarce. In this work, we propose a new instance-based adaptation model, called in-target-domain logistic approximation (ILA). In ILA, we adapt the source-domain data to the target domain by a logistic approximation. The normalized in-target-domain probability is assigned as an instance weight to each of the source-domain training data. An instance-weighted classification model is trained finally for the cross-domain classification problem. Compared to the previous techniques, ILA conducts instance adaptation in a dimensionality-reduced linear feature space to ensure efficiency in high-dimensional NLP tasks. The instance weights in ILA are learnt by leveraging the criteria of both maximum likelihood and minimum statistical distance. The empirical results on two NLP tasks including text categorization and sentiment classification show that our ILA model beats the state-of-the-art instance adaptation methods significantly, in cross-domain classification accuracy, parameter stability and computational efficiency.","Instance-based Domain Adaptation in NLP via In-target-domain Logistic Approximation In the field of NLP, most of the existing domain adaptation studies belong to the feature-based adaptation, while the research of instance-based adaptation is very scarce. In this work, we propose a new instance-based adaptation model, called in-target-domain logistic approximation (ILA). In ILA, we adapt the source-domain data to the target domain by a logistic approximation. The normalized in-target-domain probability is assigned as an instance weight to each of the source-domain training data. An instance-weighted classification model is trained finally for the cross-domain classification problem. Compared to the previous techniques, ILA conducts instance adaptation in a dimensionality-reduced linear feature space to ensure efficiency in high-dimensional NLP tasks. The instance weights in ILA are learnt by leveraging the criteria of both maximum likelihood and minimum statistical distance. The empirical results on two NLP tasks including text categorization and sentiment classification show that our ILA model beats the state-of-the-art instance adaptation methods significantly, in cross-domain classification accuracy, parameter stability and computational efficiency. domain adaptation
instance adaptation
instance-based adaptation
density-ratio estimation
text categorization
sentiment classification",instancebas domain adapt nlp via intargetdomain logist approxim field nlp exist domain adapt studi belong featurebas adapt research instancebas adapt scarc work propos new instancebas adapt model call intargetdomain logist approxim ila ila adapt sourcedomain data target domain logist approxim normal intargetdomain probabl assign instanc weight sourcedomain train data instanceweight classif model train final crossdomain classif problem compar previous techniqu ila conduct instanc adapt dimensionalityreduc linear featur space ensur effici highdimension nlp task instanc weight ila learnt leverag criteria maximum likelihood minimum statist distanc empir result two nlp task includ text categor sentiment classif show ila model beat stateoftheart instanc adapt method signific crossdomain classif accuraci paramet stabil comput effici domain adapt instanc adapt instancebas adapt densityratio estim text categor sentiment classif,6,-6.7638535,-4.9837723
Ordering Effects and Belief Adjustment in the Use of Comparison Shopping Agents,"Chen Hajaj, Noam Hazon and David Sarne",Humans and AI (HAI),"comparison shopping agents
belief-adjustment
ordering
experimentation
eCommerce",HAI: Human-Computer Interaction,"The popularity of online shopping has contributed to the development of comparison shopping agents (CSAs) aiming to facilitate buyers' ability to compare prices of online stores for any desired product. Furthermore, the plethora of CSAs in today’s markets enables buyers to query more than a single CSA when shopping, thus expanding even further the list of sellers whose prices they obtain. This potentially decreases the chance of a purchase based on the prices outputted as a result of any single query, and consequently decreases each CSAs’ expected revenue per-query. Obviously, a CSA can improve its competence in such settings by acquiring more sellers’ prices, potentially resulting in a more attractive ``best price''. In this paper we suggest a complementary approach that improves the attractiveness of a CSA by presenting the prices to the user in a specific intelligent manner, which is based on known cognitive-biases.
The advantage of this approach is its ability to affect the buyer’s tendency to terminate her search for a better price, hence avoid querying further CSAs, without having the CSA spend any of its resources on finding better prices to present.
The effectiveness of our method is demonstrated using real data, collected from four CSAs for five products. Our experiments with people confirm that the suggested method effectively influence people in a way that is highly advantageous to the CSA.","Ordering Effects and Belief Adjustment in the Use of Comparison Shopping Agents The popularity of online shopping has contributed to the development of comparison shopping agents (CSAs) aiming to facilitate buyers' ability to compare prices of online stores for any desired product. Furthermore, the plethora of CSAs in today’s markets enables buyers to query more than a single CSA when shopping, thus expanding even further the list of sellers whose prices they obtain. This potentially decreases the chance of a purchase based on the prices outputted as a result of any single query, and consequently decreases each CSAs’ expected revenue per-query. Obviously, a CSA can improve its competence in such settings by acquiring more sellers’ prices, potentially resulting in a more attractive ``best price''. In this paper we suggest a complementary approach that improves the attractiveness of a CSA by presenting the prices to the user in a specific intelligent manner, which is based on known cognitive-biases.
The advantage of this approach is its ability to affect the buyer’s tendency to terminate her search for a better price, hence avoid querying further CSAs, without having the CSA spend any of its resources on finding better prices to present.
The effectiveness of our method is demonstrated using real data, collected from four CSAs for five products. Our experiments with people confirm that the suggested method effectively influence people in a way that is highly advantageous to the CSA. comparison shopping agents
belief-adjustment
ordering
experimentation
eCommerce",order effect belief adjust use comparison shop agent popular onlin shop contribut develop comparison shop agent csas aim facilit buyer abil compar price onlin store desir product furthermor plethora csas today market enabl buyer queri singl csa shop thus expand even list seller whose price obtain potenti decreas chanc purchas base price output result singl queri consequ decreas csas expect revenu perqueri obvious csa improv compet set acquir seller price potenti result attract best price paper suggest complementari approach improv attract csa present price user specif intellig manner base known cognitivebias advantag approach abil affect buyer tendenc termin search better price henc avoid queri csas without csa spend resourc find better price present effect method demonstr use real data collect four csas five product experi peopl confirm suggest method effect influenc peopl way high advantag csa comparison shop agent beliefadjust order experiment ecommerc,8,6.2682495,4.500811
Scalable sparse covariance estimation via self-concordance,"Anastasios Kyrillidis, Rabeeh Karimi Mahabadi, Quoc Tran-Dinh and Volkan Cevher","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Inexact proximal Newton methods
Sparse covariance estimation
Self-concordance property","MLA: Machine Learning Applications (General/other)
NMLA: Big Data / Scalability
NMLA: Data Mining and Knowledge Discovery
NMLA: Graphical Model Learning","We consider the class of convex minimization problems, composed of a self-concordant function, such as the logdet metric, a convex data fidelity term h() and, a regularizing -- possibly non-smooth -- function g(), accompanied with an easily computable proximity operator. These type of problems have recently attracted a great deal of interest, mainly due to their omnipresence in top-notch applications. Under this locally Lipschitz continuous gradient setting, we analyze the convergence behavior of proximal Newton schemes with the added twist of a probable presence of inexact evaluations; a scenario that has not been considered yet, to the best of our knowledge. By using standard convex tools combined with self-concordance machinery, we provide a concise convergence theory with attractive convergence rate guarantees, and enhance state-of-the-art optimization schemes to accommodate such developments. Experimental results on sparse covariance estimation show the merits of our algorithm, both in terms of recovery efficiency and complexity, rendering the proposed framework a suitable choice for such problems.","Scalable sparse covariance estimation via self-concordance We consider the class of convex minimization problems, composed of a self-concordant function, such as the logdet metric, a convex data fidelity term h() and, a regularizing -- possibly non-smooth -- function g(), accompanied with an easily computable proximity operator. These type of problems have recently attracted a great deal of interest, mainly due to their omnipresence in top-notch applications. Under this locally Lipschitz continuous gradient setting, we analyze the convergence behavior of proximal Newton schemes with the added twist of a probable presence of inexact evaluations; a scenario that has not been considered yet, to the best of our knowledge. By using standard convex tools combined with self-concordance machinery, we provide a concise convergence theory with attractive convergence rate guarantees, and enhance state-of-the-art optimization schemes to accommodate such developments. Experimental results on sparse covariance estimation show the merits of our algorithm, both in terms of recovery efficiency and complexity, rendering the proposed framework a suitable choice for such problems. Inexact proximal Newton methods
Sparse covariance estimation
Self-concordance property",scalabl spars covari estim via selfconcord consid class convex minim problem compos selfconcord function logdet metric convex data fidel term h regular possibl nonsmooth function g accompani easili comput proxim oper type problem recent attract great deal interest main due omnipres topnotch applic local lipschitz continu gradient set analyz converg behavior proxim newton scheme ad twist probabl presenc inexact evalu scenario consid yet best knowledg use standard convex tool combin selfconcord machineri provid concis converg theori attract converg rate guarante enhanc stateoftheart optim scheme accommod develop experiment result spars covari estim show merit algorithm term recoveri effici complex render propos framework suitabl choic problem inexact proxim newton method spars covari estim selfconcord properti,4,3.4527142,-18.901747
Trading Multiple Indivisible Goods with Indifferences: Beyond Sönmez's Result,"Akihisa Sonoda, Etsushi Fujita, Taiki Todo and Makoto Yokoo",Game Theory and Economic Paradigms (GTEP),"Mechanism design
Exchange
Indifference
Pareto efficiency
Strategy-proofness
Individual rationality","GTEP: Game Theory
GTEP: Social Choice / Voting
MAS: E-Commerce
MAS: Mechanism Design","Designing mechanisms that satisfy individual rationality, Pareto efficiency, and strategyproofness is one of the most important problems in mechanism design. In this paper we investigate mechanism design for exchange models where each agent is initially endowed with a set of goods, each agent may have indifferences on distinct bundles of goods, and monetary transfers are not allowed. Sönmez (1999) showed that in such models, those three properties are not compatible in general. The impossibility, however, only holds under an assumption on preference domains.
The purpose of this paper is to give a discussion on the compatibility of those three properties when the assumption does not hold. We first establish a preference domain called top-only preferences, which violates the assumption, and develop a class of exchange mechanisms satisfying all those properties. Each mechanism in the class utilizes one instance of mechanisms introduced by Saban and Sethuraman (2013). We also find a class of preference domains called m-chotomous preferences, where the assumption fails and those properties are incompatible.","Trading Multiple Indivisible Goods with Indifferences: Beyond Sönmez's Result Designing mechanisms that satisfy individual rationality, Pareto efficiency, and strategyproofness is one of the most important problems in mechanism design. In this paper we investigate mechanism design for exchange models where each agent is initially endowed with a set of goods, each agent may have indifferences on distinct bundles of goods, and monetary transfers are not allowed. Sönmez (1999) showed that in such models, those three properties are not compatible in general. The impossibility, however, only holds under an assumption on preference domains.
The purpose of this paper is to give a discussion on the compatibility of those three properties when the assumption does not hold. We first establish a preference domain called top-only preferences, which violates the assumption, and develop a class of exchange mechanisms satisfying all those properties. Each mechanism in the class utilizes one instance of mechanisms introduced by Saban and Sethuraman (2013). We also find a class of preference domains called m-chotomous preferences, where the assumption fails and those properties are incompatible. Mechanism design
Exchange
Indifference
Pareto efficiency
Strategy-proofness
Individual rationality",trade multipl indivis good indiffer beyond sönmez result design mechan satisfi individu ration pareto effici strategyproof one import problem mechan design paper investig mechan design exchang model agent initi endow set good agent may indiffer distinct bundl good monetari transfer allow sönmez 1999 show model three properti compat general imposs howev hold assumpt prefer domain purpos paper give discuss compat three properti assumpt hold first establish prefer domain call topon prefer violat assumpt develop class exchang mechan satisfi properti mechan class util one instanc mechan introduc saban sethuraman 2013 also find class prefer domain call mchotom prefer assumpt fail properti incompat mechan design exchang indiffer pareto effici strategyproof individu ration,9,15.36462,15.767107
SenticNet 3: A Common and Common-Sense Knowledge Base for Cognition-Driven Sentiment Analysis,Erik Cambria,"Cognitive Systems (CS)
Knowledge Representation and Reasoning (KRR)
NLP and Knowledge Representation (NLPKR)","concept-level sentiment analysis
natural language processing
common-sense reasoning","CS: Conceptual inference and reasoning
KRR: Common-Sense Reasoning
KRR: Knowledge Representation (General/Other)
NLPKR: Natural Language Processing (General/Other)","SenticNet is a publicly available semantic and affective resource for concept-level opinion mining and sentiment analysis. Rather than using graph-mining and dimensionality-reduction techniques, SenticNet 3 makes use of `energy flows' to connect various parts of extended common and common-sense knowledge representations to one another. SenticNet 3 models nuanced semantics and sentics (that is, the conceptual and affective information associated with multi-word natural language expressions), representing information with a symbolic opacity intermediate between that of neural networks and of typical symbolic systems.","SenticNet 3: A Common and Common-Sense Knowledge Base for Cognition-Driven Sentiment Analysis SenticNet is a publicly available semantic and affective resource for concept-level opinion mining and sentiment analysis. Rather than using graph-mining and dimensionality-reduction techniques, SenticNet 3 makes use of `energy flows' to connect various parts of extended common and common-sense knowledge representations to one another. SenticNet 3 models nuanced semantics and sentics (that is, the conceptual and affective information associated with multi-word natural language expressions), representing information with a symbolic opacity intermediate between that of neural networks and of typical symbolic systems. concept-level sentiment analysis
natural language processing
common-sense reasoning",senticnet 3 common commonsens knowledg base cognitiondriven sentiment analysi senticnet public avail semant affect resourc conceptlevel opinion mine sentiment analysi rather use graphmin dimensionalityreduct techniqu senticnet 3 make use energi flow connect various part extend common commonsens knowledg represent one anoth senticnet 3 model nuanc semant sentic conceptu affect inform associ multiword natur languag express repres inform symbol opac intermedi neural network typic symbol system conceptlevel sentiment analysi natur languag process commonsens reason,9,11.995418,-5.3709044
Combining Multiple Correlated Reward and Shaping Signals by Measuring Confidence,"Tim Brys, Ann Nowé, Daniel Kudenko and Matthew E. Taylor",Novel Machine Learning Algorithms (NMLA),"Reinforcement Learning
Reward Shaping
Multi-Objective Optimization
Traffic Light Control
Pursuit Domain",NMLA: Reinforcement Learning,"Multi-objective problems with correlated objectives are a class of problems that deserve specific attention. In contrast to typical multi-objective problems, they do not require the identification of trade-offs between the objectives, as (near-) optimal solutions for any objective are (near-) optimal for every objective. Intelligently combining the feedback from these objectives, instead of only looking at a single one, can improve optimization. This class of problems is very relevant in reinforcement learning, as any single-objective reinforcement learning problem can be framed as such a multi-objective problem using multiple reward shaping functions. After discussing this problem class, we propose a solution technique for such reinforcement learning problems, called adaptive objective selection. This technique  makes a temporal difference learner estimate the Q-function for each objective in parallel, and introduces a way to measure confidence in these estimates. This confidence metric is then used to choose which objective's estimates to use for action selection. We show significant improvements in performance over other plausible techniques on two problem domains. Finally, we provide an intuitive analysis of the technique's decisions, yielding insights into the nature of the problems being solved.","Combining Multiple Correlated Reward and Shaping Signals by Measuring Confidence Multi-objective problems with correlated objectives are a class of problems that deserve specific attention. In contrast to typical multi-objective problems, they do not require the identification of trade-offs between the objectives, as (near-) optimal solutions for any objective are (near-) optimal for every objective. Intelligently combining the feedback from these objectives, instead of only looking at a single one, can improve optimization. This class of problems is very relevant in reinforcement learning, as any single-objective reinforcement learning problem can be framed as such a multi-objective problem using multiple reward shaping functions. After discussing this problem class, we propose a solution technique for such reinforcement learning problems, called adaptive objective selection. This technique  makes a temporal difference learner estimate the Q-function for each objective in parallel, and introduces a way to measure confidence in these estimates. This confidence metric is then used to choose which objective's estimates to use for action selection. We show significant improvements in performance over other plausible techniques on two problem domains. Finally, we provide an intuitive analysis of the technique's decisions, yielding insights into the nature of the problems being solved. Reinforcement Learning
Reward Shaping
Multi-Objective Optimization
Traffic Light Control
Pursuit Domain",combin multipl correl reward shape signal measur confid multiobject problem correl object class problem deserv specif attent contrast typic multiobject problem requir identif tradeoff object near optim solut object near optim everi object intellig combin feedback object instead look singl one improv optim class problem relev reinforc learn singleobject reinforc learn problem frame multiobject problem use multipl reward shape function discuss problem class propos solut techniqu reinforc learn problem call adapt object select techniqu make tempor differ learner estim qfunction object parallel introduc way measur confid estim confid metric use choos object estim use action select show signific improv perform plausibl techniqu two problem domain final provid intuit analysi techniqu decis yield insight natur problem solv reinforc learn reward shape multiobject optim traffic light control pursuit domain,4,-2.1144881,-12.561625
Avoiding Plagiarism in Markov Sequence Generation,"Alexandre Papadopoulos, Pierre Roy and François Pachet","Machine Learning Applications (MLA)
Search and Constraint Satisfaction (SCS)","markov chains
plagiarism
constraint satisfaction
global constraints","APP: Art and Music
MLA: Machine Learning Applications (General/other)
SCS: Constraint Satisfaction
SCS: Global Constraints
SCS: Constraint Satisfaction (General/other)","Markov processes are widely used to generate sequences that imitate a given style, using random walk. Random walk generates sequences by iteratively concatenating states to prefixes of length equal or less than the given Markov order. However, at higher orders, Markov chains tend to replicate chunks of the corpus with a size possibly higher than the order, a primary form of plagiarism. In fact, the Markov order defines a maximum length for training but not for generation. In the framework of constraint satisfaction (CSP), we introduce MaxOrder. This global constraint ensures that generated sequences do not include chunks larger than a given maximum order. We exhibit an automaton that recognises the solution set, with a size linear in the size of the corpus. We propose a linear-time procedure to generate this automaton from a corpus and a given max order. We then use this automaton to achieve generalised arc consistency for the MaxOrder constraint, holding on a sequence of size n, in O(n.T) time, where T is the size of the automaton. We illustrate our approach by generating text sequences from text corpora with a maximum order guarantee, effectively controlling plagiarism.","Avoiding Plagiarism in Markov Sequence Generation Markov processes are widely used to generate sequences that imitate a given style, using random walk. Random walk generates sequences by iteratively concatenating states to prefixes of length equal or less than the given Markov order. However, at higher orders, Markov chains tend to replicate chunks of the corpus with a size possibly higher than the order, a primary form of plagiarism. In fact, the Markov order defines a maximum length for training but not for generation. In the framework of constraint satisfaction (CSP), we introduce MaxOrder. This global constraint ensures that generated sequences do not include chunks larger than a given maximum order. We exhibit an automaton that recognises the solution set, with a size linear in the size of the corpus. We propose a linear-time procedure to generate this automaton from a corpus and a given max order. We then use this automaton to achieve generalised arc consistency for the MaxOrder constraint, holding on a sequence of size n, in O(n.T) time, where T is the size of the automaton. We illustrate our approach by generating text sequences from text corpora with a maximum order guarantee, effectively controlling plagiarism. markov chains
plagiarism
constraint satisfaction
global constraints",avoid plagiar markov sequenc generat markov process wide use generat sequenc imit given style use random walk random walk generat sequenc iter concaten state prefix length equal less given markov order howev higher order markov chain tend replic chunk corpus size possibl higher order primari form plagiar fact markov order defin maximum length train generat framework constraint satisfact csp introduc maxord global constraint ensur generat sequenc includ chunk larger given maximum order exhibit automaton recognis solut set size linear size corpus propos lineartim procedur generat automaton corpus given max order use automaton achiev generalis arc consist maxord constraint hold sequenc size n ont time size automaton illustr approach generat text sequenc text corpora maximum order guarante effect control plagiar markov chain plagiar constraint satisfact global constraint,7,-5.3116302,2.634578
A Region-Based Model for Estimating Urban Air Pollution,"Arnaud Jutzeler, Jason Jingshi Li and Boi Faltings","Computational Sustainability and AI (CSAI)
Machine Learning Applications (MLA)","Spatial Reasoning
Computational Sustainability
Gaussian Process
Urban Air Quality","CSAI: Modeling and prediction of dynamic and spatiotemporal phenomena and systems
MLA: Environmental","Air pollution has a direct impact to human health, and data-driven air quality models are useful for evaluating population exposure to air pollutants. In this paper, we propose a novel region-based Gaussian Process model for estimating urban air pollution dispersion, and applied it to a large dataset of ultrafine particle measurements collected from a network of trams monitoring levels of ultrafine particle dispersion in the city of Zurich. We show that compared to existing grid-based models, the region-based model produces better predictions across all aggregate time scales. The new model is appropriate for many useful user applications such as anomaly detection, exposure assessment and sensor optimization.","A Region-Based Model for Estimating Urban Air Pollution Air pollution has a direct impact to human health, and data-driven air quality models are useful for evaluating population exposure to air pollutants. In this paper, we propose a novel region-based Gaussian Process model for estimating urban air pollution dispersion, and applied it to a large dataset of ultrafine particle measurements collected from a network of trams monitoring levels of ultrafine particle dispersion in the city of Zurich. We show that compared to existing grid-based models, the region-based model produces better predictions across all aggregate time scales. The new model is appropriate for many useful user applications such as anomaly detection, exposure assessment and sensor optimization. Spatial Reasoning
Computational Sustainability
Gaussian Process
Urban Air Quality",regionbas model estim urban air pollut air pollut direct impact human health datadriven air qualiti model use evalu popul exposur air pollut paper propos novel regionbas gaussian process model estim urban air pollut dispers appli larg dataset ultrafin particl measur collect network tram monitor level ultrafin particl dispers citi zurich show compar exist gridbas model regionbas model produc better predict across aggreg time scale new model appropri mani use user applic anomali detect exposur assess sensor optim spatial reason comput sustain gaussian process urban air qualiti,5,3.1854153,0.22584106
Item Bidding  for Combinatorial Public Projects,Evangelos Markakis and Orestis Telelis,"Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Public Project
Mechanisms
Valuation Function
Social Welfare
Nash Equilibrium
Strong Equilibrium
Price of Anarchy","GTEP: Game Theory
GTEP: Coordination and Collaboration
GTEP: Equilibrium
MAS: Coordination and Collaboration
MAS: Mechanism Design","We present and analyze a mechanism for the Combinatorial Public Project Problem (CPPP). The problem asks to select k out of m available items, so as to maximize the social welfare for autonomous agents with combinatorial preferences (valuation functions) over subsets of items. The CPPP constitutes an abstract model for decision making by autonomous agents and has been shown to present severe computational hardness, in the design of truthful approximation mechanisms. We study a non-truthful mechanism that is, however, practically relevant to multi-agent environments, by virtue of its natural simplicity. It employs an Item Bidding interface, wherein every agent issues a separate bid for the inclusion of each distinct item in the outcome; the k items with the highest sums of bids are chosen and agents are charged according to a VCG-based payment rule. For fairly expressive classes of the agents' valuation functions, we establish existence of socially optimal pure Nash and strong equilibria, that are resilient to coordinated deviations of subsets of agents. Subsequently we derive tight worst-case bounds on the approximation of the optimum social welfare achieved in equilibrium. We show that the mechanism's performance improves with the number of agents that can coordinate, and reaches half of the optimum welfare at strong equilibrium.","Item Bidding  for Combinatorial Public Projects We present and analyze a mechanism for the Combinatorial Public Project Problem (CPPP). The problem asks to select k out of m available items, so as to maximize the social welfare for autonomous agents with combinatorial preferences (valuation functions) over subsets of items. The CPPP constitutes an abstract model for decision making by autonomous agents and has been shown to present severe computational hardness, in the design of truthful approximation mechanisms. We study a non-truthful mechanism that is, however, practically relevant to multi-agent environments, by virtue of its natural simplicity. It employs an Item Bidding interface, wherein every agent issues a separate bid for the inclusion of each distinct item in the outcome; the k items with the highest sums of bids are chosen and agents are charged according to a VCG-based payment rule. For fairly expressive classes of the agents' valuation functions, we establish existence of socially optimal pure Nash and strong equilibria, that are resilient to coordinated deviations of subsets of agents. Subsequently we derive tight worst-case bounds on the approximation of the optimum social welfare achieved in equilibrium. We show that the mechanism's performance improves with the number of agents that can coordinate, and reaches half of the optimum welfare at strong equilibrium. Public Project
Mechanisms
Valuation Function
Social Welfare
Nash Equilibrium
Strong Equilibrium
Price of Anarchy",item bid combinatori public project present analyz mechan combinatori public project problem cppp problem ask select k avail item maxim social welfar autonom agent combinatori prefer valuat function subset item cppp constitut abstract model decis make autonom agent shown present sever comput hard design truth approxim mechan studi nontruth mechan howev practic relev multiag environ virtu natur simplic employ item bid interfac wherein everi agent issu separ bid inclus distinct item outcom k item highest sum bid chosen agent charg accord vcgbase payment rule fair express class agent valuat function establish exist social optim pure nash strong equilibria resili coordin deviat subset agent subsequ deriv tight worstcas bound approxim optimum social welfar achiev equilibrium show mechan perform improv number agent coordin reach half optimum welfar strong equilibrium public project mechan valuat function social welfar nash equilibrium strong equilibrium price anarchi,9,9.792896,14.510752
How Long Will It Take? Accurate Prediction of Ontology Reasoning Performance,"Yong-Bin Kang, Jeff Z. Pan, Shonali Krishnaswamy, Wudhichart Sawangphol and Yuan-Fang Li",AI and the Web (AIW),"Ontology
Reasoning performance
Semantic Web
Prediction
Regression
Performance hotspot detection","AIW: Ontologies and the web: creation, extraction, evolution, mapping, merging, and alignment; tags and folksonomies","For expressive ontology languages such as OWL 2 DL, classification is a computationally expensive task---2\textsc{NExpTime}-complete in the worst case. Hence, it is highly desirable to be able to accurately estimate classification time, especially for large and complex ontologies. Recently, machine learning techniques have been successfully applied to predicting the reasoning \emph{hardness category} for a given (ontology, reasoner) pair. In this paper, we further develop predictive models to estimate actual classification time using regression techniques, with ontology metrics as features. Our large-scale experiments on 6 state-of-the-art OWL 2 DL reasoners and more than 450 significantly diverse ontologies demonstrate that the prediction models achieve high accuracy, good generalizability and statistical significance. Such prediction models have a wide range of applications. We demonstrate how they can be used to efficiently and accurately identify \emph{performance hotspots} in an large and complex ontology, an otherwise very time-consuming and resource-intensive task.","How Long Will It Take? Accurate Prediction of Ontology Reasoning Performance For expressive ontology languages such as OWL 2 DL, classification is a computationally expensive task---2\textsc{NExpTime}-complete in the worst case. Hence, it is highly desirable to be able to accurately estimate classification time, especially for large and complex ontologies. Recently, machine learning techniques have been successfully applied to predicting the reasoning \emph{hardness category} for a given (ontology, reasoner) pair. In this paper, we further develop predictive models to estimate actual classification time using regression techniques, with ontology metrics as features. Our large-scale experiments on 6 state-of-the-art OWL 2 DL reasoners and more than 450 significantly diverse ontologies demonstrate that the prediction models achieve high accuracy, good generalizability and statistical significance. Such prediction models have a wide range of applications. We demonstrate how they can be used to efficiently and accurately identify \emph{performance hotspots} in an large and complex ontology, an otherwise very time-consuming and resource-intensive task. Ontology
Reasoning performance
Semantic Web
Prediction
Regression
Performance hotspot detection",long take accur predict ontolog reason perform express ontolog languag owl 2 dl classif comput expens task2textscnexptimecomplet worst case henc high desir abl accur estim classif time especi larg complex ontolog recent machin learn techniqu success appli predict reason emphhard categori given ontolog reason pair paper develop predict model estim actual classif time use regress techniqu ontolog metric featur largescal experi 6 stateoftheart owl 2 dl reason 450 signific divers ontolog demonstr predict model achiev high accuraci good generaliz statist signific predict model wide rang applic demonstr use effici accur identifi emphperform hotspot larg complex ontolog otherwis timeconsum resourceintens task ontolog reason perform semant web predict regress perform hotspot detect,8,23.27228,7.519686
Backdoors to Planning,"Martin Kronegger, Sebastian Ordyniak and Andreas Pfandler","Knowledge Representation and Reasoning (KRR)
Planning and Scheduling (PS)","Planning
Backdoors
Fixed-parameter tractable algorithms
(Parameterized) complexity","KRR: Computational Complexity of Reasoning
PS: Deterministic Planning","Backdoors measure the distance to tractable fragments
and have become an important tool to find fixed-parameter
tractable (fpt) algorithms. Despite their success, backdoors
have not been used for planning, a central problem
in AI that has a high computational complexity. In this
work, we introduce two notions of backdoors building
upon the causal graph. We analyze the complexity of
finding a small backdoor (detection) and using the backdoor
to solve the problem (evaluation) in the light of
planning with (un)bounded domain/plan length. For each
setting we present either an fpt-result or rule out the existence
thereof by showing parameterized intractability.
In three cases we achieve the most desirable outcome:
detection and evaluation are fpt.","Backdoors to Planning Backdoors measure the distance to tractable fragments
and have become an important tool to find fixed-parameter
tractable (fpt) algorithms. Despite their success, backdoors
have not been used for planning, a central problem
in AI that has a high computational complexity. In this
work, we introduce two notions of backdoors building
upon the causal graph. We analyze the complexity of
finding a small backdoor (detection) and using the backdoor
to solve the problem (evaluation) in the light of
planning with (un)bounded domain/plan length. For each
setting we present either an fpt-result or rule out the existence
thereof by showing parameterized intractability.
In three cases we achieve the most desirable outcome:
detection and evaluation are fpt. Planning
Backdoors
Fixed-parameter tractable algorithms
(Parameterized) complexity",backdoor plan backdoor measur distanc tractabl fragment becom import tool find fixedparamet tractabl fpt algorithm despit success backdoor use plan central problem ai high comput complex work introduc two notion backdoor build upon causal graph analyz complex find small backdoor detect use backdoor solv problem evalu light plan unbound domainplan length set present either fptresult rule exist thereof show parameter intract three case achiev desir outcom detect evalu fpt plan backdoor fixedparamet tractabl algorithm parameter complex,5,17.748465,-6.8568363
Dynamic Multi-Agent Task Allocation with Spatial and Temporal Constraints,"Sofia Amador, Steven Okamoto and Roie Zivan",Multiagent Systems (MAS),"Task Allocation
Dynamic Problem
Cooperative Agents","MAS: Coordination and Collaboration
MAS: Distributed Problem Solving","Realistic multi-agent team applications often feature dynamic environments with soft deadlines that penalize late execution of tasks.  This puts a premium on quickly allocating tasks to agents, but finding the optimal allocation is NP-hard due to temporal and spatial constraints that require tasks to be executed sequentially by agents.

We propose FMC_TA, a novel task allocation algorithm that allows tasks to be easily sequenced to yield high-quality solutions. FMC_TA first finds allocations that are fair (envy-free), balancing the load and sharing important tasks between agents, and efficient (Pareto optimal) in a simplified version of the problem.  It computes such allocations in polynomial or pseudo-polynomial time (centrally or distributedly, respectively) using a Fisher market with agents as buyers and tasks as goods. It then heuristically schedules the allocations, taking into account inter-agent constraints on shared tasks.

We empirically compare our algorithm to state-of-the-art incomplete methods, both centralized and distributed, on law enforcement problems inspired by real police logs.  The results show a clear advantage for FMC_TA both in total utility and in other measures commonly used by law enforcement authorities.","Dynamic Multi-Agent Task Allocation with Spatial and Temporal Constraints Realistic multi-agent team applications often feature dynamic environments with soft deadlines that penalize late execution of tasks.  This puts a premium on quickly allocating tasks to agents, but finding the optimal allocation is NP-hard due to temporal and spatial constraints that require tasks to be executed sequentially by agents.

We propose FMC_TA, a novel task allocation algorithm that allows tasks to be easily sequenced to yield high-quality solutions. FMC_TA first finds allocations that are fair (envy-free), balancing the load and sharing important tasks between agents, and efficient (Pareto optimal) in a simplified version of the problem.  It computes such allocations in polynomial or pseudo-polynomial time (centrally or distributedly, respectively) using a Fisher market with agents as buyers and tasks as goods. It then heuristically schedules the allocations, taking into account inter-agent constraints on shared tasks.

We empirically compare our algorithm to state-of-the-art incomplete methods, both centralized and distributed, on law enforcement problems inspired by real police logs.  The results show a clear advantage for FMC_TA both in total utility and in other measures commonly used by law enforcement authorities. Task Allocation
Dynamic Problem
Cooperative Agents",dynam multiag task alloc spatial tempor constraint realist multiag team applic often featur dynam environ soft deadlin penal late execut task put premium quick alloc task agent find optim alloc nphard due tempor spatial constraint requir task execut sequenti agent propos fmcta novel task alloc algorithm allow task easili sequenc yield highqual solut fmcta first find alloc fair envyfre balanc load share import task agent effici pareto optim simplifi version problem comput alloc polynomi pseudopolynomi time central distribut respect use fisher market agent buyer task good heurist schedul alloc take account interag constraint share task empir compar algorithm stateoftheart incomplet method central distribut law enforc problem inspir real polic log result show clear advantag fmcta total util measur common use law enforc author task alloc dynam problem cooper agent,3,-20.281462,9.898532
Datalog Rewritability of Disjunctive Datalog Programs and its Applications to Ontology Reasoning,"Mark Kaminski, Yavor Nenov and Bernardo Cuenca Grau",Knowledge Representation and Reasoning (KRR),"disjunctive datalog
tractable reasoning
ontology-based query answering
OWL 2","KRR: Ontologies
KRR: Automated Reasoning and Theorem Proving
KRR: Computational Complexity of Reasoning
KRR: Description Logics
KRR: Knowledge Representation Languages
KRR: Logic Programming","We study the problem of rewriting a disjunctive datalog program into plain datalog. We show that a disjunctive program is rewritable if and only if it is equivalent to a linear disjunctive program, thus providing a novel characterisation of datalog rewritability. Motivated by this result, we propose weakly linear disjunctive datalog---a novel rule-based KR language that extends both datalog and linear disjunctive datalog and for which reasoning is tractable in data complexity. We then explore applications of weakly linear programs to ontology reasoning and propose a tractable extension of OWL 2 RL with disjunctive axioms. Our empirical results suggest that many non-Horn ontologies can be reduced to weakly linear programs and that query answering over such ontologies using a datalog engine is feasible in practice.","Datalog Rewritability of Disjunctive Datalog Programs and its Applications to Ontology Reasoning We study the problem of rewriting a disjunctive datalog program into plain datalog. We show that a disjunctive program is rewritable if and only if it is equivalent to a linear disjunctive program, thus providing a novel characterisation of datalog rewritability. Motivated by this result, we propose weakly linear disjunctive datalog---a novel rule-based KR language that extends both datalog and linear disjunctive datalog and for which reasoning is tractable in data complexity. We then explore applications of weakly linear programs to ontology reasoning and propose a tractable extension of OWL 2 RL with disjunctive axioms. Our empirical results suggest that many non-Horn ontologies can be reduced to weakly linear programs and that query answering over such ontologies using a datalog engine is feasible in practice. disjunctive datalog
tractable reasoning
ontology-based query answering
OWL 2",datalog rewrit disjunct datalog program applic ontolog reason studi problem rewrit disjunct datalog program plain datalog show disjunct program rewrit equival linear disjunct program thus provid novel characteris datalog rewrit motiv result propos weak linear disjunct dataloga novel rulebas kr languag extend datalog linear disjunct datalog reason tractabl data complex explor applic weak linear program ontolog reason propos tractabl extens owl 2 rl disjunct axiom empir result suggest mani nonhorn ontolog reduc weak linear program queri answer ontolog use datalog engin feasibl practic disjunct datalog tractabl reason ontologybas queri answer owl 2,8,-11.930668,2.8055704
Increasing VCG revenue by decreasing the quality of items,"Mingyu Guo, Argyrios Deligkas and Rahul Savani","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","auctions
mechanism design
VCG
revenue maximization","GTEP: Auctions and Market-Based Systems
MAS: Mechanism Design","The VCG mechanism is the standard method to incentivize bidders in combinatorial auctions to bid truthfully. Under the VCG mechanism, the auctioneer can sometimes increase revenue by “burning” items. We study this phenomenon in a setting where items are described by a number of attributes. The value of an attribute corresponds to a quality level, and bidders’ valuations are non-decreasing in the quality levels. In addition to burning items, we allow the auctioneer to present
some of the attributes as lower quality than they actually are. We study the following two revenue maximization problems under VCG: finding an optimal way to mark down items by reducing their quality levels, and finding an optimal set of items to burn. We study the effect of the following parameters on the computational complexity of these two problems: the number of attributes, the number of quality levels per attribute, and the complexity of the bidders’ valuation functions. Bidders have unit demand, so VCG’s outcome can be computed in polynomial time, and the valuation functions we consider are step functions that are non-decreasing with the quality levels. We prove that both problems are NP-hard even in the following three simple settings: a) four attributes, arbitrarily many quality levels per attribute, and single-step valuation functions, b) arbitrarily many attributes, two quality levels per attribute, and single-step valuation functions, and c) one attribute, arbitrarily many quality-levels, and multi-step valuation functions. For the case where items have only one attribute, and every bidder has a single-step valuations (that is zero below some quality threshold), we show that both problems can be solved in polynomial-time using a dynamic programming approach. For this case, we also quantify how much better marking down is than item burning, and provide examples where the improvement is best possible. Finally, we compare the revenue of both approaches with computational experiments.","Increasing VCG revenue by decreasing the quality of items The VCG mechanism is the standard method to incentivize bidders in combinatorial auctions to bid truthfully. Under the VCG mechanism, the auctioneer can sometimes increase revenue by “burning” items. We study this phenomenon in a setting where items are described by a number of attributes. The value of an attribute corresponds to a quality level, and bidders’ valuations are non-decreasing in the quality levels. In addition to burning items, we allow the auctioneer to present
some of the attributes as lower quality than they actually are. We study the following two revenue maximization problems under VCG: finding an optimal way to mark down items by reducing their quality levels, and finding an optimal set of items to burn. We study the effect of the following parameters on the computational complexity of these two problems: the number of attributes, the number of quality levels per attribute, and the complexity of the bidders’ valuation functions. Bidders have unit demand, so VCG’s outcome can be computed in polynomial time, and the valuation functions we consider are step functions that are non-decreasing with the quality levels. We prove that both problems are NP-hard even in the following three simple settings: a) four attributes, arbitrarily many quality levels per attribute, and single-step valuation functions, b) arbitrarily many attributes, two quality levels per attribute, and single-step valuation functions, and c) one attribute, arbitrarily many quality-levels, and multi-step valuation functions. For the case where items have only one attribute, and every bidder has a single-step valuations (that is zero below some quality threshold), we show that both problems can be solved in polynomial-time using a dynamic programming approach. For this case, we also quantify how much better marking down is than item burning, and provide examples where the improvement is best possible. Finally, we compare the revenue of both approaches with computational experiments. auctions
mechanism design
VCG
revenue maximization",increas vcg revenu decreas qualiti item vcg mechan standard method incentiv bidder combinatori auction bid truth vcg mechan auction sometim increas revenu “burning” item studi phenomenon set item describ number attribut valu attribut correspond qualiti level bidder valuat nondecreas qualiti level addit burn item allow auction present attribut lower qualiti actual studi follow two revenu maxim problem vcg find optim way mark item reduc qualiti level find optim set item burn studi effect follow paramet comput complex two problem number attribut number qualiti level per attribut complex bidder valuat function bidder unit demand vcg outcom comput polynomi time valuat function consid step function nondecreas qualiti level prove problem nphard even follow three simpl set four attribut arbitrarili mani qualiti level per attribut singlestep valuat function b arbitrarili mani attribut two qualiti level per attribut singlestep valuat function c one attribut arbitrarili mani qualitylevel multistep valuat function case item one attribut everi bidder singlestep valuat zero qualiti threshold show problem solv polynomialtim use dynam program approach case also quantifi much better mark item burn provid exampl improv best possibl final compar revenu approach comput experi auction mechan design vcg revenu maxim,5,11.0165205,13.9259815
Grandpa Hates Robots - Interaction Constraints for Planning in Inhabited Environments,"Uwe Köckemann, Federico Pecora and Lars Karlsson","Knowledge Representation and Reasoning (KRR)
Planning and Scheduling (PS)
Search and Constraint Satisfaction (SCS)","Constraint-based planning
Planning in inhabited environments
Human-aware planning","KRR: Preferences
PS: Scheduling
PS: Temporal Planning
PS: Planning (General/Other)
SCS: Constraint Satisfaction (General/other)","Consider a family whose home is equipped with several service robots.  The actions planned for the robots (e.g., doing chores, playing with the children) must adhere to {\em interaction constraints} relating them to human activities and preferences.  These constraints must be sufficiently expressive to model both temporal and logical dependencies among robot actions and human behavior, and must accommodate incomplete information regarding human activities.  In this paper we introduce an approach for automatically generating plans that are conformant wrt. given interaction constraints and partially specified human activities.  The approach allows to separate causal reasoning about actions from reasoning about interaction constraints, and we illustrate the computational advantage this brings with experiments on a large-scale (semi-)realistic household domain with hundreds of human activities and several robots.","Grandpa Hates Robots - Interaction Constraints for Planning in Inhabited Environments Consider a family whose home is equipped with several service robots.  The actions planned for the robots (e.g., doing chores, playing with the children) must adhere to {\em interaction constraints} relating them to human activities and preferences.  These constraints must be sufficiently expressive to model both temporal and logical dependencies among robot actions and human behavior, and must accommodate incomplete information regarding human activities.  In this paper we introduce an approach for automatically generating plans that are conformant wrt. given interaction constraints and partially specified human activities.  The approach allows to separate causal reasoning about actions from reasoning about interaction constraints, and we illustrate the computational advantage this brings with experiments on a large-scale (semi-)realistic household domain with hundreds of human activities and several robots. Constraint-based planning
Planning in inhabited environments
Human-aware planning",grandpa hate robot interact constraint plan inhabit environ consid famili whose home equip sever servic robot action plan robot eg chore play children must adher em interact constraint relat human activ prefer constraint must suffici express model tempor logic depend among robot action human behavior must accommod incomplet inform regard human activ paper introduc approach automat generat plan conform wrt given interact constraint partial specifi human activ approach allow separ causal reason action reason interact constraint illustr comput advantag bring experi largescal semirealist household domain hundr human activ sever robot constraintbas plan plan inhabit environ humanawar plan,3,-0.72437567,12.249732
The Most Uncreative Examinee: A First Step toward Wide Coverage Natural Language Math Problem Solving,"Takuya Matsuzaki, Hidenao Iwane, Hirokazu Anai and Noriko Arai",Knowledge Representation and Reasoning (KRR),"natural language semantics
mathematical problem solving
automated reasoning
computer algebra",KRR: Automated Reasoning and Theorem Proving,"We report on a project aiming at developing a system that solves a
wide range of math problems written in natural language. In the
system, formal analysis of natural language semantics is coupled with
automated reasoning technologies including computer algebra, using
logic as their common language. We have developed a prototype system
that accepts as its input a linguistically annotated problem text.
Using the prototype system as a reference point, we analyzed real
university entrance examination problems from the viewpoint of
end-to-end automated reasoning. Further, evaluation on entrance exam
mock tests revealed that an optimistic estimate of the system’s
performance already matches human averages on a few test sets.","The Most Uncreative Examinee: A First Step toward Wide Coverage Natural Language Math Problem Solving We report on a project aiming at developing a system that solves a
wide range of math problems written in natural language. In the
system, formal analysis of natural language semantics is coupled with
automated reasoning technologies including computer algebra, using
logic as their common language. We have developed a prototype system
that accepts as its input a linguistically annotated problem text.
Using the prototype system as a reference point, we analyzed real
university entrance examination problems from the viewpoint of
end-to-end automated reasoning. Further, evaluation on entrance exam
mock tests revealed that an optimistic estimate of the system’s
performance already matches human averages on a few test sets. natural language semantics
mathematical problem solving
automated reasoning
computer algebra",uncreat examine first step toward wide coverag natur languag math problem solv report project aim develop system solv wide rang math problem written natur languag system formal analysi natur languag semant coupl autom reason technolog includ comput algebra use logic common languag develop prototyp system accept input linguist annot problem text use prototyp system refer point analyz real univers entranc examin problem viewpoint endtoend autom reason evalu entranc exam mock test reveal optimist estim system perform alreadi match human averag test set natur languag semant mathemat problem solv autom reason comput algebra,8,7.4676704,-2.7355862
Acquiring Commonsense Knowledge for Sentiment Analysis through Human Computation,"Marina Boia, Claudiu Cristian Musat and Boi Faltings","Human-Computation and Crowd Sourcing (HCC)
Knowledge Representation and Reasoning (KRR)
NLP and Machine Learning (NLPML)","human computation
games with a purpose
crowdsourcing
commonsense knowledge
sentiment analysis
context","HCC: Domain-specific implementation challenges in human computation games
KRR: Knowledge Acquisition
NLPML: Text Classification","Many Artificial Intelligence tasks need large amounts of commonsense knowledge. Because obtaining this knowledge through machine learning would require a huge amount of data, a better alternative is to elicit it from people through human computation. We consider the sentiment classification task, where knowledge about the contexts that impact word polarities is crucial but hard to acquire from data. We show a novel task design that allows us to crowdsource this knowledge through Amazon Mechanical Turk with high quality. We show that the commonsense knowledge acquired in this way dramatically improves the performance of established sentiment classification methods.","Acquiring Commonsense Knowledge for Sentiment Analysis through Human Computation Many Artificial Intelligence tasks need large amounts of commonsense knowledge. Because obtaining this knowledge through machine learning would require a huge amount of data, a better alternative is to elicit it from people through human computation. We consider the sentiment classification task, where knowledge about the contexts that impact word polarities is crucial but hard to acquire from data. We show a novel task design that allows us to crowdsource this knowledge through Amazon Mechanical Turk with high quality. We show that the commonsense knowledge acquired in this way dramatically improves the performance of established sentiment classification methods. human computation
games with a purpose
crowdsourcing
commonsense knowledge
sentiment analysis
context",acquir commonsens knowledg sentiment analysi human comput mani artifici intellig task need larg amount commonsens knowledg obtain knowledg machin learn would requir huge amount data better altern elicit peopl human comput consid sentiment classif task knowledg context impact word polar crucial hard acquir data show novel task design allow us crowdsourc knowledg amazon mechan turk high qualiti show commonsens knowledg acquir way dramat improv perform establish sentiment classif method human comput game purpos crowdsourc commonsens knowledg sentiment analysi context,6,12.389559,-5.2111797
Optimistic Adaptive Submodularity at Scale,"Victor Gabillon, Branislav Kveton, Brian Eriksson, S. Muthukrishnan and Zheng Wen","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)
Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","Submodularity
Adaptive submodularity
Linear bandits
Online learning","APP: Other Applications
MLA: Machine Learning Applications (General/other)
NMLA: Active Learning
NMLA: Online Learning
NMLA: Recommender Systems
PS: Planning (General/Other)
RU: Sequential Decision Making","Maximization of submodular functions has wide applications in artificial intelligence and machine learning. In this work, we study the problem of learning how to maximize an adaptive submodular function. The function is initially unknown and we learn it by interacting repeatedly with the environment. A major problem in applying existing solutions to this problem is that their regret bounds scale linearly with the size of the problem. Therefore, these solutions are impractical even for moderately large problems. In this work, we use the structure of real-world problems to make learning practical. We make three main contributions. First, we propose a practical algorithm for learning how to maximize an adaptive submodular function where the distribution of the states of each item is conditioned on its features. Second, we analyze this algorithm and show that its expected cumulative regret is polylogarithmic in time. Finally, we evaluate our algorithm on two real-world problems, movie recommendation and face detection, and show that high-quality policies can be learned in just several hundred interactions.","Optimistic Adaptive Submodularity at Scale Maximization of submodular functions has wide applications in artificial intelligence and machine learning. In this work, we study the problem of learning how to maximize an adaptive submodular function. The function is initially unknown and we learn it by interacting repeatedly with the environment. A major problem in applying existing solutions to this problem is that their regret bounds scale linearly with the size of the problem. Therefore, these solutions are impractical even for moderately large problems. In this work, we use the structure of real-world problems to make learning practical. We make three main contributions. First, we propose a practical algorithm for learning how to maximize an adaptive submodular function where the distribution of the states of each item is conditioned on its features. Second, we analyze this algorithm and show that its expected cumulative regret is polylogarithmic in time. Finally, we evaluate our algorithm on two real-world problems, movie recommendation and face detection, and show that high-quality policies can be learned in just several hundred interactions. Submodularity
Adaptive submodularity
Linear bandits
Online learning",optimist adapt submodular scale maxim submodular function wide applic artifici intellig machin learn work studi problem learn maxim adapt submodular function function initi unknown learn interact repeat environ major problem appli exist solut problem regret bound scale linear size problem therefor solut impract even moder larg problem work use structur realworld problem make learn practic make three main contribut first propos practic algorithm learn maxim adapt submodular function distribut state item condit featur second analyz algorithm show expect cumul regret polylogarithm time final evalu algorithm two realworld problem movi recommend face detect show highqual polici learn sever hundr interact submodular adapt submodular linear bandit onlin learn,4,1.0915493,-10.217975
Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks,"Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin Wang and Tie-Yan Liu",Machine Learning Applications (MLA),"Sponsored Search
Recurrent Neural Network
Sequential Click Prediction","AIW: Enhancing web search and information retrieval
AIW: Machine learning and the web
MLA: Applications of Supervised Learning
MLA: Machine Learning Applications (General/other)","Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. While these studies aimed at providing a stationary interpretation on ad clicks, they were lack of the capability to understand user clicks in a dynamic way. As observed in real-world sponsored search system, user's behavior on the ad yield high dependency on how the user previously behaved along the time, especially in terms of the queries user submitted, click / non-click on ads, dwell time on landing page, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN) to model user's sequential behaviors into the click prediction process. Compared to traditional methods, this framework aims at effective click prediction by leveraging not only user's stationary historical behaviors but the rich information and patterns implied by user's dynamic sequential behaviors. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to other time-independent approaches.","Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. While these studies aimed at providing a stationary interpretation on ad clicks, they were lack of the capability to understand user clicks in a dynamic way. As observed in real-world sponsored search system, user's behavior on the ad yield high dependency on how the user previously behaved along the time, especially in terms of the queries user submitted, click / non-click on ads, dwell time on landing page, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN) to model user's sequential behaviors into the click prediction process. Compared to traditional methods, this framework aims at effective click prediction by leveraging not only user's stationary historical behaviors but the rich information and patterns implied by user's dynamic sequential behaviors. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to other time-independent approaches. Sponsored Search
Recurrent Neural Network
Sequential Click Prediction",sequenti click predict sponsor search recurr neural network click predict one fundament problem sponsor search exist studi took advantag machin learn approach predict ad click event ad view independ studi aim provid stationari interpret ad click lack capabl understand user click dynam way observ realworld sponsor search system user behavior ad yield high depend user previous behav along time especi term queri user submit click nonclick ad dwell time land page etc inspir observ introduc novel framework base recurr neural network rnn model user sequenti behavior click predict process compar tradit method framework aim effect click predict leverag user stationari histor behavior rich inform pattern impli user dynam sequenti behavior larg scale evalu clickthrough log commerci search engin demonstr approach signific improv click predict accuraci compar timeindepend approach sponsor search recurr neural network sequenti click predict,0,4.4390426,3.5399873
Backdoors into Heterogeneous Classes of SAT and CSP,"Serge Gaspers, Neeldhara Misra, Sebastian Ordyniak, Stefan Szeider and Stanislav Živný",Search and Constraint Satisfaction (SCS),"theoretical analysis
Constraint Satisfaction Problem (CSP)
Satisfiability (SAT)
polymorphism
backdoor set
parameterized complexity","KRR: Computational Complexity of Reasoning
SCS: Constraint Satisfaction
SCS: Satisfiability (General/Other)
SCS: Constraint Satisfaction (General/other)","Backdoor sets represent clever reasoning shortcuts through the search space for SAT and CSP.  By instantiating the backdoor variables one reduces the given instance to several easy instances that belong to a tractable class.  The overall time needed to solve the instance is exponential in the size of the backdoor set, hence it is a challenging problem to find a small backdoor set if one exists; over the last years this problem has been subject of intensive research.

In this paper we extend the classical notion of a strong backdoor set by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes; the union of the base classes forms a heterogeneous base class.  Backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones, hence they are much more desirable but possibly harder to find.

We draw a detailed complexity landscape for the problem of detecting strong backdoor sets into heterogeneous base classes for SAT and CSP. We provide algorithms that establish fixed-parameter tractability under natural parameterizations, and we contrast the tractability results with hardness results that pinpoint the theoretical limits.","Backdoors into Heterogeneous Classes of SAT and CSP Backdoor sets represent clever reasoning shortcuts through the search space for SAT and CSP.  By instantiating the backdoor variables one reduces the given instance to several easy instances that belong to a tractable class.  The overall time needed to solve the instance is exponential in the size of the backdoor set, hence it is a challenging problem to find a small backdoor set if one exists; over the last years this problem has been subject of intensive research.

In this paper we extend the classical notion of a strong backdoor set by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes; the union of the base classes forms a heterogeneous base class.  Backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones, hence they are much more desirable but possibly harder to find.

We draw a detailed complexity landscape for the problem of detecting strong backdoor sets into heterogeneous base classes for SAT and CSP. We provide algorithms that establish fixed-parameter tractability under natural parameterizations, and we contrast the tractability results with hardness results that pinpoint the theoretical limits. theoretical analysis
Constraint Satisfaction Problem (CSP)
Satisfiability (SAT)
polymorphism
backdoor set
parameterized complexity",backdoor heterogen class sat csp backdoor set repres clever reason shortcut search space sat csp instanti backdoor variabl one reduc given instanc sever easi instanc belong tractabl class overal time need solv instanc exponenti size backdoor set henc challeng problem find small backdoor set one exist last year problem subject intens research paper extend classic notion strong backdoor set allow differ instanti backdoor variabl result instanc belong differ base class union base class form heterogen base class backdoor set heterogen base class much smaller backdoor set homogen one henc much desir possibl harder find draw detail complex landscap problem detect strong backdoor set heterogen base class sat csp provid algorithm establish fixedparamet tractabl natur parameter contrast tractabl result hard result pinpoint theoret limit theoret analysi constraint satisfact problem csp satisfi sat polymorph backdoor set parameter complex,5,17.739895,-6.8499093
Theory of Cooperation in Complex Social Networks,"Bijan Ranjbar-Sahraei, Haitham Bou Ammar, Daan Bloembergen, Karl Tuyls and Gerhard Weiss","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","coevolutionary networks
evolution of cooperation
influencing social networks","GTEP: Game Theory
GTEP: Coordination and Collaboration
GTEP: Equilibrium
MAS: Agent-based Simulation and Emergent Behavior
MAS: Coordination and Collaboration
MAS: Evaluation and Analysis (Multiagent Systems)","This paper presents a theoretical as well as empirical study on the evolution of cooperation on complex social networks, following the continuous action iterated prisoner's dilemma (CAIPD) model. In particular, convergence to network-wide agreement is proven for both evolutionary networks with fixed interaction dynamics, as well as for coevolutionary networks where these dynamics change over time. Moreover, an extension to the CAIPD model is proposed that allows to model active influence of the evolution of cooperation in social networks. As such, this work contributes to a better understanding of behavioral change on social networks, and provides a first step towards their active control.","Theory of Cooperation in Complex Social Networks This paper presents a theoretical as well as empirical study on the evolution of cooperation on complex social networks, following the continuous action iterated prisoner's dilemma (CAIPD) model. In particular, convergence to network-wide agreement is proven for both evolutionary networks with fixed interaction dynamics, as well as for coevolutionary networks where these dynamics change over time. Moreover, an extension to the CAIPD model is proposed that allows to model active influence of the evolution of cooperation in social networks. As such, this work contributes to a better understanding of behavioral change on social networks, and provides a first step towards their active control. coevolutionary networks
evolution of cooperation
influencing social networks",theori cooper complex social network paper present theoret well empir studi evolut cooper complex social network follow continu action iter prison dilemma caipd model particular converg networkwid agreement proven evolutionari network fix interact dynam well coevolutionari network dynam chang time moreov extens caipd model propos allow model activ influenc evolut cooper social network work contribut better understand behavior chang social network provid first step toward activ control coevolutionari network evolut cooper influenc social network,5,12.877388,4.032888
Explanation-Based Approximate Weighted Model Counting for Probabilistic Logics,"Joris Renkens, Angelika Kimmig, Guy Van den Broeck and Luc De Raedt","Knowledge Representation and Reasoning (KRR)
Reasoning under Uncertainty (RU)","Probabilistic Logic Programming
Bounded Approximate Inference
Weighted Model Counting","KRR: Logic Programming
RU: Probabilistic Inference
RU: Relational Probabilistic Models","Probabilistic inference in statistical relational learning and probabilistic programming can be realised using weighted model counting. Despite a lot of progress, computing weighted model counts exactly is still infeasible for most problems of interest, and one typically has to resort to approximation methods. We contribute a new bounded approximation method for weighted model counting based on probabilistic logic programming principles. Our bounded approximation algorithm is an anytime algorithm that provides lower and upper bounds on the weighted model count. An empirical evaluation on probabilistic logic programs shows that our approach is effective in many cases that are currently beyond the reach of exact methods.","Explanation-Based Approximate Weighted Model Counting for Probabilistic Logics Probabilistic inference in statistical relational learning and probabilistic programming can be realised using weighted model counting. Despite a lot of progress, computing weighted model counts exactly is still infeasible for most problems of interest, and one typically has to resort to approximation methods. We contribute a new bounded approximation method for weighted model counting based on probabilistic logic programming principles. Our bounded approximation algorithm is an anytime algorithm that provides lower and upper bounds on the weighted model count. An empirical evaluation on probabilistic logic programs shows that our approach is effective in many cases that are currently beyond the reach of exact methods. Probabilistic Logic Programming
Bounded Approximate Inference
Weighted Model Counting",explanationbas approxim weight model count probabilist logic probabilist infer statist relat learn probabilist program realis use weight model count despit lot progress comput weight model count exact still infeas problem interest one typic resort approxim method contribut new bound approxim method weight model count base probabilist logic program principl bound approxim algorithm anytim algorithm provid lower upper bound weight model count empir evalu probabilist logic program show approach effect mani case current beyond reach exact method probabilist logic program bound approxim infer weight model count,5,10.908961,-14.711813
A Knowledge Compilation Map for Ordered Real-Valued Decision Diagrams,"Helene Fargier, Pierre Marquis, Alexandre Niveau and Nicolas Schmidt",Knowledge Representation and Reasoning (KRR),"Decision Diagrams (ADD - AADD - OBDD - SLDD)
Knowledge Compilation
Complexity","KRR: Computational Complexity of Reasoning
KRR: Knowledge Representation Languages
KRR: Preferences
SCS: Constraint Optimization","Valued decision diagrams (VDDs) are languages that represent functions mapping variable-value assignments to non-negative real numbers. They prove useful to compile cost functions, utility functions, or probability distributions. While the complexity of some queries (notably optimization) and transformations (notably conditioning) on VDD languages has been known for some time, there remain many significant queries and transformations, such as the various kinds of cuts, marginalizations, and combinations, the complexity of which has not been identified so far. This paper contributes to filling this gap and completing previous results about the time and space efficiency of VDD languages, thus leading to a knowledge compilation map for real-valued functions. Our results show that many tasks that are hard on valued CSPs are actually tractable on VDDs.","A Knowledge Compilation Map for Ordered Real-Valued Decision Diagrams Valued decision diagrams (VDDs) are languages that represent functions mapping variable-value assignments to non-negative real numbers. They prove useful to compile cost functions, utility functions, or probability distributions. While the complexity of some queries (notably optimization) and transformations (notably conditioning) on VDD languages has been known for some time, there remain many significant queries and transformations, such as the various kinds of cuts, marginalizations, and combinations, the complexity of which has not been identified so far. This paper contributes to filling this gap and completing previous results about the time and space efficiency of VDD languages, thus leading to a knowledge compilation map for real-valued functions. Our results show that many tasks that are hard on valued CSPs are actually tractable on VDDs. Decision Diagrams (ADD - AADD - OBDD - SLDD)
Knowledge Compilation
Complexity",knowledg compil map order realvalu decis diagram valu decis diagram vdds languag repres function map variablevalu assign nonneg real number prove use compil cost function util function probabl distribut complex queri notabl optim transform notabl condit vdd languag known time remain mani signific queri transform various kind cut margin combin complex identifi far paper contribut fill gap complet previous result time space effici vdd languag thus lead knowledg compil map realvalu function result show mani task hard valu csps actual tractabl vdds decis diagram add aadd obdd sldd knowledg compil complex,8,0.6032562,-3.9821317
Prices Matter for the Parameterized Complexity of Shift Bribery,"Robert Bredereck, Jiehua Chen, Piotr Faliszewski, André Nichterlein and Rolf Niedermeier","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","preferenced-based voting
campaign management
computational (in)tractability
parameterized complexity analysis
approximation","GTEP: Game Theory
GTEP: Social Choice / Voting
MAS: E-Commerce","In the Shift Bribery problem, we are given an election (based on preference orders), a preferred candidate p, and a budget. The goal is to ensure that p wins by shifting p higher in some voters' preference orders. However, each such shift request comes at a price (depending on the voter and on the extent of the shift) and we must not exceed the given budget. We study the parameterized computational complexity of Shift Bribery with respect to a number of parameters (pertaining to the nature of the solution sought and the size of the election) and several classes of price functions. When we parameterize Shift Bribery by the number of affected voters, then for each of our voting rules (Borda, Maximin, Copeland) the problem is W[2]-hard. If, instead, we parameterize by the number of positions by which p is shifted in total, then the problem is fixed-parameter tractable for Borda and Maximin, and is W[1]-hard for Copeland. If we parameterize by the budget for the cost of shifting, then the results depend on the price function class. We also show that Shift Bribery tends to be tractable when parameterized by the number of voters, but that the results for the number of candidates are more enigmatic.","Prices Matter for the Parameterized Complexity of Shift Bribery In the Shift Bribery problem, we are given an election (based on preference orders), a preferred candidate p, and a budget. The goal is to ensure that p wins by shifting p higher in some voters' preference orders. However, each such shift request comes at a price (depending on the voter and on the extent of the shift) and we must not exceed the given budget. We study the parameterized computational complexity of Shift Bribery with respect to a number of parameters (pertaining to the nature of the solution sought and the size of the election) and several classes of price functions. When we parameterize Shift Bribery by the number of affected voters, then for each of our voting rules (Borda, Maximin, Copeland) the problem is W[2]-hard. If, instead, we parameterize by the number of positions by which p is shifted in total, then the problem is fixed-parameter tractable for Borda and Maximin, and is W[1]-hard for Copeland. If we parameterize by the budget for the cost of shifting, then the results depend on the price function class. We also show that Shift Bribery tends to be tractable when parameterized by the number of voters, but that the results for the number of candidates are more enigmatic. preferenced-based voting
campaign management
computational (in)tractability
parameterized complexity analysis
approximation",price matter parameter complex shift briberi shift briberi problem given elect base prefer order prefer candid p budget goal ensur p win shift p higher voter prefer order howev shift request come price depend voter extent shift must exceed given budget studi parameter comput complex shift briberi respect number paramet pertain natur solut sought size elect sever class price function parameter shift briberi number affect voter vote rule borda maximin copeland problem w2hard instead parameter number posit p shift total problem fixedparamet tractabl borda maximin w1hard copeland parameter budget cost shift result depend price function class also show shift briberi tend tractabl parameter number voter result number candid enigmat preferencedbas vote campaign manag comput intract parameter complex analysi approxim,9,18.431942,-3.145035
Decomposing Activities of Daily Living to Discover Routine Clusters,"Onur Yuruten, Jiyong Zhang and Pearl Pu",Machine Learning Applications (MLA),"activity recognition
activities of daily living
time series clustering
low rank and sparse matrix decomposition","MLA: Applications of Unsupervised Learning
MLA: Machine Learning Applications (General/other)","An activity recognition system tries to analyze measurements of activities of daily living (ADLs) and automatically recognize whether someone is sitting, walking or running. Most of the existing approaches either have to rely on a model trained by a preselected and manually labeled set of activities, or perform micro-pattern analysis method, which requires manual selection of the lengths and the number of micro-patterns. Because real life ADL datasets are massive, the cost associated with these manual efforts is too high. As a result, these approaches limit the discovery of ADL patterns from real life datasets in a scalable way.

We propose a novel approach to extract meaningful patterns found in time-series ADL data. We use a matrix decomposition method to isolate routines and deviations to obtain two different sets of clusters. We obtain the final memberships via the cross product of these sets. We validate our approach using two real-life ADL datasets and a well-known artificial dataset. Based on average silhouette width scores, our approach can capture strong structures in the underlying data. Furthermore, results show that our approach improves on the accuracy of the baseline algorithms by 12% with a statistical significance (p<0.05) using the Wilcoxon signed-rank comparison test.","Decomposing Activities of Daily Living to Discover Routine Clusters An activity recognition system tries to analyze measurements of activities of daily living (ADLs) and automatically recognize whether someone is sitting, walking or running. Most of the existing approaches either have to rely on a model trained by a preselected and manually labeled set of activities, or perform micro-pattern analysis method, which requires manual selection of the lengths and the number of micro-patterns. Because real life ADL datasets are massive, the cost associated with these manual efforts is too high. As a result, these approaches limit the discovery of ADL patterns from real life datasets in a scalable way.

We propose a novel approach to extract meaningful patterns found in time-series ADL data. We use a matrix decomposition method to isolate routines and deviations to obtain two different sets of clusters. We obtain the final memberships via the cross product of these sets. We validate our approach using two real-life ADL datasets and a well-known artificial dataset. Based on average silhouette width scores, our approach can capture strong structures in the underlying data. Furthermore, results show that our approach improves on the accuracy of the baseline algorithms by 12% with a statistical significance (p<0.05) using the Wilcoxon signed-rank comparison test. activity recognition
activities of daily living
time series clustering
low rank and sparse matrix decomposition",decompos activ daili live discov routin cluster activ recognit system tri analyz measur activ daili live adl automat recogn whether someon sit walk run exist approach either reli model train preselect manual label set activ perform micropattern analysi method requir manual select length number micropattern real life adl dataset massiv cost associ manual effort high result approach limit discoveri adl pattern real life dataset scalabl way propos novel approach extract meaning pattern found timeseri adl data use matrix decomposit method isol routin deviat obtain two differ set cluster obtain final membership via cross product set valid approach use two reallif adl dataset wellknown artifici dataset base averag silhouett width score approach captur strong structur under data furthermor result show approach improv accuraci baselin algorithm 12 statist signific p005 use wilcoxon signedrank comparison test activ recognit activ daili live time seri cluster low rank spars matrix decomposit,7,-1.8577495,-2.7243218
On Hair Recognition in the Wild by Machine,Joseph Roth and Xiaoming Liu,Vision (VIS),"Vision
Biometrics
Face Recognition","VIS: Face and Gesture Recognition
VIS: Statistical Methods and Learning","We present an algorithm for identity inference using only the information from the hair. Face recognition in the wild (i.e., unconstrained settings) is highly useful in a variety of applications, but performance suffers due to many factors, e.g., obscured face, lighting variation, extreme pose angle, and expression. It is well known that humans use hair information to guide identity decisions under many of these scenarios due to either the consistent hair appearance of the same subject or obvious hair discrepancy of different subjects, but little work exists to replicate this intelligence artificially. We propose a learned hair matcher using shape, color, and texture features derived from localized patches through an AdaBoost technique with abstaining weak classifiers when features are not present in the given location. The proposed hair matcher achieves 71.53% accuracy on the LFW View 2 dataset. Hair also reduces the error of a COTS face matcher through simple score-level fusion by 5.7%.","On Hair Recognition in the Wild by Machine We present an algorithm for identity inference using only the information from the hair. Face recognition in the wild (i.e., unconstrained settings) is highly useful in a variety of applications, but performance suffers due to many factors, e.g., obscured face, lighting variation, extreme pose angle, and expression. It is well known that humans use hair information to guide identity decisions under many of these scenarios due to either the consistent hair appearance of the same subject or obvious hair discrepancy of different subjects, but little work exists to replicate this intelligence artificially. We propose a learned hair matcher using shape, color, and texture features derived from localized patches through an AdaBoost technique with abstaining weak classifiers when features are not present in the given location. The proposed hair matcher achieves 71.53% accuracy on the LFW View 2 dataset. Hair also reduces the error of a COTS face matcher through simple score-level fusion by 5.7%. Vision
Biometrics
Face Recognition",hair recognit wild machin present algorithm ident infer use inform hair face recognit wild ie unconstrain set high use varieti applic perform suffer due mani factor eg obscur face light variat extrem pose angl express well known human use hair inform guid ident decis mani scenario due either consist hair appear subject obvious hair discrep differ subject littl work exist replic intellig artifici propos learn hair matcher use shape color textur featur deriv local patch adaboost techniqu abstain weak classifi featur present given locat propos hair matcher achiev 7153 accuraci lfw view 2 dataset hair also reduc error cot face matcher simpl scorelevel fusion 57 vision biometr face recognit,1,2.7014508,-5.5373483
Capturing Relational Schemas and Functional Dependencies in RDFS,"Diego Calvanese, Wolfgang Fischl, Reinhard Pichler, Emanuel Sallinger and Mantas Simkus","AI and the Web (AIW)
Knowledge Representation and Reasoning (KRR)","identification constraints
functional dependencies
normal forms","AIW: Ontologies and the web: creation, extraction, evolution, mapping, merging, and alignment; tags and folksonomies
KRR: Description Logics
KRR: Knowledge Representation (General/Other)","Mapping relational data to RDF is an important task for the development of the
Semantic Web. To this end, the W3C has recently released a Recommendation for
the so-called direct mapping of relational data to RDF. In this work, we 
propose an enrichment of the direct mapping to make it more faithful by 
transferring also semantic information present in the relational schema from 
the relational world to the RDF world. We thus introduce expressive 
identification constraints to capture functional dependencies and define an 
RDF Normal Form, which precisely captures the classical Boyce-Codd Normal Form 
of relational schemas.","Capturing Relational Schemas and Functional Dependencies in RDFS Mapping relational data to RDF is an important task for the development of the
Semantic Web. To this end, the W3C has recently released a Recommendation for
the so-called direct mapping of relational data to RDF. In this work, we 
propose an enrichment of the direct mapping to make it more faithful by 
transferring also semantic information present in the relational schema from 
the relational world to the RDF world. We thus introduce expressive 
identification constraints to capture functional dependencies and define an 
RDF Normal Form, which precisely captures the classical Boyce-Codd Normal Form 
of relational schemas. identification constraints
functional dependencies
normal forms",captur relat schema function depend rdfs map relat data rdf import task develop semant web end w3c recent releas recommend socal direct map relat data rdf work propos enrich direct map make faith transfer also semant inform present relat schema relat world rdf world thus introduc express identif constraint captur function depend defin rdf normal form precis captur classic boycecodd normal form relat schema identif constraint function depend normal form,6,-4.743428,0.9189243
Maximum Satisfiability using core-guided MaxSAT Resolution,Nina Narodytska and Fahiem Bacchus,Search and Constraint Satisfaction (SCS),"maximum satisfiability
maxsat resolution
iterative SAT solving
weighted partial MaxSAT","SCS: Constraint Optimization
SCS: SAT and CSP: Evaluation and Analysis
SCS: SAT and CSP: Solvers and Tools
SCS: Satisfiability (General/Other)","Core-guided approaches to solving MaxSat have proved to be  effective on industrial problems containing hard clauses and  weighted soft clauses (weighted partial MaxSat or WPM). These  approaches solve WPM problems by building a sequence of new WPM formulas, where in each formula a greater weight of soft clauses can be relaxed. Relaxation of the soft clauses is achieved via the addition of blocking variables to the soft clauses, along with constraints on these blocking variables. In this work we propose an alternative approach. Our approach also builds a sequence of new WPM formulas. However, these formulas are constructed using MaxSat resolution, a sound rule of inference for MaxSat. MaxSat resolution can in the worst case cause a quadratic blowup in the formula, so we propose a new compressed version of MaxSat resolution. Using compressed MaxSat resolution our new core-guided solver improves the state-of-the-art solving significantly more problems than other state-of-the-art solvers on the industrial benchmarks used in the 2013 MaxSat Solver Evaluation.","Maximum Satisfiability using core-guided MaxSAT Resolution Core-guided approaches to solving MaxSat have proved to be  effective on industrial problems containing hard clauses and  weighted soft clauses (weighted partial MaxSat or WPM). These  approaches solve WPM problems by building a sequence of new WPM formulas, where in each formula a greater weight of soft clauses can be relaxed. Relaxation of the soft clauses is achieved via the addition of blocking variables to the soft clauses, along with constraints on these blocking variables. In this work we propose an alternative approach. Our approach also builds a sequence of new WPM formulas. However, these formulas are constructed using MaxSat resolution, a sound rule of inference for MaxSat. MaxSat resolution can in the worst case cause a quadratic blowup in the formula, so we propose a new compressed version of MaxSat resolution. Using compressed MaxSat resolution our new core-guided solver improves the state-of-the-art solving significantly more problems than other state-of-the-art solvers on the industrial benchmarks used in the 2013 MaxSat Solver Evaluation. maximum satisfiability
maxsat resolution
iterative SAT solving
weighted partial MaxSAT",maximum satisfi use coreguid maxsat resolut coreguid approach solv maxsat prove effect industri problem contain hard claus weight soft claus weight partial maxsat wpm approach solv wpm problem build sequenc new wpm formula formula greater weight soft claus relax relax soft claus achiev via addit block variabl soft claus along constraint block variabl work propos altern approach approach also build sequenc new wpm formula howev formula construct use maxsat resolut sound rule infer maxsat maxsat resolut worst case caus quadrat blowup formula propos new compress version maxsat resolut use compress maxsat resolut new coreguid solver improv stateoftheart solv signific problem stateoftheart solver industri benchmark use 2013 maxsat solver evalu maximum satisfi maxsat resolut iter sat solv weight partial maxsat,7,-11.419193,17.284883
Mind the Gap: Machine Translation by Minimizing the Semantic Gap in Embedding Space,Jiajun Zhang,"NLP and Knowledge Representation (NLPKR)
NLP and Machine Learning (NLPML)","Statistical Machine Translation
Semantic Phrase Representation
Recursive Neural Networks
Semantic Gap Minimization","NLPKR: Natural Language Processing (General/Other)
NLPML: Natural Language Processing (General/Other)","The conventional statistical machine translation (SMT) methods perform the decoding process by compositing a set of the translation rules which have the highest probability. However, the probabilities of the translation rules are calculated only according to the cooccurrence statistics in the bilingual corpus rather than the semantic meaning similarity. In this paper, we propose a Recursive Neural Network (RNN) based model that converts each translation rule into a compact real-valued vector in the semantic embedding space and performs the decoding process by minimizing the semantic gap between the source language string and its translation candidates at each state in a bottom-up structure. The RNN-based translation model is trained using a max-margin objective function. Extensive experiments on Chinese-to-English translation show that our RNN-based model can significantly improve the translation quality by up to 1.68 BLEU score.","Mind the Gap: Machine Translation by Minimizing the Semantic Gap in Embedding Space The conventional statistical machine translation (SMT) methods perform the decoding process by compositing a set of the translation rules which have the highest probability. However, the probabilities of the translation rules are calculated only according to the cooccurrence statistics in the bilingual corpus rather than the semantic meaning similarity. In this paper, we propose a Recursive Neural Network (RNN) based model that converts each translation rule into a compact real-valued vector in the semantic embedding space and performs the decoding process by minimizing the semantic gap between the source language string and its translation candidates at each state in a bottom-up structure. The RNN-based translation model is trained using a max-margin objective function. Extensive experiments on Chinese-to-English translation show that our RNN-based model can significantly improve the translation quality by up to 1.68 BLEU score. Statistical Machine Translation
Semantic Phrase Representation
Recursive Neural Networks
Semantic Gap Minimization",mind gap machin translat minim semant gap embed space convent statist machin translat smt method perform decod process composit set translat rule highest probabl howev probabl translat rule calcul accord cooccurr statist bilingu corpus rather semant mean similar paper propos recurs neural network rnn base model convert translat rule compact realvalu vector semant embed space perform decod process minim semant gap sourc languag string translat candid state bottomup structur rnnbase translat model train use maxmargin object function extens experi chinesetoenglish translat show rnnbase model signific improv translat qualiti 168 bleu score statist machin translat semant phrase represent recurs neural network semant gap minim,5,-16.992008,15.215556
Rounded Dynamic Programming for Tree-Structured Stochastic Network Design,"Xiaojian Wu, Daniel Sheldon and Shlomo Zilberstein","Computational Sustainability and AI (CSAI)
Heuristic Search and Optimization (HSO)
Planning and Scheduling (PS)","stochastic network design
dynamic programming
barrier removal
river networks
influence maximization
stochastic optimization","CSAI: Modeling and prediction of dynamic and spatiotemporal phenomena and systems
CSAI: Control and optimization of dynamic and spatiotemporal systems
CSAI: Network modeling, prediction, and optimization.
HSO: Optimization
PS: Probabilistic Planning","We develop a fast approximation algorithm called rounded dynamic programming (RDP) for stochastic network design problems on directed trees. The underlying model describes phenomena that spread away from the root of a tree, for example, the spread of influence in a hierarchical organization or fish in a river network. Actions can be taken to intervene in the network—for some cost—to increase the probability of propagation along an edge. Our algorithm selects a set of actions to maximize the overall spread in the network under a limited budget. We prove that the algorithm is a fully polynomial-time approximation scheme (FPTAS), that is, it finds (1−ε)-optimal solutions in time polynomial in the input size and 1/ε. We apply the algorithm to an important motivating problem in Computational Sustainability: that of efficiently allocating funds to remove barriers in a river network so fish can reach greater portions of their native range. Our experiments show that our algorithm is able to produce near- optimal solutions much faster than an existing technique.","Rounded Dynamic Programming for Tree-Structured Stochastic Network Design We develop a fast approximation algorithm called rounded dynamic programming (RDP) for stochastic network design problems on directed trees. The underlying model describes phenomena that spread away from the root of a tree, for example, the spread of influence in a hierarchical organization or fish in a river network. Actions can be taken to intervene in the network—for some cost—to increase the probability of propagation along an edge. Our algorithm selects a set of actions to maximize the overall spread in the network under a limited budget. We prove that the algorithm is a fully polynomial-time approximation scheme (FPTAS), that is, it finds (1−ε)-optimal solutions in time polynomial in the input size and 1/ε. We apply the algorithm to an important motivating problem in Computational Sustainability: that of efficiently allocating funds to remove barriers in a river network so fish can reach greater portions of their native range. Our experiments show that our algorithm is able to produce near- optimal solutions much faster than an existing technique. stochastic network design
dynamic programming
barrier removal
river networks
influence maximization
stochastic optimization",round dynam program treestructur stochast network design develop fast approxim algorithm call round dynam program rdp stochast network design problem direct tree under model describ phenomena spread away root tree exampl spread influenc hierarch organ fish river network action taken interven network—for cost—to increas probabl propag along edg algorithm select set action maxim overal spread network limit budget prove algorithm fulli polynomialtim approxim scheme fptas find 1−εoptim solut time polynomi input size 1ε appli algorithm import motiv problem comput sustain effici alloc fund remov barrier river network fish reach greater portion nativ rang experi show algorithm abl produc near optim solut much faster exist techniqu stochast network design dynam program barrier remov river network influenc maxim stochast optim,5,10.810271,3.4776301
Incentives for Truthful Information Elicitation of Continuous Signals,Goran Radanovic and Boi Faltings,"Game Theory and Economic Paradigms (GTEP)
Human-Computation and Crowd Sourcing (HCC)
Multiagent Systems (MAS)","Mechanism Design
Information elicitation
Peer prediction","GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information
HCC: Game-theoretic mechanism design of incentives for motivation and honest reporting
MAS: E-Commerce
MAS: Mechanism Design
MAS: Multiagent Systems (General/other)","Information elicitation mechanisms represent an important component of many information aggregation techniques, such as product reviews, community sensing, or opinion polls. We propose a novel mechanism that elicits both private signals and beliefs. The mechanism extends the previous versions of the Bayesian Truth Serums (the original BTS, the RBTS, and the multi-valued BTS), by allowing small populations and non-binary private signals, while not requiring additional assumptions on the belief updating process. For priors that are sufficiently smooth, such as Gaussians, the mechanism allows signals to be continuous.","Incentives for Truthful Information Elicitation of Continuous Signals Information elicitation mechanisms represent an important component of many information aggregation techniques, such as product reviews, community sensing, or opinion polls. We propose a novel mechanism that elicits both private signals and beliefs. The mechanism extends the previous versions of the Bayesian Truth Serums (the original BTS, the RBTS, and the multi-valued BTS), by allowing small populations and non-binary private signals, while not requiring additional assumptions on the belief updating process. For priors that are sufficiently smooth, such as Gaussians, the mechanism allows signals to be continuous. Mechanism Design
Information elicitation
Peer prediction",incent truth inform elicit continu signal inform elicit mechan repres import compon mani inform aggreg techniqu product review communiti sens opinion poll propos novel mechan elicit privat signal belief mechan extend previous version bayesian truth serum origin bts rbts multivalu bts allow small popul nonbinari privat signal requir addit assumpt belief updat process prior suffici smooth gaussian mechan allow signal continu mechan design inform elicit peer predict,0,12.917212,14.334572
Equilibria in Epidemic Containment Games,"Sudip Saha, Abhijin Adiga and Anil Kumar S. Vullikanti","Applications (APP)
Computational Sustainability and AI (CSAI)
Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","network security game
nash equilibria
malware propagation
epidemic control
security
protection
network infection
immunization
graph theory
game theory
spectral radius","APP: Security and Privacy
CSAI: Modeling the interactions of agents with different and often conflicting interests
GTEP: Game Theory
GTEP: Equilibrium
MAS: Evaluation and Analysis (Multiagent Systems)
MAS: Multiagent Systems (General/other)","The spread of epidemics and malware is commonly modeled by diffusion processes
on networks. Protective interventions such as vaccinations or installing anti-virus software are used to contain their spread. Typically, each node in the network has to decide its own strategy of securing itself, and its benefit depends on which other nodes are secure, making this a natural game-theoretic setting. There has been a lot of work on network security game models, but most of the focus has been either on simplified epidemic models or homogeneous network structure.

We develop a new formulation for an epidemic containment game, which relies on the
characterization of the SIS model in terms of the spectral radius of the network.
We show that, in this model, pure Nash equilibria (NE) always exist, and can be found by a best response strategy. We analyze the complexity of finding NE, and derive rigorous bounds on their costs and the Price of Anarchy or PoA (the ratio of the costs of the worst NE to the best NE) in general graphs as well as in random graph models. In particular, for arbitrary power-law graphs with exponent $\beta>2$, we show that the PoA is bounded by $O(T^{2(\beta-1)})$, where $T=\gamma/\alpha$ is the ratio of the recovery rate to the transmission rate in the SIS model.
For the Chung-Lu random power-law graph model, we prove this bound is tight for the PoA. We study the characteristics of Nash equilibria empirically in different real communication and infrastructure networks, and find that our analytical results can help explain some of the empirical observations.","Equilibria in Epidemic Containment Games The spread of epidemics and malware is commonly modeled by diffusion processes
on networks. Protective interventions such as vaccinations or installing anti-virus software are used to contain their spread. Typically, each node in the network has to decide its own strategy of securing itself, and its benefit depends on which other nodes are secure, making this a natural game-theoretic setting. There has been a lot of work on network security game models, but most of the focus has been either on simplified epidemic models or homogeneous network structure.

We develop a new formulation for an epidemic containment game, which relies on the
characterization of the SIS model in terms of the spectral radius of the network.
We show that, in this model, pure Nash equilibria (NE) always exist, and can be found by a best response strategy. We analyze the complexity of finding NE, and derive rigorous bounds on their costs and the Price of Anarchy or PoA (the ratio of the costs of the worst NE to the best NE) in general graphs as well as in random graph models. In particular, for arbitrary power-law graphs with exponent $\beta>2$, we show that the PoA is bounded by $O(T^{2(\beta-1)})$, where $T=\gamma/\alpha$ is the ratio of the recovery rate to the transmission rate in the SIS model.
For the Chung-Lu random power-law graph model, we prove this bound is tight for the PoA. We study the characteristics of Nash equilibria empirically in different real communication and infrastructure networks, and find that our analytical results can help explain some of the empirical observations. network security game
nash equilibria
malware propagation
epidemic control
security
protection
network infection
immunization
graph theory
game theory
spectral radius",equilibria epidem contain game spread epidem malwar common model diffus process network protect intervent vaccin instal antivirus softwar use contain spread typic node network decid strategi secur benefit depend node secur make natur gametheoret set lot work network secur game model focus either simplifi epidem model homogen network structur develop new formul epidem contain game reli character sis model term spectral radius network show model pure nash equilibria ne alway exist found best respons strategi analyz complex find ne deriv rigor bound cost price anarchi poa ratio cost worst ne best ne general graph well random graph model particular arbitrari powerlaw graph expon beta2 show poa bound ot2beta1 tgammaalpha ratio recoveri rate transmiss rate sis model chunglu random powerlaw graph model prove bound tight poa studi characterist nash equilibria empir differ real communic infrastructur network find analyt result help explain empir observ network secur game nash equilibria malwar propag epidem control secur protect network infect immun graph theori game theori spectral radius,2,9.161877,4.260829
Beat the Cheater: Computing Game-Theoretic Strategies for When to Kick a Gambler out of a Casino,"Troels Bjerre Sørensen, Melissa Dalis, Joshua Letchford, Dmytro Korzhyk and Vincent Conitzer","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Security
Stackelberg
Gambling
Game Theory","GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information","Gambles in casinos are usually set up so that the casino makes a profit in expectation---as long as gamblers play honestly. However, some gamblers are able to cheat, reducing the casino's profit. How should the casino address this? A common strategy is to selectively kick gamblers out, possibly even without being sure that they were cheating. In this paper, we address the following question. Based solely on a gambler's track record, when is it optimal for the casino to kick the gambler out? Because cheaters will adapt to the casino's policy, this is a game-theoretic question. Specifically, we model the problem as a Bayesian game in which the casino is a Stackelberg leader that can commit to a (possibly randomized) policy for when to kick gamblers out, and provide efficient algorithms for computing the optimal policy.
Besides being potentially useful to casinos, we imagine that similar techniques could be useful for addressing related problems---for example, illegal trades in financial markets.","Beat the Cheater: Computing Game-Theoretic Strategies for When to Kick a Gambler out of a Casino Gambles in casinos are usually set up so that the casino makes a profit in expectation---as long as gamblers play honestly. However, some gamblers are able to cheat, reducing the casino's profit. How should the casino address this? A common strategy is to selectively kick gamblers out, possibly even without being sure that they were cheating. In this paper, we address the following question. Based solely on a gambler's track record, when is it optimal for the casino to kick the gambler out? Because cheaters will adapt to the casino's policy, this is a game-theoretic question. Specifically, we model the problem as a Bayesian game in which the casino is a Stackelberg leader that can commit to a (possibly randomized) policy for when to kick gamblers out, and provide efficient algorithms for computing the optimal policy.
Besides being potentially useful to casinos, we imagine that similar techniques could be useful for addressing related problems---for example, illegal trades in financial markets. Security
Stackelberg
Gambling
Game Theory",beat cheater comput gametheoret strategi kick gambler casino gambl casino usual set casino make profit expectationa long gambler play honest howev gambler abl cheat reduc casino profit casino address common strategi select kick gambler possibl even without sure cheat paper address follow question base sole gambler track record optim casino kick gambler cheater adapt casino polici gametheoret question specif model problem bayesian game casino stackelberg leader commit possibl random polici kick gambler provid effici algorithm comput optim polici besid potenti use casino imagin similar techniqu could use address relat problemsfor exampl illeg trade financi market secur stackelberg gambl game theori,2,1.1046084,-1.3001033
A Characterization of the Single-Peaked Single-Crossing Domain,"Edith Elkind, Piotr Faliszewski and Piotr Skowron","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","elections
voting
single-peaked
single-crossing
1-Euclidean
proportional representation
Monroe
algorithms","GTEP: Game Theory
GTEP: Social Choice / Voting
MAS: E-Commerce","We investigate elections that are simultaneously single-peaked and single-crossing
(SPSC). We show that the domain of 1-dimensional Euclidean elections (where voters and candidates are points on the real line, and each voter prefers the candidates that are close to her to the ones that are further away) is a proper subdomain of the SPSC domain, by constructing an election that is single-peaked and single-crossing, but not 1-Euclidean. We then establish a connection between narcissistic elections (where each candidate is ranked first by at least one voter), single-peaked elections and single-crossing elections, by showing that an election is SPSC if and only if it can be obtained from a narcissistic single-crossing election by deleting voters. We use this characterization to show that the SPSC domain admits an efficient algorithm for a problem in fully proportional representation.","A Characterization of the Single-Peaked Single-Crossing Domain We investigate elections that are simultaneously single-peaked and single-crossing
(SPSC). We show that the domain of 1-dimensional Euclidean elections (where voters and candidates are points on the real line, and each voter prefers the candidates that are close to her to the ones that are further away) is a proper subdomain of the SPSC domain, by constructing an election that is single-peaked and single-crossing, but not 1-Euclidean. We then establish a connection between narcissistic elections (where each candidate is ranked first by at least one voter), single-peaked elections and single-crossing elections, by showing that an election is SPSC if and only if it can be obtained from a narcissistic single-crossing election by deleting voters. We use this characterization to show that the SPSC domain admits an efficient algorithm for a problem in fully proportional representation. elections
voting
single-peaked
single-crossing
1-Euclidean
proportional representation
Monroe
algorithms",character singlepeak singlecross domain investig elect simultan singlepeak singlecross spsc show domain 1dimension euclidean elect voter candid point real line voter prefer candid close one away proper subdomain spsc domain construct elect singlepeak singlecross 1euclidean establish connect narcissist elect candid rank first least one voter singlepeak elect singlecross elect show elect spsc obtain narcissist singlecross elect delet voter use character show spsc domain admit effici algorithm problem fulli proport represent elect vote singlepeak singlecross 1euclidean proport represent monro algorithm,9,19.712166,-2.9412122
A Support-Based Algorithm for the Bi-Objective Pareto Constraint,Renaud Hartert and Pierre Schaus,Search and Constraint Satisfaction (SCS),"Constraint Programming
Bi-Objective Combinatorial Optimization
Global Constraint
Pareto Constraint","SCS: Constraint Satisfaction
SCS: Constraint Optimization
SCS: Global Constraints","Bi-objective combinatorial optimization problems are ubiquitous in real-world applications and designing approaches to solve them efficiently is an important research area of Artificial Intelligence. In Constraint Programming, the recently introduced bi-objective Pareto constraint allows one to solve bi-objective combinatorial optimization problems exactly. Using this constraint, every non-dominated solution is collected in a single tree-search while pruning sub-trees that cannot lead to a non-dominated solution. This paper introduces a simpler and more efficient filtering algorithm for the bi-objective Pareto constraint. The efficiency of our algorithm is experimentally confirmed on classical bi-objective benchmarks.","A Support-Based Algorithm for the Bi-Objective Pareto Constraint Bi-objective combinatorial optimization problems are ubiquitous in real-world applications and designing approaches to solve them efficiently is an important research area of Artificial Intelligence. In Constraint Programming, the recently introduced bi-objective Pareto constraint allows one to solve bi-objective combinatorial optimization problems exactly. Using this constraint, every non-dominated solution is collected in a single tree-search while pruning sub-trees that cannot lead to a non-dominated solution. This paper introduces a simpler and more efficient filtering algorithm for the bi-objective Pareto constraint. The efficiency of our algorithm is experimentally confirmed on classical bi-objective benchmarks. Constraint Programming
Bi-Objective Combinatorial Optimization
Global Constraint
Pareto Constraint",supportbas algorithm biobject pareto constraint biobject combinatori optim problem ubiquit realworld applic design approach solv effici import research area artifici intellig constraint program recent introduc biobject pareto constraint allow one solv biobject combinatori optim problem exact use constraint everi nondomin solut collect singl treesearch prune subtre cannot lead nondomin solut paper introduc simpler effici filter algorithm biobject pareto constraint effici algorithm experiment confirm classic biobject benchmark constraint program biobject combinatori optim global constraint pareto constraint,3,-4.0064135,5.2912793
HC-Search for Multi-Label Prediction: An Empirical Study,"Janardhan Rao Doppa, Jun Yu, Chao Ma, Alan Fern and Prasad Tadepalli",Novel Machine Learning Algorithms (NMLA),"Supervised Learning
Multi-Label Classification
Structured Prediction",NMLA: Supervised Learning (Other),"Multi-label learning concerns learning multiple, overlapping, and correlated classes. In this paper, we adapt a recent structured prediction framework called HC-Search for multi-label prediction problems. One of the main advantages of this framework is that its training is sensitive to the loss function, unlike the other multi-label approaches that either assume a specific loss function or require a manual adaptation to each loss function. We empirically evaluate our instantiation of the HC-Search framework along with many existing multi-label learning algorithms on a variety of benchmarks by employing diverse task loss functions. Our results demonstrate that the performance of existing algorithms tends to be very similar in most cases, and that the HC-Search approach is comparable and often better than all other algorithms across different loss functions.","HC-Search for Multi-Label Prediction: An Empirical Study Multi-label learning concerns learning multiple, overlapping, and correlated classes. In this paper, we adapt a recent structured prediction framework called HC-Search for multi-label prediction problems. One of the main advantages of this framework is that its training is sensitive to the loss function, unlike the other multi-label approaches that either assume a specific loss function or require a manual adaptation to each loss function. We empirically evaluate our instantiation of the HC-Search framework along with many existing multi-label learning algorithms on a variety of benchmarks by employing diverse task loss functions. Our results demonstrate that the performance of existing algorithms tends to be very similar in most cases, and that the HC-Search approach is comparable and often better than all other algorithms across different loss functions. Supervised Learning
Multi-Label Classification
Structured Prediction",hcsearch multilabel predict empir studi multilabel learn concern learn multipl overlap correl class paper adapt recent structur predict framework call hcsearch multilabel predict problem one main advantag framework train sensit loss function unlik multilabel approach either assum specif loss function requir manual adapt loss function empir evalu instanti hcsearch framework along mani exist multilabel learn algorithm varieti benchmark employ divers task loss function result demonstr perform exist algorithm tend similar case hcsearch approach compar often better algorithm across differ loss function supervis learn multilabel classif structur predict,6,-5.95216,-12.035398
Semi-supervised Matrix Completion for Cross-Lingual Text Classification,Min Xiao and Yuhong Guo,"Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","cross lingual classification
semi-supervised learning
matrix completion","MLA: Machine Learning Applications (General/other)
NMLA: Classification
NMLA: Semisupervised Learning","Cross-lingual text classification is the task of assigning labels to a given document in a label-scarce target language by using a prediction model trained with labeled documents from a label-rich source language, which is popularly studied in the natural language processing area as it can largely decrease the expensive manual annotation effort in the target language. In this work, we proposed a novel semi-supervised representation learning approach to address this challenging task, which discovers interlingual features by simultaneously performing semi-supervised matrix completion. To evaluate the proposed learning technique, we conducted extensive experiments on eighteen cross language sentiment classification tasks with four different languages. The empirical results demonstrated the efficacy of our approach and outperformed the other comparison methods.","Semi-supervised Matrix Completion for Cross-Lingual Text Classification Cross-lingual text classification is the task of assigning labels to a given document in a label-scarce target language by using a prediction model trained with labeled documents from a label-rich source language, which is popularly studied in the natural language processing area as it can largely decrease the expensive manual annotation effort in the target language. In this work, we proposed a novel semi-supervised representation learning approach to address this challenging task, which discovers interlingual features by simultaneously performing semi-supervised matrix completion. To evaluate the proposed learning technique, we conducted extensive experiments on eighteen cross language sentiment classification tasks with four different languages. The empirical results demonstrated the efficacy of our approach and outperformed the other comparison methods. cross lingual classification
semi-supervised learning
matrix completion",semisupervis matrix complet crosslingu text classif crosslingu text classif task assign label given document labelscarc target languag use predict model train label document labelrich sourc languag popular studi natur languag process area larg decreas expens manual annot effort target languag work propos novel semisupervis represent learn approach address challeng task discov interlingu featur simultan perform semisupervis matrix complet evalu propos learn techniqu conduct extens experi eighteen cross languag sentiment classif task four differ languag empir result demonstr efficaci approach outperform comparison method cross lingual classif semisupervis learn matrix complet,6,-9.269544,-7.373244
Approximate Lifting Techniques for Belief Propagation,"Parag Singla, Aniruddh Nath and Pedro Domingos",Reasoning under Uncertainty (RU),"Lifted Inference
Belief Propagation
Graphical Models","RU: Graphical Models (Other)
RU: Probabilistic Inference
RU: Relational Probabilistic Models","Many AI applications need to explicitly represent the relational structure as well as handle uncertainty. First order probabilistic models combine the power of logic and probability to deal with such domains. A naive approach to inference in these models is to propositionalize the whole theory and carry out the inference on the ground network. Lifted inference techniques (such as Lifted Belief Propagation; Singla & Domingos, 2008) provide a scalable approach to inference by combining together groups of objects which behave identically. In many cases, constructing the lifted network can itself be quite costly. In addition, the exact lifted network is often very close in size to the fully propositionalized model. To overcome these problems, we present approximate lifted inference, which groups together similar but distinguishable objects and treats them as if they were identical. Early stopping terminates the execution of the lifted network construction at an early stage resulting in a coarser network. Noise tolerant hypercubes allow for marginal errors in the representation of the lifted network itself. Both of our algorithms can significantly speed-up the process of lifted network construction as well as result in much smaller models. The coarseness of the approximation can be adjusted depending on the accuracy required, and we can bound the resulting error. Extensive evaluation on six domains demonstrates great efficiency gains with only minor (or no) loss in accuracy.","Approximate Lifting Techniques for Belief Propagation Many AI applications need to explicitly represent the relational structure as well as handle uncertainty. First order probabilistic models combine the power of logic and probability to deal with such domains. A naive approach to inference in these models is to propositionalize the whole theory and carry out the inference on the ground network. Lifted inference techniques (such as Lifted Belief Propagation; Singla & Domingos, 2008) provide a scalable approach to inference by combining together groups of objects which behave identically. In many cases, constructing the lifted network can itself be quite costly. In addition, the exact lifted network is often very close in size to the fully propositionalized model. To overcome these problems, we present approximate lifted inference, which groups together similar but distinguishable objects and treats them as if they were identical. Early stopping terminates the execution of the lifted network construction at an early stage resulting in a coarser network. Noise tolerant hypercubes allow for marginal errors in the representation of the lifted network itself. Both of our algorithms can significantly speed-up the process of lifted network construction as well as result in much smaller models. The coarseness of the approximation can be adjusted depending on the accuracy required, and we can bound the resulting error. Extensive evaluation on six domains demonstrates great efficiency gains with only minor (or no) loss in accuracy. Lifted Inference
Belief Propagation
Graphical Models",approxim lift techniqu belief propag mani ai applic need explicit repres relat structur well handl uncertainti first order probabilist model combin power logic probabl deal domain naiv approach infer model proposition whole theori carri infer ground network lift infer techniqu lift belief propag singla domingo 2008 provid scalabl approach infer combin togeth group object behav ident mani case construct lift network quit cost addit exact lift network often close size fulli proposition model overcom problem present approxim lift infer group togeth similar distinguish object treat ident earli stop termin execut lift network construct earli stage result coarser network nois toler hypercub allow margin error represent lift network algorithm signific speedup process lift network construct well result much smaller model coars approxim adjust depend accuraci requir bound result error extens evalu six domain demonstr great effici gain minor loss accuraci lift infer belief propag graphic model,5,-8.153299,2.824283
Cost-Based Query Optimization via AI Planning,"Nathan Robinson, Sheila Mcilraith and David Toman","Knowledge Representation and Reasoning (KRR)
Planning and Scheduling (PS)","relational query optimization
delete-free planning
cost-optimal planning
heuristic search
applications of planning","KRR: Knowledge Representation (General/Other)
PS: Deterministic Planning
PS: Planning (General/Other)","The generation of high quality query plans is at the heart of query processing in traditional database management systems as well as in heterogeneous distributed data sources on corporate intranets and in the cloud. A diversity of techniques are employed for query plan generation and optimization, many of them proprietary. In this paper we revisit the problem of generating a query plan using AI automated planning. Characterizing query planning as AI planning enables us to leverage state-of-the-art planning techniques -- techniques which have proven to be highly effective for a diversity of dynamical reasoning tasks. While our long-term view is broad, here our efforts focus on the specific problem of cost-based join-order optimization, a central component of production-quality query optimizers. We characterize the general query planning problem as a delete-free planning problem, and query plan optimization as a context-sensitive cost-optimal planning problem. We propose algorithms that generate high quality query plans, guaranteeing optimality under certain conditions. Our approach is general, supporting the use of a broad suite of domain-independent and domain-specific optimization criteria. Experimental results demonstrate the effectiveness of AI planning techniques for query plan generation and optimization.","Cost-Based Query Optimization via AI Planning The generation of high quality query plans is at the heart of query processing in traditional database management systems as well as in heterogeneous distributed data sources on corporate intranets and in the cloud. A diversity of techniques are employed for query plan generation and optimization, many of them proprietary. In this paper we revisit the problem of generating a query plan using AI automated planning. Characterizing query planning as AI planning enables us to leverage state-of-the-art planning techniques -- techniques which have proven to be highly effective for a diversity of dynamical reasoning tasks. While our long-term view is broad, here our efforts focus on the specific problem of cost-based join-order optimization, a central component of production-quality query optimizers. We characterize the general query planning problem as a delete-free planning problem, and query plan optimization as a context-sensitive cost-optimal planning problem. We propose algorithms that generate high quality query plans, guaranteeing optimality under certain conditions. Our approach is general, supporting the use of a broad suite of domain-independent and domain-specific optimization criteria. Experimental results demonstrate the effectiveness of AI planning techniques for query plan generation and optimization. relational query optimization
delete-free planning
cost-optimal planning
heuristic search
applications of planning",costbas queri optim via ai plan generat high qualiti queri plan heart queri process tradit databas manag system well heterogen distribut data sourc corpor intranet cloud divers techniqu employ queri plan generat optim mani proprietari paper revisit problem generat queri plan use ai autom plan character queri plan ai plan enabl us leverag stateoftheart plan techniqu techniqu proven high effect divers dynam reason task longterm view broad effort focus specif problem costbas joinord optim central compon productionqu queri optim character general queri plan problem deletefre plan problem queri plan optim contextsensit costoptim plan problem propos algorithm generat high qualiti queri plan guarante optim certain condit approach general support use broad suit domainindepend domainspecif optim criteria experiment result demonstr effect ai plan techniqu queri plan generat optim relat queri optim deletefre plan costoptim plan heurist search applic plan,8,-3.0327582,19.38346
Efficient buyer groups for prediction-of-use electricity tariffs,"Valentin Robu, Meritxell Vinyals, Alex Rogers and Nick Jennings","Computational Sustainability and AI (CSAI)
Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","electricity tariff
group buying
smart grid","CSAI: Modeling the interactions of agents with different and often conflicting interests
CSAI: Support for public engagement and decision making by the public
GTEP: Coordination and Collaboration
MAS: Coordination and Collaboration
MAS: Evaluation and Analysis (Multiagent Systems)","Current electricity tariffs do not reflect the real cost that customers incur to suppliers, as units are charged at the same rate, regardless of how predictable each customer's consumption is. A recent proposal to address this problem are prediction-of-use tariffs. In such tariffs, a customer is asked in advance to predict her future consumption, and is charged based both on her actual consumption and the deviation from her prediction. Prior work studied the cost game induced by a single such tariff, and showed consumers would have an incentive to minimize their risk, by joining together when buying electricity as a grand coalition. In this work we study the efficient (i.e. cost-minimizing) structure of buying groups for the more realistic setting when multiple, competing prediction-of-use tariffs are available. We propose a polynomial time algorithm to compute efficient buyer groups, and validate our approach experimentally, using a large-scale data set of domestic electricity consumers in the UK.","Efficient buyer groups for prediction-of-use electricity tariffs Current electricity tariffs do not reflect the real cost that customers incur to suppliers, as units are charged at the same rate, regardless of how predictable each customer's consumption is. A recent proposal to address this problem are prediction-of-use tariffs. In such tariffs, a customer is asked in advance to predict her future consumption, and is charged based both on her actual consumption and the deviation from her prediction. Prior work studied the cost game induced by a single such tariff, and showed consumers would have an incentive to minimize their risk, by joining together when buying electricity as a grand coalition. In this work we study the efficient (i.e. cost-minimizing) structure of buying groups for the more realistic setting when multiple, competing prediction-of-use tariffs are available. We propose a polynomial time algorithm to compute efficient buyer groups, and validate our approach experimentally, using a large-scale data set of domestic electricity consumers in the UK. electricity tariff
group buying
smart grid",effici buyer group predictionofus electr tariff current electr tariff reflect real cost custom incur supplier unit charg rate regardless predict custom consumpt recent propos address problem predictionofus tariff tariff custom ask advanc predict futur consumpt charg base actual consumpt deviat predict prior work studi cost game induc singl tariff show consum would incent minim risk join togeth buy electr grand coalit work studi effici ie costminim structur buy group realist set multipl compet predictionofus tariff avail propos polynomi time algorithm comput effici buyer group valid approach experiment use largescal data set domest electr consum uk electr tariff group buy smart grid,0,7.033031,9.724395
Distribution-Aware Sampling and Weighted Model Counting for SAT,"Supratik Chakraborty, Daniel J. Fremont, Kuldeep S. Meel, Sanjit A. Seshia and Moshe Vardi",Search and Constraint Satisfaction (SCS),"Weighted Model Counting
Weight Generation
SAT","SCS: SAT and CSP: Evaluation and Analysis
SCS: SAT and CSP: Solvers and Tools
SCS: Satisfiability (General/Other)","Given a CNF formula and a weight for each assignment of values to
variables, two natural problems are weighted model counting and
distribution-aware sampling of satisfying assignments.  Both problems 
have a wide variety of important applications.  Due to the inherent
complexity of the exact versions of the problems, interest has focused
on solving them approximately.  Prior work in this area scaled only to
small problems in practice, or failed to provide strong theoretical
guarantees, or employed a computationally-expensive maximum a poste-
riori probability (MAP) oracle that assumes prior knowledge of a
factored representation of the weight distribution.  We present a
novel approach that works with a black-box oracle for weights of
assignments and requires only an {\NP}-oracle to solve both the
counting and sampling problems.  Our approach works 
under mild assumptions on the distribution of weights of satisfying
assignments, provides strong theoretical guarantees, and scales to
problems involving several thousand variables. We also show that the
assumptions can be significantly relaxed if a factored representation 
of the weights is known.","Distribution-Aware Sampling and Weighted Model Counting for SAT Given a CNF formula and a weight for each assignment of values to
variables, two natural problems are weighted model counting and
distribution-aware sampling of satisfying assignments.  Both problems 
have a wide variety of important applications.  Due to the inherent
complexity of the exact versions of the problems, interest has focused
on solving them approximately.  Prior work in this area scaled only to
small problems in practice, or failed to provide strong theoretical
guarantees, or employed a computationally-expensive maximum a poste-
riori probability (MAP) oracle that assumes prior knowledge of a
factored representation of the weight distribution.  We present a
novel approach that works with a black-box oracle for weights of
assignments and requires only an {\NP}-oracle to solve both the
counting and sampling problems.  Our approach works 
under mild assumptions on the distribution of weights of satisfying
assignments, provides strong theoretical guarantees, and scales to
problems involving several thousand variables. We also show that the
assumptions can be significantly relaxed if a factored representation 
of the weights is known. Weighted Model Counting
Weight Generation
SAT",distributionawar sampl weight model count sat given cnf formula weight assign valu variabl two natur problem weight model count distributionawar sampl satisfi assign problem wide varieti import applic due inher complex exact version problem interest focus solv approxim prior work area scale small problem practic fail provid strong theoret guarante employ computationallyexpens maximum post riori probabl map oracl assum prior knowledg factor represent weight distribut present novel approach work blackbox oracl weight assign requir nporacl solv count sampl problem approach work mild assumpt distribut weight satisfi assign provid strong theoret guarante scale problem involv sever thousand variabl also show assumpt signific relax factor represent weight known weight model count weight generat sat,5,10.897112,-14.091077
Online and Stochastic Learning with a Human Cognitive Bias,Hidekazu Oiwa and Hiroshi Nakagawa,"Human-Computation and Crowd Sourcing (HCC)
Novel Machine Learning Algorithms (NMLA)","Machine Learning
Human Cognitive Bias
Online Learning
Stochastic Learning
Endowment effect","HCC: Optimality in the context of human computation
NMLA: Classification
NMLA: Online Learning","Sequential learning for classification tasks is an effective tool in the machine learning community. In sequential learning settings, algorithms sometimes make incorrect predictions on data that were correctly classified in the past. This paper explicitly deals with such inconsistent prediction behavior. Our main contributions are 1) to experimentally show its effect for user utilities as a human cognitive bias, 2) to formalize a new framework by internalizing this bias into the optimization problem, 3) to develop new algorithms without memorization of the past prediction history, and 4) to show some theoretical guarantees of our derived algorithm for both online and stochastic learning settings. Our experimental results show the superiority of the derived algorithm for problems involving human cognition.","Online and Stochastic Learning with a Human Cognitive Bias Sequential learning for classification tasks is an effective tool in the machine learning community. In sequential learning settings, algorithms sometimes make incorrect predictions on data that were correctly classified in the past. This paper explicitly deals with such inconsistent prediction behavior. Our main contributions are 1) to experimentally show its effect for user utilities as a human cognitive bias, 2) to formalize a new framework by internalizing this bias into the optimization problem, 3) to develop new algorithms without memorization of the past prediction history, and 4) to show some theoretical guarantees of our derived algorithm for both online and stochastic learning settings. Our experimental results show the superiority of the derived algorithm for problems involving human cognition. Machine Learning
Human Cognitive Bias
Online Learning
Stochastic Learning
Endowment effect",onlin stochast learn human cognit bias sequenti learn classif task effect tool machin learn communiti sequenti learn set algorithm sometim make incorrect predict data correct classifi past paper explicit deal inconsist predict behavior main contribut 1 experiment show effect user util human cognit bias 2 formal new framework intern bias optim problem 3 develop new algorithm without memor past predict histori 4 show theoret guarante deriv algorithm onlin stochast learn set experiment result show superior deriv algorithm problem involv human cognit machin learn human cognit bias onlin learn stochast learn endow effect,4,0.9497026,-13.52203
Adaptive Singleton-based Consistencies,"Amine Balafrej, Christian Bessiere, Gilles Trombettoni and El Houssine Bouyakhf",Search and Constraint Satisfaction (SCS),"CSP
Singleton-based Consistencies
Adaptive Consistencies",SCS: Constraint Satisfaction,"Singleton-based consistencies have been shown to dramatically
   improve the performance of constraint solvers on some difficult
   instances. However, they are in general too expensive to be applied
   exhaustively during the whole search. In this paper, we focus on
   partition-one-AC, a singleton-based consistency which, as opposed
   to singleton arc consistency, is able to prune values on all
   variables at each singleton test.
   We propose adaptive variants of partition-one-AC that do not
   necessarily run until having proved the fixpoint. The pruning
   can be weaker than the full version but the computational effort
   can be significantly reduced. Our experiments
   show that adaptive Partition-one-AC can obtain significant speedups over arc
   consistency and over the full version of partition-one-AC.","Adaptive Singleton-based Consistencies Singleton-based consistencies have been shown to dramatically
   improve the performance of constraint solvers on some difficult
   instances. However, they are in general too expensive to be applied
   exhaustively during the whole search. In this paper, we focus on
   partition-one-AC, a singleton-based consistency which, as opposed
   to singleton arc consistency, is able to prune values on all
   variables at each singleton test.
   We propose adaptive variants of partition-one-AC that do not
   necessarily run until having proved the fixpoint. The pruning
   can be weaker than the full version but the computational effort
   can be significantly reduced. Our experiments
   show that adaptive Partition-one-AC can obtain significant speedups over arc
   consistency and over the full version of partition-one-AC. CSP
Singleton-based Consistencies
Adaptive Consistencies",adapt singletonbas consist singletonbas consist shown dramat improv perform constraint solver difficult instanc howev general expens appli exhaust whole search paper focus partitiononeac singletonbas consist oppos singleton arc consist abl prune valu variabl singleton test propos adapt variant partitiononeac necessarili run prove fixpoint prune weaker full version comput effort signific reduc experi show adapt partitiononeac obtain signific speedup arc consist full version partitiononeac csp singletonbas consist adapt consist,5,-2.5464106,1.2447112
Scheduling for Transfers in Pickup and Delivery Problems with Very Large Neighborhood Search,Brian Coltin and Manuela Veloso,Planning and Scheduling (PS),"scheduling
transfers
PDP","HSO: Heuristic Search
HSO: Metareasoning and Metaheuristics
PS: Scheduling","In pickup and delivery problems (PDPs), vehicles pick up and deliver a set of items under various constraints. We extend the well-studied PDP by allowing vehicles to transfer items to and from one another. By scheduling transfers, the fleet of vehicles can deliver the items faster and at lower cost. We introduce the Very Large Neighborhood Search with Transfers (VLNS-T) algorithm to form schedules for PDPs with transfers. We show that VLNS-T algorithm makes use of transfers to improve upon the best known solutions for selected benchmark problems, and demonstrate its effectiveness on real world taxi data in New York City.","Scheduling for Transfers in Pickup and Delivery Problems with Very Large Neighborhood Search In pickup and delivery problems (PDPs), vehicles pick up and deliver a set of items under various constraints. We extend the well-studied PDP by allowing vehicles to transfer items to and from one another. By scheduling transfers, the fleet of vehicles can deliver the items faster and at lower cost. We introduce the Very Large Neighborhood Search with Transfers (VLNS-T) algorithm to form schedules for PDPs with transfers. We show that VLNS-T algorithm makes use of transfers to improve upon the best known solutions for selected benchmark problems, and demonstrate its effectiveness on real world taxi data in New York City. scheduling
transfers
PDP",schedul transfer pickup deliveri problem larg neighborhood search pickup deliveri problem pdps vehicl pick deliv set item various constraint extend wellstudi pdp allow vehicl transfer item one anoth schedul transfer fleet vehicl deliv item faster lower cost introduc larg neighborhood search transfer vlnst algorithm form schedul pdps transfer show vlnst algorithm make use transfer improv upon best known solut select benchmark problem demonstr effect real world taxi data new york citi schedul transfer pdp,6,-15.147548,-7.2073054
Unsupervised Alignment of Natural Language Instructions with Video Segments,"Iftekhar Naim, Young Song, Qiguang Liu, Henry Kautz, Jiebo Luo and Daniel Gildea","Machine Learning Applications (MLA)
NLP and Machine Learning (NLPML)","Unsupervised Video Alignment
Grounded Language Acquisition
HMM
IBM Model 1
Language and Vision","MLA: Applications of Unsupervised Learning
NLPML: Natural Language Processing (General/Other)
VIS: Language and Vision","We propose an unsupervised learning algorithm for automatically inferring the mappings between English nouns and corresponding video objects. Given a sequence of natural language instructions and an unaligned video recording, we simultaneously align each instruction to its corresponding video segment, and also align nouns in each instruction to their corresponding objects in video. While existing grounded language acquisition algorithms rely on pre-aligned supervised data (each sentence paired with corresponding image frame or video segment), our algorithm aims to automatically infer the alignment from the temporal structure of the video and parallel text instructions. We propose two generative models that are closely related to the HMM and IBM 1 word alignment models used in statistical machine translation. We evaluate our algorithm on videos of biological experiments performed in wetlabs, and demonstrate its capability of aligning video segments to text instructions and matching video objects to nouns in the absence of any direct supervision.","Unsupervised Alignment of Natural Language Instructions with Video Segments We propose an unsupervised learning algorithm for automatically inferring the mappings between English nouns and corresponding video objects. Given a sequence of natural language instructions and an unaligned video recording, we simultaneously align each instruction to its corresponding video segment, and also align nouns in each instruction to their corresponding objects in video. While existing grounded language acquisition algorithms rely on pre-aligned supervised data (each sentence paired with corresponding image frame or video segment), our algorithm aims to automatically infer the alignment from the temporal structure of the video and parallel text instructions. We propose two generative models that are closely related to the HMM and IBM 1 word alignment models used in statistical machine translation. We evaluate our algorithm on videos of biological experiments performed in wetlabs, and demonstrate its capability of aligning video segments to text instructions and matching video objects to nouns in the absence of any direct supervision. Unsupervised Video Alignment
Grounded Language Acquisition
HMM
IBM Model 1
Language and Vision",unsupervis align natur languag instruct video segment propos unsupervis learn algorithm automat infer map english noun correspond video object given sequenc natur languag instruct unalign video record simultan align instruct correspond video segment also align noun instruct correspond object video exist ground languag acquisit algorithm reli prealign supervis data sentenc pair correspond imag frame video segment algorithm aim automat infer align tempor structur video parallel text instruct propos two generat model close relat hmm ibm 1 word align model use statist machin translat evalu algorithm video biolog experi perform wetlab demonstr capabl align video segment text instruct match video object noun absenc direct supervis unsupervis video align ground languag acquisit hmm ibm model 1 languag vision,6,-20.298225,-0.11418656
Detecting information-dense texts in multiple news domains,Yinfei Yang and Ani Nenkova,"AI and the Web (AIW)
NLP and Knowledge Representation (NLPKR)
NLP and Machine Learning (NLPML)
NLP and Text Mining (NLPTM)","writing style
information-dense text
summarization","AIW: Human language technologies for web systems, including text summarization and machine translation
NLPKR: Semantics and Summarization
NLPML: Text Classification
NLPTM: Natural Language Processing (General/Other)","In this paper we introduce the task of identifying information-dense texts, which report important factual information in direct, succinct manner.  We describe a procedure that allows us to label automatically a large training corpus of New York Times texts. We train a classifier based on lexical, discourse and unlexicalized syntactic features and test its performance on a set of manually annotated articles from international relations, U.S. politics, sports and science domains. Our results indicate that the task is feasible and that  both syntactic and lexical features are highly predictive for the distinction. We observe considerable variation of prediction accuracy across domains and find that domain-specific models are more accurate.","Detecting information-dense texts in multiple news domains In this paper we introduce the task of identifying information-dense texts, which report important factual information in direct, succinct manner.  We describe a procedure that allows us to label automatically a large training corpus of New York Times texts. We train a classifier based on lexical, discourse and unlexicalized syntactic features and test its performance on a set of manually annotated articles from international relations, U.S. politics, sports and science domains. Our results indicate that the task is feasible and that  both syntactic and lexical features are highly predictive for the distinction. We observe considerable variation of prediction accuracy across domains and find that domain-specific models are more accurate. writing style
information-dense text
summarization",detect informationdens text multipl news domain paper introduc task identifi informationdens text report import factual inform direct succinct manner describ procedur allow us label automat larg train corpus new york time text train classifi base lexic discours unlexic syntact featur test perform set manual annot articl intern relat us polit sport scienc domain result indic task feasibl syntact lexic featur high predict distinct observ consider variat predict accuraci across domain find domainspecif model accur write style informationdens text summar,6,-6.4536376,-3.5247374
Smarter Than You Think: Acquiring Comparative Commonsense from the Web,"Niket Tandon, Gerard de Melo and Gerhard Weikum","AI and the Web (AIW)
NLP and Text Mining (NLPTM)","commonsense knowledge
information extraction
word sense disambiguation","AIW: Knowledge acquisition from the web
NLPTM: Information Extraction","This paper presents a method for automatically constructing a large comparative commonsense knowledge base from Big Data. The resulting knowledge base is semantically refined and organized. Our method is based on linear optimization methods to clean and consolidate the noisy input knowledge, while also inferring new information. Our method achieves a high precision while maintaining good coverage.","Smarter Than You Think: Acquiring Comparative Commonsense from the Web This paper presents a method for automatically constructing a large comparative commonsense knowledge base from Big Data. The resulting knowledge base is semantically refined and organized. Our method is based on linear optimization methods to clean and consolidate the noisy input knowledge, while also inferring new information. Our method achieves a high precision while maintaining good coverage. commonsense knowledge
information extraction
word sense disambiguation",smarter think acquir compar commonsens web paper present method automat construct larg compar commonsens knowledg base big data result knowledg base semant refin organ method base linear optim method clean consolid noisi input knowledg also infer new inform method achiev high precis maintain good coverag commonsens knowledg inform extract word sens disambigu,5,12.531421,-5.1439495
A reasoner for the RCC-5 and RCC-8 calculi extended with constants,"Stella Giannakopoulou, Charalampos Nikolaou and Manolis Koubarakis","Knowledge Representation and Reasoning (KRR)
Reasoning under Uncertainty (RU)
Search and Constraint Satisfaction (SCS)","Qualitative spatial reasoning
Constraint Satisfaction Problems
Landmarks","KRR: Computational Complexity of Reasoning
KRR: Geometric, Spatial, and Temporal Reasoning
KRR: Qualitative Reasoning
RU: Uncertainty in AI (General/Other)
SCS: Constraint Satisfaction","The problem of checking the consistency in qualitative calculi that contain both unknown and known entities (constants, i.e., real geometries) has recently appeared and has applications in many areas. Until now, all the approaches are theoretical and no implementation has been proposed. In this paper we present the first reasoner that takes as input RCC-5 or RCC-8 networks that involve entities with specific geometries and decides their consistency. We investigate the performance of the 
reasoner and contrary to lots of other works in this area we consider real datasets in our experimental analysis.","A reasoner for the RCC-5 and RCC-8 calculi extended with constants The problem of checking the consistency in qualitative calculi that contain both unknown and known entities (constants, i.e., real geometries) has recently appeared and has applications in many areas. Until now, all the approaches are theoretical and no implementation has been proposed. In this paper we present the first reasoner that takes as input RCC-5 or RCC-8 networks that involve entities with specific geometries and decides their consistency. We investigate the performance of the 
reasoner and contrary to lots of other works in this area we consider real datasets in our experimental analysis. Qualitative spatial reasoning
Constraint Satisfaction Problems
Landmarks",reason rcc5 rcc8 calculi extend constant problem check consist qualit calculi contain unknown known entiti constant ie real geometri recent appear applic mani area approach theoret implement propos paper present first reason take input rcc5 rcc8 network involv entiti specif geometri decid consist investig perform reason contrari lot work area consid real dataset experiment analysi qualit spatial reason constraint satisfact problem landmark,5,-5.0329256,10.861712
Schedule-based Robotic Search for Multiple Residents in a Retirement Home Environment,"Markus Schwenk, Tiago Vaquero and Goldie Nejat","Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)
Robotics (ROB)","Uncertainty in AI
Probabilistic Planning
Temporal Planning
Robotics","PS: Probabilistic Planning
PS: Temporal Planning
RU: Uncertainty in AI (General/Other)
ROB: Robotics (General/Other)","In this paper we address the planning problem of a robot searching for multiple residents in a retirement home in order to remind them of an upcoming multi-person recreational activity before a given deadline. We introduce a novel Multi-User Schedule Based (M-USB) Search approach which generates a high-level-plan to maximize the number of residents that are found within the given time frame. From the schedules of the residents, the layout of the retirement home environment as well as direct observations by the robot, we obtain spatio-temporal likelihood functions for the individual residents. The main contribution of our work is the development of a novel approach to compute a reward to find a search plan for the robot using: 1) the likelihood functions, 2) the availabilities of the residents, and 3) the order in which the residents should be found. Simulations were conducted on a floor of a real retirement home to compare our proposed M-USB Search approach to a Weighted Informed Walk and a Random Walk. Our results show that the proposed M-USB Search finds residents in a shorter amount of time by visiting fewer rooms when compared to the other approaches.","Schedule-based Robotic Search for Multiple Residents in a Retirement Home Environment In this paper we address the planning problem of a robot searching for multiple residents in a retirement home in order to remind them of an upcoming multi-person recreational activity before a given deadline. We introduce a novel Multi-User Schedule Based (M-USB) Search approach which generates a high-level-plan to maximize the number of residents that are found within the given time frame. From the schedules of the residents, the layout of the retirement home environment as well as direct observations by the robot, we obtain spatio-temporal likelihood functions for the individual residents. The main contribution of our work is the development of a novel approach to compute a reward to find a search plan for the robot using: 1) the likelihood functions, 2) the availabilities of the residents, and 3) the order in which the residents should be found. Simulations were conducted on a floor of a real retirement home to compare our proposed M-USB Search approach to a Weighted Informed Walk and a Random Walk. Our results show that the proposed M-USB Search finds residents in a shorter amount of time by visiting fewer rooms when compared to the other approaches. Uncertainty in AI
Probabilistic Planning
Temporal Planning
Robotics",schedulebas robot search multipl resid retir home environ paper address plan problem robot search multipl resid retir home order remind upcom multiperson recreat activ given deadlin introduc novel multius schedul base musb search approach generat highlevelplan maxim number resid found within given time frame schedul resid layout retir home environ well direct observ robot obtain spatiotempor likelihood function individu resid main contribut work develop novel approach comput reward find search plan robot use 1 likelihood function 2 avail resid 3 order resid found simul conduct floor real retir home compar propos musb search approach weight inform walk random walk result show propos musb search find resid shorter amount time visit fewer room compar approach uncertainti ai probabilist plan tempor plan robot,3,0.25715154,10.508813
Diagram Understanding in Geometry Problems,"Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi and Oren Etzioni",Applications (APP),"Diagram Understanding
Submodular Optimization
Language and Vision","APP: Other Applications
VIS: Language and Vision","Automatically solving geometry questions is a long-standing AI
problem. A geometry question typically includes a textual description
accompanied by a diagram.  The first step in solving geometry
questions is diagram understanding, which consists of identifying visual
elements in the diagram, their location, their geometric properties,
and aligning them to corresponding textual descriptions. In this
paper, we present a method for diagram understanding that identifies
visual elements in a diagram while maximizing agreement between
textual and visual data. We show that the method's objective function
is submodular; thus we are able to introduce an efficient method for
diagram understanding that is close to optimal.  To empirically
evaluate our method, we compile a new dataset of geometry questions
(textual descriptions and diagrams) and compare with baselines that
utilize standard vision techniques.  Our experimental evaluation shows
an F1 boost of more than 17\% in identifying visual elements and 25\% in
aligning visual elements with their textual descriptions.","Diagram Understanding in Geometry Problems Automatically solving geometry questions is a long-standing AI
problem. A geometry question typically includes a textual description
accompanied by a diagram.  The first step in solving geometry
questions is diagram understanding, which consists of identifying visual
elements in the diagram, their location, their geometric properties,
and aligning them to corresponding textual descriptions. In this
paper, we present a method for diagram understanding that identifies
visual elements in a diagram while maximizing agreement between
textual and visual data. We show that the method's objective function
is submodular; thus we are able to introduce an efficient method for
diagram understanding that is close to optimal.  To empirically
evaluate our method, we compile a new dataset of geometry questions
(textual descriptions and diagrams) and compare with baselines that
utilize standard vision techniques.  Our experimental evaluation shows
an F1 boost of more than 17\% in identifying visual elements and 25\% in
aligning visual elements with their textual descriptions. Diagram Understanding
Submodular Optimization
Language and Vision",diagram understand geometri problem automat solv geometri question longstand ai problem geometri question typic includ textual descript accompani diagram first step solv geometri question diagram understand consist identifi visual element diagram locat geometr properti align correspond textual descript paper present method diagram understand identifi visual element diagram maxim agreement textual visual data show method object function submodular thus abl introduc effici method diagram understand close optim empir evalu method compil new dataset geometri question textual descript diagram compar baselin util standard vision techniqu experiment evalu show f1 boost 17 identifi visual element 25 align visual element textual descript diagram understand submodular optim languag vision,5,7.284667,-5.191542
Latent Domains Modeling for Domain Adaptation,"Caiming Xiong, Scott McCloskey and Jason Corso","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)
Vision (VIS)","latent model
local linear subspace
domain adaptation","MLA: Applications of Unsupervised Learning
MLA: Machine Learning Applications (General/other)
NMLA: Clustering
NMLA: Feature Construction/Reformulation
NMLA: Transfer, Adaptation, Multitask Learning
NMLA: Semisupervised Learning
VIS: Categorization
VIS: Object Recognition
VIS: Statistical Methods and Learning","To improve robustness to significant mismatches between
source domain and target domain - arising from changes such
as illumination, pose and image quality - domain adaptation
is increasingly popular in computer vision. But most of methods
assume that the source data is from single domain, or that
multi-domain datasets provide the domain label for training
instances. In practice, most datasets are mixtures of multiple
latent domains, and difficult to manually provide the domain
label of each data point. In this paper, we propose a model
that automatically discovers latent domains in visual datasets.
We first assume the visual images are sampled from multiple
manifolds, each of which represents different domain,
and which are represented by different subspaces. Using the
neighborhood structure estimated from images belonging to
the same category, we approximate the local linear invariant
subspace for each image based on its local structure, eliminating
the category-specific elements of the feature. Based
on the effectiveness of this representation, we then propose a
squared-loss mutual information based clustering model with
category distribution prior in each domain to infer the domain
assignment for images. In experiment, we test our approach
on two common image datasets, the results show that
our method outperforms the existing state-of-the-art methods,
and also show the superiority of multiple latent domain discovery","Latent Domains Modeling for Domain Adaptation To improve robustness to significant mismatches between
source domain and target domain - arising from changes such
as illumination, pose and image quality - domain adaptation
is increasingly popular in computer vision. But most of methods
assume that the source data is from single domain, or that
multi-domain datasets provide the domain label for training
instances. In practice, most datasets are mixtures of multiple
latent domains, and difficult to manually provide the domain
label of each data point. In this paper, we propose a model
that automatically discovers latent domains in visual datasets.
We first assume the visual images are sampled from multiple
manifolds, each of which represents different domain,
and which are represented by different subspaces. Using the
neighborhood structure estimated from images belonging to
the same category, we approximate the local linear invariant
subspace for each image based on its local structure, eliminating
the category-specific elements of the feature. Based
on the effectiveness of this representation, we then propose a
squared-loss mutual information based clustering model with
category distribution prior in each domain to infer the domain
assignment for images. In experiment, we test our approach
on two common image datasets, the results show that
our method outperforms the existing state-of-the-art methods,
and also show the superiority of multiple latent domain discovery latent model
local linear subspace
domain adaptation",latent domain model domain adapt improv robust signific mismatch sourc domain target domain aris chang illumin pose imag qualiti domain adapt increas popular comput vision method assum sourc data singl domain multidomain dataset provid domain label train instanc practic dataset mixtur multipl latent domain difficult manual provid domain label data point paper propos model automat discov latent domain visual dataset first assum visual imag sampl multipl manifold repres differ domain repres differ subspac use neighborhood structur estim imag belong categori approxim local linear invari subspac imag base local structur elimin categoryspecif element featur base effect represent propos squaredloss mutual inform base cluster model categori distribut prior domain infer domain assign imag experi test approach two common imag dataset result show method outperform exist stateoftheart method also show superior multipl latent domain discoveri latent model local linear subspac domain adapt,6,-16.241066,-12.717583
Improving Domain-independent Cloud-based Speech Recognition with Domain-dependent Phonetic Post-processing,"Johannes Twiefel, Timo Baumann, Stefan Heinrich and Stefan Wermter","AI and the Web (AIW)
Applications (APP)
NLP and Knowledge Representation (NLPKR)
Robotics (ROB)","speech recognition
phonetics
domain-dependent knowledge","AIW: Human language technologies for web systems, including text summarization and machine translation
APP: Intelligent User Interfaces
NLPKR: Natural Language Processing (General/Other)
ROB: Human-Robot Interaction","Automated speech recognition (ASR) technology has been developed to such a level that off-the-shelf distributed speech recognition services are available (free of cost) that allow researchers to integrate speech into their applications with little development effort or expert knowledge leading to better results compared with previously used open-source tools. Often, however, such services do not accept language models or grammars but process free speech from any domain. While results are very good given the enormous size of the search space, results frequently contain out-of-domain words or constructs that cannot be understood by subsequent domain-dependent natural language understanding (NLU) components. In this paper we present a versatile post-processing technique based on phonetic distance that integrates domain knowledge with open-domain ASR results, leading to improved ASR performance. Notably, our technique is able to make use of domain restrictions using various degrees of domain knowledge, ranging from pure vocabulary restrictions via grammars or N-grams to restrictions of the acceptable utterances. We present results for a variety of corpora (mainly from human-robot interaction) where our combined approach significantly outperforms Google ASR as well as a plain open-source ASR solution.","Improving Domain-independent Cloud-based Speech Recognition with Domain-dependent Phonetic Post-processing Automated speech recognition (ASR) technology has been developed to such a level that off-the-shelf distributed speech recognition services are available (free of cost) that allow researchers to integrate speech into their applications with little development effort or expert knowledge leading to better results compared with previously used open-source tools. Often, however, such services do not accept language models or grammars but process free speech from any domain. While results are very good given the enormous size of the search space, results frequently contain out-of-domain words or constructs that cannot be understood by subsequent domain-dependent natural language understanding (NLU) components. In this paper we present a versatile post-processing technique based on phonetic distance that integrates domain knowledge with open-domain ASR results, leading to improved ASR performance. Notably, our technique is able to make use of domain restrictions using various degrees of domain knowledge, ranging from pure vocabulary restrictions via grammars or N-grams to restrictions of the acceptable utterances. We present results for a variety of corpora (mainly from human-robot interaction) where our combined approach significantly outperforms Google ASR as well as a plain open-source ASR solution. speech recognition
phonetics
domain-dependent knowledge",improv domainindepend cloudbas speech recognit domaindepend phonet postprocess autom speech recognit asr technolog develop level offtheshelf distribut speech recognit servic avail free cost allow research integr speech applic littl develop effort expert knowledg lead better result compar previous use opensourc tool often howev servic accept languag model grammar process free speech domain result good given enorm size search space result frequent contain outofdomain word construct cannot understood subsequ domaindepend natur languag understand nlu compon paper present versatil postprocess techniqu base phonet distanc integr domain knowledg opendomain asr result lead improv asr perform notabl techniqu abl make use domain restrict use various degre domain knowledg rang pure vocabulari restrict via grammar ngram restrict accept utter present result varieti corpora main humanrobot interact combin approach signific outperform googl asr well plain opensourc asr solut speech recognit phonet domaindepend knowledg,6,3.1602638,-4.660812
A Strategy-Aware Technique for Learning Behaviors from Discrete Human Feedback,"Robert Loftin, James MacGlashan, Bei Peng, Michael Littman, Matthew E. Taylor, Jeff Huang and David Roberts","Humans and AI (HAI)
Novel Machine Learning Algorithms (NMLA)","Learning from Feedback
Human Computer Interaction
Reinforcement Learning
Canine Learning","APP: Philosophical and Ethical Issues
CM: Bayesian Learning
HCC: Active learning from imperfect human labelers
HAI: Human-Computer Interaction
NMLA: Bayesian Learning
NMLA: Reinforcement Learning
ROB: Human-Robot Interaction","This paper introduces two novel algorithms, SABL and I-SABL, for learning behaviors from human-provided rewards. The primary novelty of these algorithms is that instead of treating the feedback as a numeric reward signal, they interpret feedback as a form of discrete communication that depends on both the behavior the trainer is trying to teach and the teaching strategy used by the trainer.  For example, some humans use a strategy where the lack of feedback may indicate whether the action was correct or incorrect, and interpreting this lack of feedback accurately can significantly improve learning speed. Results from user studies show that 1) humans use a variety of training strategies in practice, and 2) both algorithms can successfully learn a contextual bandit task faster than approaches that treat the feedback as numeric. Additionally, simulated trainers are employed to evaluate the algorithms in both contextual bandit and sequential decision-making domains with similar results.","A Strategy-Aware Technique for Learning Behaviors from Discrete Human Feedback This paper introduces two novel algorithms, SABL and I-SABL, for learning behaviors from human-provided rewards. The primary novelty of these algorithms is that instead of treating the feedback as a numeric reward signal, they interpret feedback as a form of discrete communication that depends on both the behavior the trainer is trying to teach and the teaching strategy used by the trainer.  For example, some humans use a strategy where the lack of feedback may indicate whether the action was correct or incorrect, and interpreting this lack of feedback accurately can significantly improve learning speed. Results from user studies show that 1) humans use a variety of training strategies in practice, and 2) both algorithms can successfully learn a contextual bandit task faster than approaches that treat the feedback as numeric. Additionally, simulated trainers are employed to evaluate the algorithms in both contextual bandit and sequential decision-making domains with similar results. Learning from Feedback
Human Computer Interaction
Reinforcement Learning
Canine Learning",strategyawar techniqu learn behavior discret human feedback paper introduc two novel algorithm sabl isabl learn behavior humanprovid reward primari novelti algorithm instead treat feedback numer reward signal interpret feedback form discret communic depend behavior trainer tri teach teach strategi use trainer exampl human use strategi lack feedback may indic whether action correct incorrect interpret lack feedback accur signific improv learn speed result user studi show 1 human use varieti train strategi practic 2 algorithm success learn contextu bandit task faster approach treat feedback numer addit simul trainer employ evalu algorithm contextu bandit sequenti decisionmak domain similar result learn feedback human comput interact reinforc learn canin learn,4,-2.3484647,-11.150818
Learning Instance Concepts from Multiple-Instance Data with Bags as Distributions,Gary Doran and Soumya Ray,,"multiple-instance learning
supervised learning
classification","NMLA: Evaluation and Analysis (Machine Learning)
NMLA: Supervised Learning (Other)
NMLA: Machine Learning (General/other)","We analyze and evaluate a generative process for multiple-instance learning (MIL) in which bags are distributions over instances. We show that our generative process contains as special cases generative models explored in prior work, while excluding scenarios known to be hard for MIL. Further, under the mild assumption that every negative instance is observed with nonzero probability in some negative bag, we show that it is possible to learn concepts that accurately label instances from MI data in this setting. Finally, we show that standard supervised approaches can learn concepts with low area-under-ROC error from MI data in this setting. We validate this surprising result with experiments using several real-world MI datasets that have been annotated with instance labels.","Learning Instance Concepts from Multiple-Instance Data with Bags as Distributions We analyze and evaluate a generative process for multiple-instance learning (MIL) in which bags are distributions over instances. We show that our generative process contains as special cases generative models explored in prior work, while excluding scenarios known to be hard for MIL. Further, under the mild assumption that every negative instance is observed with nonzero probability in some negative bag, we show that it is possible to learn concepts that accurately label instances from MI data in this setting. Finally, we show that standard supervised approaches can learn concepts with low area-under-ROC error from MI data in this setting. We validate this surprising result with experiments using several real-world MI datasets that have been annotated with instance labels. multiple-instance learning
supervised learning
classification",learn instanc concept multipleinst data bag distribut analyz evalu generat process multipleinst learn mil bag distribut instanc show generat process contain special case generat model explor prior work exclud scenario known hard mil mild assumpt everi negat instanc observ nonzero probabl negat bag show possibl learn concept accur label instanc mi data set final show standard supervis approach learn concept low areaunderroc error mi data set valid surpris result experi use sever realworld mi dataset annot instanc label multipleinst learn supervis learn classif,6,-6.600633,-10.308534
Tractability through Exchangeability: A New Perspective on Efficient Probabilistic Inference,Mathias Niepert and Guy Van den Broeck,Reasoning under Uncertainty (RU),"efficient inference
lifted inference
probabilistic inference
exchangeability
statistical relational learning","RU: Probabilistic Inference
RU: Relational Probabilistic Models","Exchangeability is a central notion in statistics and probability theory. The assumption that an infinite sequence of data points is exchangeable is at the core of Bayesian statistics. However, finite exchangeability as a statistical property that renders probabilistic inference tractable is less well-understood. We develop a theory of finite exchangeability and its relation to tractable probabilistic inference. The theory is complementary to that of independence and conditional independence. We show that tractable inference in probabilistic models with high treewidth and millions of variables can be explained with the notion of finite (partial) exchangeability. We also show that existing lifted inference algorithms implicitly utilize a combination of conditional independence and partial exchangeability.","Tractability through Exchangeability: A New Perspective on Efficient Probabilistic Inference Exchangeability is a central notion in statistics and probability theory. The assumption that an infinite sequence of data points is exchangeable is at the core of Bayesian statistics. However, finite exchangeability as a statistical property that renders probabilistic inference tractable is less well-understood. We develop a theory of finite exchangeability and its relation to tractable probabilistic inference. The theory is complementary to that of independence and conditional independence. We show that tractable inference in probabilistic models with high treewidth and millions of variables can be explained with the notion of finite (partial) exchangeability. We also show that existing lifted inference algorithms implicitly utilize a combination of conditional independence and partial exchangeability. efficient inference
lifted inference
probabilistic inference
exchangeability
statistical relational learning",tractabl exchang new perspect effici probabilist infer exchang central notion statist probabl theori assumpt infinit sequenc data point exchang core bayesian statist howev finit exchang statist properti render probabilist infer tractabl less wellunderstood develop theori finit exchang relat tractabl probabilist infer theori complementari independ condit independ show tractabl infer probabilist model high treewidth million variabl explain notion finit partial exchang also show exist lift infer algorithm implicit util combin condit independ partial exchang effici infer lift infer probabilist infer exchang statist relat learn,9,17.3908,17.022043
Collaborative Models for Referring Expression Generation in Situated Dialogue,"Rui Fang, Malcolm Doering and Joyce Chai",NLP and Machine Learning (NLPML),"referring expression generation
collaborative models
situated dialogue","NLPML: Discourse and Dialogue
NLPML: Natural Language Processing (General/Other)","In situated dialogue with artificial agents (e.g., robots), although a human and an agent are co-present, the agent's representation and the human's representation of the shared environment are significantly mismatched. Because of this misalignment, previous work has shown that when the agent applies traditional approaches to generate referring expressions to describe target objects, the intended objects often cannot be correctly identified by the human. To address this problem, motivated by collaborative behaviors in human referential communication, we have developed two collaborative models - an episodic model and an installment model - for referring expression generation. In both models, instead of generating a single referring expression to describe a target object as in the previous work, it generates multiple small expressions that lead to the target object with a goal to minimize the collaborative effort. In particular, our installment model incorporates human feedback in a reinforcement learning framework to learn the optimal generation strategies. Our empirical results have shown that the episodic model and the installment model outperform previous non-collaborative models with an absolute gain of 6% and 21% respectively.","Collaborative Models for Referring Expression Generation in Situated Dialogue In situated dialogue with artificial agents (e.g., robots), although a human and an agent are co-present, the agent's representation and the human's representation of the shared environment are significantly mismatched. Because of this misalignment, previous work has shown that when the agent applies traditional approaches to generate referring expressions to describe target objects, the intended objects often cannot be correctly identified by the human. To address this problem, motivated by collaborative behaviors in human referential communication, we have developed two collaborative models - an episodic model and an installment model - for referring expression generation. In both models, instead of generating a single referring expression to describe a target object as in the previous work, it generates multiple small expressions that lead to the target object with a goal to minimize the collaborative effort. In particular, our installment model incorporates human feedback in a reinforcement learning framework to learn the optimal generation strategies. Our empirical results have shown that the episodic model and the installment model outperform previous non-collaborative models with an absolute gain of 6% and 21% respectively. referring expression generation
collaborative models
situated dialogue",collabor model refer express generat situat dialogu situat dialogu artifici agent eg robot although human agent copres agent represent human represent share environ signific mismatch misalign previous work shown agent appli tradit approach generat refer express describ target object intend object often cannot correct identifi human address problem motiv collabor behavior human referenti communic develop two collabor model episod model instal model refer express generat model instead generat singl refer express describ target object previous work generat multipl small express lead target object goal minim collabor effort particular instal model incorpor human feedback reinforc learn framework learn optim generat strategi empir result shown episod model instal model outperform previous noncollabor model absolut gain 6 21 respect refer express generat collabor model situat dialogu,0,3.3292527,11.367467
Efficient Optimization for Autonomous Manipulation of Natural Objects,"Abdeslam Boularias, J. Andrew Bagnell and Anthony Stentz","Machine Learning Applications (MLA)
Robotics (ROB)","Robotic grasping
Planning
Bayesian optimization
Gaussian Processes
Anytime optimization","MLA: Machine Learning Applications (General/other)
ROB: Behavior and Control
ROB: Robotics (General/Other)","Manipulating irregular natural objects, such as rocks, is an essential capability of robots operating in outdoor environments. Previous studies have shown that stable grasps for known, man-made, objects can usually be planned by using physics-based simulators. However, planning is an expensive process that requires simulation of hand and object trajectories in different configurations, and evaluating the outcome of each trajectory. This problem is particularly concerning when the objects are irregular and cluttered, because the space of stable grasps is significantly smaller, and more configurations need to be evaluated before finding a good one. We present a learning approach, based on template matching, for fast detection of a small initial set of potentially stable grasps in a cluttered scene, using depth features. The predicted best grasps are further optimized by fine-tunning the configuration of the hand in simulation. To reduce the computational cost of this last operation, we model the predicted outcomes of the grasps as a Gaussian Process, and use an entropy-search method in order to focus the optimization on regions where the best grasp configuration is most likely to be. This approach is tested on the challenging task of clearing piles of real, unknown, rock debris using an autonomous robot. Empirical results show a clear advantage of this approach.","Efficient Optimization for Autonomous Manipulation of Natural Objects Manipulating irregular natural objects, such as rocks, is an essential capability of robots operating in outdoor environments. Previous studies have shown that stable grasps for known, man-made, objects can usually be planned by using physics-based simulators. However, planning is an expensive process that requires simulation of hand and object trajectories in different configurations, and evaluating the outcome of each trajectory. This problem is particularly concerning when the objects are irregular and cluttered, because the space of stable grasps is significantly smaller, and more configurations need to be evaluated before finding a good one. We present a learning approach, based on template matching, for fast detection of a small initial set of potentially stable grasps in a cluttered scene, using depth features. The predicted best grasps are further optimized by fine-tunning the configuration of the hand in simulation. To reduce the computational cost of this last operation, we model the predicted outcomes of the grasps as a Gaussian Process, and use an entropy-search method in order to focus the optimization on regions where the best grasp configuration is most likely to be. This approach is tested on the challenging task of clearing piles of real, unknown, rock debris using an autonomous robot. Empirical results show a clear advantage of this approach. Robotic grasping
Planning
Bayesian optimization
Gaussian Processes
Anytime optimization",effici optim autonom manipul natur object manipul irregular natur object rock essenti capabl robot oper outdoor environ previous studi shown stabl grasp known manmad object usual plan use physicsbas simul howev plan expens process requir simul hand object trajectori differ configur evalu outcom trajectori problem particular concern object irregular clutter space stabl grasp signific smaller configur need evalu find good one present learn approach base templat match fast detect small initi set potenti stabl grasp clutter scene use depth featur predict best grasp optim finetun configur hand simul reduc comput cost last oper model predict outcom grasp gaussian process use entropysearch method order focus optim region best grasp configur like approach test challeng task clear pile real unknown rock debri use autonom robot empir result show clear advantag approach robot grasp plan bayesian optim gaussian process anytim optim,4,0.5899725,9.456446
Effective Management of Electric Vehicle Storage using Smart Charging,"Konstantina Valogianni, Wolfgang Ketter, John Collins and Dmitry Zhdanov",Computational Sustainability and AI (CSAI),"Electric Vehicles
Smart Grid
Optimization
Reinforcement Learning",CSAI: Control and optimization of dynamic and spatiotemporal systems,"The growing Electric Vehicles' (EVs) popularity among commuters creates new challenges for the smart grid. The most important of them is the uncoordinated EV charging that substantially increases the energy demand peaks, putting the smart grid under constant strain. In order to cope with these peaks the grid needs extra infrastructure, a costly solution. We propose an Adaptive Management of EV Storage (AMEVS) algorithm, implemented through a learning agent that acts on behalf of individual EV owners and schedules EV charging over a weekly horizon. It accounts for individual preferences so that mobility service is not violated but also individual benefit is maximized. We observe that it reshapes the energy demand making it less volatile so that fewer resources are needed to cover peaks. It assumes Vehicle-to-Grid discharging when the customer has excess capacity. Our agent uses Reinforcement Learning trained on real world data to learn individual household consumption behavior and to schedule EV charging. Unlike previous work, AMEVS is a fully distributed approach. We show that AMEVS achieves significant reshaping of the energy demand curve and peak reduction, which is correlated with customer preferences regarding perceived utility of energy availability. Additionally, we show that the average and peak energy prices are reduced as a result of smarter energy use.","Effective Management of Electric Vehicle Storage using Smart Charging The growing Electric Vehicles' (EVs) popularity among commuters creates new challenges for the smart grid. The most important of them is the uncoordinated EV charging that substantially increases the energy demand peaks, putting the smart grid under constant strain. In order to cope with these peaks the grid needs extra infrastructure, a costly solution. We propose an Adaptive Management of EV Storage (AMEVS) algorithm, implemented through a learning agent that acts on behalf of individual EV owners and schedules EV charging over a weekly horizon. It accounts for individual preferences so that mobility service is not violated but also individual benefit is maximized. We observe that it reshapes the energy demand making it less volatile so that fewer resources are needed to cover peaks. It assumes Vehicle-to-Grid discharging when the customer has excess capacity. Our agent uses Reinforcement Learning trained on real world data to learn individual household consumption behavior and to schedule EV charging. Unlike previous work, AMEVS is a fully distributed approach. We show that AMEVS achieves significant reshaping of the energy demand curve and peak reduction, which is correlated with customer preferences regarding perceived utility of energy availability. Additionally, we show that the average and peak energy prices are reduced as a result of smarter energy use. Electric Vehicles
Smart Grid
Optimization
Reinforcement Learning",effect manag electr vehicl storag use smart charg grow electr vehicl ev popular among commut creat new challeng smart grid import uncoordin ev charg substanti increas energi demand peak put smart grid constant strain order cope peak grid need extra infrastructur cost solut propos adapt manag ev storag amev algorithm implement learn agent act behalf individu ev owner schedul ev charg week horizon account individu prefer mobil servic violat also individu benefit maxim observ reshap energi demand make less volatil fewer resourc need cover peak assum vehicletogrid discharg custom excess capac agent use reinforc learn train real world data learn individu household consumpt behavior schedul ev charg unlik previous work amev fulli distribut approach show amev achiev signific reshap energi demand curv peak reduct correl custom prefer regard perceiv util energi avail addit show averag peak energi price reduc result smarter energi use electr vehicl smart grid optim reinforc learn,5,7.140934,10.9843445
Social Planning: Achieving Goals by Altering Others' Mental States,"Chris Pearce, Ben Meadows, Pat Langley and Mike Barley","Cognitive Systems (CS)
Planning and Scheduling (PS)","Cognitive Systems
Social Planning
Deception","CS: Conceptual inference and reasoning
CS: Social cognition and interaction
CS: Problem solving and decision making
KRR: Reasoning with Beliefs
PS: Planning (General/Other)","In this paper, we discuss a computational approach to the cognitive 
task of social planning. First, we specify a class of planning 
problems that involve an agent who attempts to achieve its goals 
by altering other agents' mental states. Next, we describe SFPS, 
a flexible problem solver that generates social plans of this sort, 
including ones that include deception and reasoning about other 
agents' beliefs. We report the results for experiments on social 
scenarios that involve different levels of sophistication and that 
demonstrate both SFPS' capabilities and the sources of its power. 
Finally, we discuss how our approach to social planning has been 
informed by earlier work in the area and propose directions for 
additional research on the topic.","Social Planning: Achieving Goals by Altering Others' Mental States In this paper, we discuss a computational approach to the cognitive 
task of social planning. First, we specify a class of planning 
problems that involve an agent who attempts to achieve its goals 
by altering other agents' mental states. Next, we describe SFPS, 
a flexible problem solver that generates social plans of this sort, 
including ones that include deception and reasoning about other 
agents' beliefs. We report the results for experiments on social 
scenarios that involve different levels of sophistication and that 
demonstrate both SFPS' capabilities and the sources of its power. 
Finally, we discuss how our approach to social planning has been 
informed by earlier work in the area and propose directions for 
additional research on the topic. Cognitive Systems
Social Planning
Deception",social plan achiev goal alter other mental state paper discuss comput approach cognit task social plan first specifi class plan problem involv agent attempt achiev goal alter agent mental state next describ sfps flexibl problem solver generat social plan sort includ one includ decept reason agent belief report result experi social scenario involv differ level sophist demonstr sfps capabl sourc power final discuss approach social plan inform earlier work area propos direct addit research topic cognit system social plan decept,0,-1.8072017,18.372976
Feature-Cost Sensitive Learning with Submodular Trees of Classifiers,"Matt Kusner, Wenlin Chen, Quan Zhou, Eddie Xu and Kilian Weinberger",Novel Machine Learning Algorithms (NMLA),"submodular optimization
feature-cost sensitive learning
tree-based learning","NMLA: Classification
NMLA: Supervised Learning (Other)","During the past decade, machine learning algorithms have become commonplace in large-scale real-world industrial applications. In these settings, the computation time to train and test machine learning algorithms is a key consideration. At training-time the algorithms must scale to very large data set sizes. At testing-time, the cost of feature extraction can dominate the CPU runtime. Recently, a promising method was proposed to account for the feature extraction cost at testing time, called Cost-sensitive Tree of Classifiers (CSTC). Although the CSTC problem is NP-hard, the authors suggest an approximation through a mixed-norm relaxation across many classifiers. This relaxation is slow to train and requires involved optimization hyperparameter tuning. We propose a different relaxation using approximate submodularity, called Approximately Submodular Tree of Classifiers (ASTC). ASTC is much simpler to implement, yields equivalent results but requires no optimization hyperparameter tuning and is up to two orders of magnitude faster to train.","Feature-Cost Sensitive Learning with Submodular Trees of Classifiers During the past decade, machine learning algorithms have become commonplace in large-scale real-world industrial applications. In these settings, the computation time to train and test machine learning algorithms is a key consideration. At training-time the algorithms must scale to very large data set sizes. At testing-time, the cost of feature extraction can dominate the CPU runtime. Recently, a promising method was proposed to account for the feature extraction cost at testing time, called Cost-sensitive Tree of Classifiers (CSTC). Although the CSTC problem is NP-hard, the authors suggest an approximation through a mixed-norm relaxation across many classifiers. This relaxation is slow to train and requires involved optimization hyperparameter tuning. We propose a different relaxation using approximate submodularity, called Approximately Submodular Tree of Classifiers (ASTC). ASTC is much simpler to implement, yields equivalent results but requires no optimization hyperparameter tuning and is up to two orders of magnitude faster to train. submodular optimization
feature-cost sensitive learning
tree-based learning",featurecost sensit learn submodular tree classifi past decad machin learn algorithm becom commonplac largescal realworld industri applic set comput time train test machin learn algorithm key consider trainingtim algorithm must scale larg data set size testingtim cost featur extract domin cpu runtim recent promis method propos account featur extract cost test time call costsensit tree classifi cstc although cstc problem nphard author suggest approxim mixednorm relax across mani classifi relax slow train requir involv optim hyperparamet tune propos differ relax use approxim submodular call approxim submodular tree classifi astc astc much simpler implement yield equival result requir optim hyperparamet tune two order magnitud faster train submodular optim featurecost sensit learn treebas learn,4,0.730728,-9.09952
Multiagent Metareasoning Through Organizational Design,Jason Sleight and Ed Durfee,"Multiagent Systems (MAS)
Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","organizational design
Dec-MDP
multiagent metareasoning","MAS: Coordination and Collaboration
MAS: Multiagent Planning
MAS: Multiagent Systems (General/other)
PS: Markov Models of Environments
PS: Model-Based Reasoning
PS: Probabilistic Planning
RU: Decision/Utility Theory
RU: Sequential Decision Making","We formulate an approach to multiagent metareasoning that uses organizational design to focus each agent's reasoning on the aspects of their respective local problems to which they can make the most worthwhile contributions to joint behavior.  By employing the decentralized Markov decision process framework, we characterize an organizational design problem that explicitly considers the quantitative impact that a design has on both the quality of the agents' behaviors and their reasoning costs.  We describe an automated organizational design process that can approximately solve our organizational design problem via incremental search, and present techniques that efficiently estimate the incremental impact of a candidate organizational influence.  Our empirical evaluation confirms that our process generates organizational designs that impart a desired metareasoning regime upon the agents.","Multiagent Metareasoning Through Organizational Design We formulate an approach to multiagent metareasoning that uses organizational design to focus each agent's reasoning on the aspects of their respective local problems to which they can make the most worthwhile contributions to joint behavior.  By employing the decentralized Markov decision process framework, we characterize an organizational design problem that explicitly considers the quantitative impact that a design has on both the quality of the agents' behaviors and their reasoning costs.  We describe an automated organizational design process that can approximately solve our organizational design problem via incremental search, and present techniques that efficiently estimate the incremental impact of a candidate organizational influence.  Our empirical evaluation confirms that our process generates organizational designs that impart a desired metareasoning regime upon the agents. organizational design
Dec-MDP
multiagent metareasoning",multiag metareason organiz design formul approach multiag metareason use organiz design focus agent reason aspect respect local problem make worthwhil contribut joint behavior employ decentr markov decis process framework character organiz design problem explicit consid quantit impact design qualiti agent behavior reason cost describ autom organiz design process approxim solv organiz design problem via increment search present techniqu effici estim increment impact candid organiz influenc empir evalu confirm process generat organiz design impart desir metareason regim upon agent organiz design decmdp multiag metareason,5,3.8499157,13.020507
Experiments on Visual Information Extraction with the Faces of Wikipedia,Md. Kamrul Hasan and Christopher Pal,"AI and the Web (AIW)
Vision (VIS)","Web mining
Information extraction
Text processing
Face verification
Identity resolution
Face recognition","AIW: Knowledge acquisition from the web
VIS: Face and Gesture Recognition
VIS: Image and Video Retrieval
VIS: Language and Vision","We present a series of visual information extraction experiments
using the Faces ofWikipedia database - a new resource
that we release into the public domain for both recognition
and extraction research containing over 50,000 identities and
60,000 disambiguated images of faces. We compare different
techniques for automatically extracting the faces corresponding
to the subject of a Wikipedia biography within the
images appearing on the page. Our top performing approach
is based on probabilistic graphical models and uses the text
of Wikipedia pages, similarities of faces as well as various
other features of the document, meta-data and image files.
Our method resolves the problem jointly for all detected faces
on a page. While our experiments focus on extracting faces
from Wikipedia biographies, our approach is easily adapted
to other types of documents and multiple documents. We focus
onWikipedia because the content is a Creative Commons
resource and we provide our database to the community including
registered faces, hand labeled and automated disambiguations,
processed captions, meta data and evaluation protocols.
Our best probabilistic extraction pipeline yields an expected
average accuracy of 77% compared to image only and
text only baselines which yield 66% and 63% respectively.","Experiments on Visual Information Extraction with the Faces of Wikipedia We present a series of visual information extraction experiments
using the Faces ofWikipedia database - a new resource
that we release into the public domain for both recognition
and extraction research containing over 50,000 identities and
60,000 disambiguated images of faces. We compare different
techniques for automatically extracting the faces corresponding
to the subject of a Wikipedia biography within the
images appearing on the page. Our top performing approach
is based on probabilistic graphical models and uses the text
of Wikipedia pages, similarities of faces as well as various
other features of the document, meta-data and image files.
Our method resolves the problem jointly for all detected faces
on a page. While our experiments focus on extracting faces
from Wikipedia biographies, our approach is easily adapted
to other types of documents and multiple documents. We focus
onWikipedia because the content is a Creative Commons
resource and we provide our database to the community including
registered faces, hand labeled and automated disambiguations,
processed captions, meta data and evaluation protocols.
Our best probabilistic extraction pipeline yields an expected
average accuracy of 77% compared to image only and
text only baselines which yield 66% and 63% respectively. Web mining
Information extraction
Text processing
Face verification
Identity resolution
Face recognition",experi visual inform extract face wikipedia present seri visual inform extract experi use face ofwikipedia databas new resourc releas public domain recognit extract research contain 50000 ident 60000 disambigu imag face compar differ techniqu automat extract face correspond subject wikipedia biographi within imag appear page top perform approach base probabilist graphic model use text wikipedia page similar face well various featur document metadata imag file method resolv problem joint detect face page experi focus extract face wikipedia biographi approach easili adapt type document multipl document focus onwikipedia content creativ common resourc provid databas communiti includ regist face hand label autom disambigu process caption meta data evalu protocol best probabilist extract pipelin yield expect averag accuraci 77 compar imag text baselin yield 66 63 respect web mine inform extract text process face verif ident resolut face recognit,1,3.4866254,-6.7263374
Signals in the Silence: Models of Implicit Feedback in a Recommender System for Crowdsourcing,"Christopher Lin, Ece Kamar and Eric Horvitz","AI and the Web (AIW)
Applications (APP)
Human-Computation and Crowd Sourcing (HCC)
Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Crowdsourcing
Recommendation Systems
Implicit Feedback
Matrix Factorization","AIW: Crowdsourcing techniques and methodologies
AIW: Web-based recommendation systems
APP: Other Applications
HCC: Programming languages, tools and platforms to support human computation
MLA: Machine Learning Applications (General/other)
NMLA: Recommender Systems","We study the opportunity to exploit the absence of signals as informative observations in the context of providing task recommendations in crowdsourcing. Workers on crowdsourcing platform do not provide explicit ratings about tasks. We present methods that enable a system to leverage implicit signals about task preferences. These signals include types of tasks that have been available and have been displayed, and the number of tasks workers select and complete. In distinction to previous work, we present a general model that can represent both positive and negative implicit signals.  We introduce algorithms that can learn these models without exceeding the computational complexity of existing approaches. Finally, using data from a large-scale, high throughput crowdsourcing platform, we show that reasoning about both positive and negative implicit feedback can improve the quality of task recommendations provided to workers.","Signals in the Silence: Models of Implicit Feedback in a Recommender System for Crowdsourcing We study the opportunity to exploit the absence of signals as informative observations in the context of providing task recommendations in crowdsourcing. Workers on crowdsourcing platform do not provide explicit ratings about tasks. We present methods that enable a system to leverage implicit signals about task preferences. These signals include types of tasks that have been available and have been displayed, and the number of tasks workers select and complete. In distinction to previous work, we present a general model that can represent both positive and negative implicit signals.  We introduce algorithms that can learn these models without exceeding the computational complexity of existing approaches. Finally, using data from a large-scale, high throughput crowdsourcing platform, we show that reasoning about both positive and negative implicit feedback can improve the quality of task recommendations provided to workers. Crowdsourcing
Recommendation Systems
Implicit Feedback
Matrix Factorization",signal silenc model implicit feedback recommend system crowdsourc studi opportun exploit absenc signal inform observ context provid task recommend crowdsourc worker crowdsourc platform provid explicit rate task present method enabl system leverag implicit signal task prefer signal includ type task avail display number task worker select complet distinct previous work present general model repres posit negat implicit signal introduc algorithm learn model without exceed comput complex exist approach final use data largescal high throughput crowdsourc platform show reason posit negat implicit feedback improv qualiti task recommend provid worker crowdsourc recommend system implicit feedback matrix factor,0,17.813179,8.009918
A Convex Formulation for Semi-supervised Multi-Label Feature Selection,"Xiaojun Chang, Feiping Nie, Yi Yang and Heng Huang",Machine Learning Applications (MLA),"Semi-supervised Learning
Multi-Label Feature Selection
Convex Algorithm","NMLA: Classification
NMLA: Dimension Reduction/Feature Selection","Explosive growth of multimedia data has brought challenge of how to efficiently browse, retrieve and organize these data. Under this circumstance, different approaches have been proposed to facilitate multimedia analysis. Several semi-supervised feature selection algorithms have been proposed to exploit both labeled and unlabeled data. However, they are implemented based on graphs, such that they cannot handle large-scale datasets. How to conduct semi-supervised feature selection on large-scale datasets has become a challenging research problem. Moreover, existing multi-label feature selection algorithms rely on eigen-decomposition with heavy computational burden, which further prevent current feature selection algorithms from being applied for big data. In this paper, we propose a novel semi-supervised multi-label feature selection for large-scale
multimedia analysis. We evaluate performance of the proposed algorithm over five benchmark datasets and compare the results with state-of-the-art supervised and
semi-supervised feature selection algorithms as well as baseline using all features. The experimental results demonstrate that our proposed algorithm consistently achieve superiors performances.","A Convex Formulation for Semi-supervised Multi-Label Feature Selection Explosive growth of multimedia data has brought challenge of how to efficiently browse, retrieve and organize these data. Under this circumstance, different approaches have been proposed to facilitate multimedia analysis. Several semi-supervised feature selection algorithms have been proposed to exploit both labeled and unlabeled data. However, they are implemented based on graphs, such that they cannot handle large-scale datasets. How to conduct semi-supervised feature selection on large-scale datasets has become a challenging research problem. Moreover, existing multi-label feature selection algorithms rely on eigen-decomposition with heavy computational burden, which further prevent current feature selection algorithms from being applied for big data. In this paper, we propose a novel semi-supervised multi-label feature selection for large-scale
multimedia analysis. We evaluate performance of the proposed algorithm over five benchmark datasets and compare the results with state-of-the-art supervised and
semi-supervised feature selection algorithms as well as baseline using all features. The experimental results demonstrate that our proposed algorithm consistently achieve superiors performances. Semi-supervised Learning
Multi-Label Feature Selection
Convex Algorithm",convex formul semisupervis multilabel featur select explos growth multimedia data brought challeng effici brows retriev organ data circumst differ approach propos facilit multimedia analysi sever semisupervis featur select algorithm propos exploit label unlabel data howev implement base graph cannot handl largescal dataset conduct semisupervis featur select largescal dataset becom challeng research problem moreov exist multilabel featur select algorithm reli eigendecomposit heavi comput burden prevent current featur select algorithm appli big data paper propos novel semisupervis multilabel featur select largescal multimedia analysi evalu perform propos algorithm five benchmark dataset compar result stateoftheart supervis semisupervis featur select algorithm well baselin use featur experiment result demonstr propos algorithm consist achiev superior perform semisupervis learn multilabel featur select convex algorithm,6,-8.238681,-20.749956
A Spatially Sensitive Kernel to Predict Cognitive Performance from Short-Term Changes in Neural Structure,"Hidayath Ansari, Michael Coen, Barbara Bendlin, Mark Sager and Sterling Johnson",Machine Learning Applications (MLA),"Machine Learning
Neuroimaging
Kernel Methods
Wide Data
Alzheimer's Disease","APP: Biomedical / Bioinformatics
MLA: Bio/Medicine
MLA: Applications of Supervised Learning","This paper introduces a novel framework for performing machine learning on longitudinal neuroimaging datasets. These datasets are characterized by their size, particularly their width (millions of features per input). 

Specifically, we address the problem of detecting subtle, short-term changes in neural structure that are indicative of cognitive decline and correlate with risk factors for Alzheimer's disease. We introduce a new spatially-sensitive kernel that allows us to reason about individuals, as opposed to populations. 

In doing so, this paper presents the first evidence demonstrating that very small changes in white matter structure over a two year period can predict change in cognitive function in healthy adults.","A Spatially Sensitive Kernel to Predict Cognitive Performance from Short-Term Changes in Neural Structure This paper introduces a novel framework for performing machine learning on longitudinal neuroimaging datasets. These datasets are characterized by their size, particularly their width (millions of features per input). 

Specifically, we address the problem of detecting subtle, short-term changes in neural structure that are indicative of cognitive decline and correlate with risk factors for Alzheimer's disease. We introduce a new spatially-sensitive kernel that allows us to reason about individuals, as opposed to populations. 

In doing so, this paper presents the first evidence demonstrating that very small changes in white matter structure over a two year period can predict change in cognitive function in healthy adults. Machine Learning
Neuroimaging
Kernel Methods
Wide Data
Alzheimer's Disease",spatial sensit kernel predict cognit perform shortterm chang neural structur paper introduc novel framework perform machin learn longitudin neuroimag dataset dataset character size particular width million featur per input specif address problem detect subtl shortterm chang neural structur indic cognit declin correl risk factor alzheim diseas introduc new spatiallysensit kernel allow us reason individu oppos popul paper present first evid demonstr small chang white matter structur two year period predict chang cognit function healthi adult machin learn neuroimag kernel method wide data alzheim diseas,1,-4.9297047,-8.412802
GP-Localize: Persistent Mobile Robot Localization using Online Sparse Gaussian Process Observation Model,"Nuo Xu, Bryan Kian Hsiang Low, Jie Chen, Keng Kiat Lim and Etkin Ozgul",Robotics (ROB),"Robot localization
Gaussian process
Online learning","MLA: Applications of Supervised Learning
NMLA: Online Learning
ROB: Localization, Mapping, and Navigation
ROB: State Estimation","Central to robot exploration and mapping is the task of persistent localization in environmental fields characterized by spatially correlated measurements.
This paper presents a novel Gaussian process localization (GP-Localize) algorithm that, in contrast to existing works, can exploit the spatially correlated field measurements taken during a robot's exploration (instead of relying on prior training data) for efficiently and scalably learning the GP observation model online.
As a result, GP-Localize is capable of achieving constant time and memory in the size of the data per filtering step, which demonstrates the practical feasibility of using GPs for persistent robot localization.
Empirical evaluation via simulated experiments with real-world datasets and a real robot experiment shows that GP-Localize outperforms existing GP localization algorithms.","GP-Localize: Persistent Mobile Robot Localization using Online Sparse Gaussian Process Observation Model Central to robot exploration and mapping is the task of persistent localization in environmental fields characterized by spatially correlated measurements.
This paper presents a novel Gaussian process localization (GP-Localize) algorithm that, in contrast to existing works, can exploit the spatially correlated field measurements taken during a robot's exploration (instead of relying on prior training data) for efficiently and scalably learning the GP observation model online.
As a result, GP-Localize is capable of achieving constant time and memory in the size of the data per filtering step, which demonstrates the practical feasibility of using GPs for persistent robot localization.
Empirical evaluation via simulated experiments with real-world datasets and a real robot experiment shows that GP-Localize outperforms existing GP localization algorithms. Robot localization
Gaussian process
Online learning",gplocal persist mobil robot local use onlin spars gaussian process observ model central robot explor map task persist local environment field character spatial correl measur paper present novel gaussian process local gplocal algorithm contrast exist work exploit spatial correl field measur taken robot explor instead reli prior train data effici scalabl learn gp observ model onlin result gplocal capabl achiev constant time memori size data per filter step demonstr practic feasibl use gps persist robot local empir evalu via simul experi realworld dataset real robot experi show gplocal outperform exist gp local algorithm robot local gaussian process onlin learn,4,0.31998804,8.3386545
On Detecting Nearly Structured Preference Profiles,Martin Lackner and Edith Elkind,"Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","single-peaked preferences
single-crossing preferences
approximation algorithms
forbidden configurations",GTEP: Social Choice / Voting,"Structured preference domains, such as, e.g., the domains of single-peaked and single-crossing preferences, are known to admit efficient algorithms for many problems in computational social choice. Some of these algorithms extend to preferences that are close to having the respective structural property, i.e., can be made to enjoy this property by making minor changes to voters’ preferences, such as deleting a small number of voters or candidates. However, it has recently been shown that finding the optimal number of voters or candidates to delete in order to achieve the desired structural property is NP-hard for many such domains. In this paper, we show that these problems admit efficient approximation algorithms. Our results apply to all domains that can be characterized in terms of forbidden configurations; this includes, in particular, single-peaked and single-crossing elections. For a large range of scenarios, our approximation results are optimal under a plausible complexity-theoretic assumption. We also provide parameterized complexity results for this class of problems.","On Detecting Nearly Structured Preference Profiles Structured preference domains, such as, e.g., the domains of single-peaked and single-crossing preferences, are known to admit efficient algorithms for many problems in computational social choice. Some of these algorithms extend to preferences that are close to having the respective structural property, i.e., can be made to enjoy this property by making minor changes to voters’ preferences, such as deleting a small number of voters or candidates. However, it has recently been shown that finding the optimal number of voters or candidates to delete in order to achieve the desired structural property is NP-hard for many such domains. In this paper, we show that these problems admit efficient approximation algorithms. Our results apply to all domains that can be characterized in terms of forbidden configurations; this includes, in particular, single-peaked and single-crossing elections. For a large range of scenarios, our approximation results are optimal under a plausible complexity-theoretic assumption. We also provide parameterized complexity results for this class of problems. single-peaked preferences
single-crossing preferences
approximation algorithms
forbidden configurations",detect near structur prefer profil structur prefer domain eg domain singlepeak singlecross prefer known admit effici algorithm mani problem comput social choic algorithm extend prefer close respect structur properti ie made enjoy properti make minor chang voter prefer delet small number voter candid howev recent shown find optim number voter candid delet order achiev desir structur properti nphard mani domain paper show problem admit effici approxim algorithm result appli domain character term forbidden configur includ particular singlepeak singlecross elect larg rang scenario approxim result optim plausibl complexitytheoret assumpt also provid parameter complex result class problem singlepeak prefer singlecross prefer approxim algorithm forbidden configur,9,19.521196,-2.249122
Relational One-Class Classification: A Non-Parametric Approach,"Tushar Khot, Sriraam Natarajan and Jude Shavlik","Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)","One-class classification
Statistical Relational Learning
Ensemble learning","NMLA: Ensemble Methods
NMLA: Relational/Graph-Based Learning
RU: Relational Probabilistic Models",One-class classification approaches have been proposed in literature to learn classifiers from examples of only one class. But these approaches are not directly applicable to relational domains due to their reliance on feature vectors or  distance measure. We propose a non-parametric relational one-class classification approach based on first-order trees. We learn a tree-based distance measure that iteratively introduces new relational features to differentiate relational examples. We update the distance measure so as to maximize the one-class classification performance of our model. We also relate our model definition to existing work on combination functions and density estimation. We also experimentally show that our approach can discover relevant features for this task and outperform three  baseline approaches.,"Relational One-Class Classification: A Non-Parametric Approach One-class classification approaches have been proposed in literature to learn classifiers from examples of only one class. But these approaches are not directly applicable to relational domains due to their reliance on feature vectors or  distance measure. We propose a non-parametric relational one-class classification approach based on first-order trees. We learn a tree-based distance measure that iteratively introduces new relational features to differentiate relational examples. We update the distance measure so as to maximize the one-class classification performance of our model. We also relate our model definition to existing work on combination functions and density estimation. We also experimentally show that our approach can discover relevant features for this task and outperform three  baseline approaches. One-class classification
Statistical Relational Learning
Ensemble learning",relat oneclass classif nonparametr approach oneclass classif approach propos literatur learn classifi exampl one class approach direct applic relat domain due relianc featur vector distanc measur propos nonparametr relat oneclass classif approach base firstord tree learn treebas distanc measur iter introduc new relat featur differenti relat exampl updat distanc measur maxim oneclass classif perform model also relat model definit exist work combin function densiti estim also experiment show approach discov relev featur task outperform three baselin approach oneclass classif statist relat learn ensembl learn,6,-5.171476,-4.530334
Using Narrative Function to Extract Qualitative Information from Natural Language Texts,"Clifton McFate, Kenneth Forbus and Thomas Hinrichs",Cognitive Systems (CS),"Cognitive Systems
Qualitative Representation
Natural Language Understanding",CS: Natural language understanding and dialogue,"Understanding natural language about the continuous world is an important problem for cognitive systems. The naturalness of qualitative reasoning suggests that qualitative representations might be an important component of the semantics of natural language.  Prior work showed that frame-based representations of qualitative process theory constructs could indeed be extracted from natural language texts. That technique relied on the parser recognizing specific syntactic constructions, which had limited coverage. This paper describes a new approach, using narrative function to represent the higher-order relationships between the constituents of a sentence and between sentences in a discourse.  We outline how narrative function combined with query-driven abduction enables the same kinds of information to be extracted from natural language texts.  Moreover, we also show how the same technique can be used to extract type-level qualitative representations from text, and used to improve performance in playing a strategy game.","Using Narrative Function to Extract Qualitative Information from Natural Language Texts Understanding natural language about the continuous world is an important problem for cognitive systems. The naturalness of qualitative reasoning suggests that qualitative representations might be an important component of the semantics of natural language.  Prior work showed that frame-based representations of qualitative process theory constructs could indeed be extracted from natural language texts. That technique relied on the parser recognizing specific syntactic constructions, which had limited coverage. This paper describes a new approach, using narrative function to represent the higher-order relationships between the constituents of a sentence and between sentences in a discourse.  We outline how narrative function combined with query-driven abduction enables the same kinds of information to be extracted from natural language texts.  Moreover, we also show how the same technique can be used to extract type-level qualitative representations from text, and used to improve performance in playing a strategy game. Cognitive Systems
Qualitative Representation
Natural Language Understanding",use narrat function extract qualit inform natur languag text understand natur languag continu world import problem cognit system natur qualit reason suggest qualit represent might import compon semant natur languag prior work show framebas represent qualit process theori construct could inde extract natur languag text techniqu reli parser recogn specif syntact construct limit coverag paper describ new approach use narrat function repres higherord relationship constitu sentenc sentenc discours outlin narrat function combin querydriven abduct enabl kind inform extract natur languag text moreov also show techniqu use extract typelevel qualit represent text use improv perform play strategi game cognit system qualit represent natur languag understand,1,-5.2277713,12.324916
A spatio-temporal pattern mining algorithm to identify objects in a continuous field: A global oceanography perspective,"James Faghmous, Hung Nguyen, Matthew Le and Vipin Kumar",Computational Sustainability and AI (CSAI),"spatio-temporal data mining
oceanography
pattern mining","CSAI: Modeling and prediction of dynamic and spatiotemporal phenomena and systems
CSAI: Control and optimization of dynamic and spatiotemporal systems
CSAI: Modeling and control of complex high-dimensional systems
CSAI: Sensor networks for monitoring environments
NMLA: Data Mining and Knowledge Discovery
NMLA: Time-Series/Data Streams
NMLA: Unsupervised Learning (Other)","Mesoscale ocean eddies are a critical component of the Earth System as they dominate the ocean's kinetic energy and impact the global distribution of oceanic heat, salinity, momentum, and nutrients. Thus, accurately representing these dynamic features is critical for our planet's sustainability. The majority of methods that identify eddies from satellite observations analyze the data in a frame-by-frame basis despite the fact that eddies are dynamic objects that propagate across space and time. We introduce the notion of spatio-temporal consistency to identify eddies in a continuous spatio-temporal field, to simultaneously ensure that the features detected are both spatially consistent and temporally persistent. Our spatio-temporal consistency approach allows us to remove most of the expert criteria used in traditional methods and enables us to better render eddy dynamics by identifying smaller and longer lived eddies than existing methods.","A spatio-temporal pattern mining algorithm to identify objects in a continuous field: A global oceanography perspective Mesoscale ocean eddies are a critical component of the Earth System as they dominate the ocean's kinetic energy and impact the global distribution of oceanic heat, salinity, momentum, and nutrients. Thus, accurately representing these dynamic features is critical for our planet's sustainability. The majority of methods that identify eddies from satellite observations analyze the data in a frame-by-frame basis despite the fact that eddies are dynamic objects that propagate across space and time. We introduce the notion of spatio-temporal consistency to identify eddies in a continuous spatio-temporal field, to simultaneously ensure that the features detected are both spatially consistent and temporally persistent. Our spatio-temporal consistency approach allows us to remove most of the expert criteria used in traditional methods and enables us to better render eddy dynamics by identifying smaller and longer lived eddies than existing methods. spatio-temporal data mining
oceanography
pattern mining",spatiotempor pattern mine algorithm identifi object continu field global oceanographi perspect mesoscal ocean eddi critic compon earth system domin ocean kinet energi impact global distribut ocean heat salin momentum nutrient thus accur repres dynam featur critic planet sustain major method identifi eddi satellit observ analyz data framebyfram basi despit fact eddi dynam object propag across space time introduc notion spatiotempor consist identifi eddi continu spatiotempor field simultan ensur featur detect spatial consist tempor persist spatiotempor consist approach allow us remov expert criteria use tradit method enabl us better render eddi dynam identifi smaller longer live eddi exist method spatiotempor data mine oceanographi pattern mine,3,-2.04523,-0.9470049
Efficient codes for inverse dynamics during walking,Leif Johnson and Dana Ballard,Cognitive Modeling (CM),"machine learning
inverse dynamics
movement",CM: Simulating Humans,"Efficient codes have been used effectively in both computer science and neuroscience to better understand the information processing in visual and auditory encoding and discrimination tasks. In this paper, we explore the use of efficient codes for representing information relevant to human movements during locomotion. Specifically, we apply motion capture data to a physical model of the human skeleton to compute joint angles (inverse kinematics) and joint torques (inverse dynamics); then, by treating the resulting data as a regression problem, we investigate the effect of sparsity in mapping from angles to torques. The results of our investigation suggest that sparse codes can indeed represent salient features of both the kinematic and dynamic views of locomotion movements in humans. However, sparsity appears to be only one parameter in building a model of inverse dynamics; we also show that the ""encoding"" process benefits significantly by integrating with the ""regression"" process for this task. Finally, we use our results to argue that representations of movement are critical to modeling and understanding these movements.","Efficient codes for inverse dynamics during walking Efficient codes have been used effectively in both computer science and neuroscience to better understand the information processing in visual and auditory encoding and discrimination tasks. In this paper, we explore the use of efficient codes for representing information relevant to human movements during locomotion. Specifically, we apply motion capture data to a physical model of the human skeleton to compute joint angles (inverse kinematics) and joint torques (inverse dynamics); then, by treating the resulting data as a regression problem, we investigate the effect of sparsity in mapping from angles to torques. The results of our investigation suggest that sparse codes can indeed represent salient features of both the kinematic and dynamic views of locomotion movements in humans. However, sparsity appears to be only one parameter in building a model of inverse dynamics; we also show that the ""encoding"" process benefits significantly by integrating with the ""regression"" process for this task. Finally, we use our results to argue that representations of movement are critical to modeling and understanding these movements. machine learning
inverse dynamics
movement",effici code invers dynam walk effici code use effect comput scienc neurosci better understand inform process visual auditori encod discrimin task paper explor use effici code repres inform relev human movement locomot specif appli motion captur data physic model human skeleton comput joint angl invers kinemat joint torqu invers dynam treat result data regress problem investig effect sparsiti map angl torqu result investig suggest spars code inde repres salient featur kinemat dynam view locomot movement human howev sparsiti appear one paramet build model invers dynam also show encod process benefit signific integr regress process task final use result argu represent movement critic model understand movement machin learn invers dynam movement,3,4.712887,0.84875333
Anytime Active Learning,"Maria E. Ramirez-Loaiza, Aron Culotta and Mustafa Bilgic","Human-Computation and Crowd Sourcing (HCC)
Machine Learning Applications (MLA)","active learning
non-uniform labeling costs
document classification","HCC: Cost, reliability, and skill of labelers
NLPML: Text Classification
NMLA: Active Learning","A common bottleneck in deploying supervised learning systems is collecting human-annotated examples. In many domains, annotators form an opinion about the label of an example incrementally --- e.g., each additional word read from a document or each additional minute spent inspecting a video helps inform the annotation.  In this paper, we investigate whether we can train learning systems more efficiently by requesting an annotation before inspection is fully complete --- e.g., after reading only 25 words of a document. While doing so may reduce the overall annotation time, it also introduces the risk that the annotator might not be able to provide a label if interrupted too early. We propose an anytime active learning approach that optimizes the annotation time and response rate simultaneously.  We conduct user studies on subsets of two document classification datasets and develop simulated annotators that mimic the users. Our simulated experiments show that anytime active learning outperforms several baselines on these two datasets. For example, with an annotation budget of one hour, training a classifier by annotating the first 25 words of each document reduces classification error by 17% over annotating the first 100 words of each document.","Anytime Active Learning A common bottleneck in deploying supervised learning systems is collecting human-annotated examples. In many domains, annotators form an opinion about the label of an example incrementally --- e.g., each additional word read from a document or each additional minute spent inspecting a video helps inform the annotation.  In this paper, we investigate whether we can train learning systems more efficiently by requesting an annotation before inspection is fully complete --- e.g., after reading only 25 words of a document. While doing so may reduce the overall annotation time, it also introduces the risk that the annotator might not be able to provide a label if interrupted too early. We propose an anytime active learning approach that optimizes the annotation time and response rate simultaneously.  We conduct user studies on subsets of two document classification datasets and develop simulated annotators that mimic the users. Our simulated experiments show that anytime active learning outperforms several baselines on these two datasets. For example, with an annotation budget of one hour, training a classifier by annotating the first 25 words of each document reduces classification error by 17% over annotating the first 100 words of each document. active learning
non-uniform labeling costs
document classification",anytim activ learn common bottleneck deploy supervis learn system collect humanannot exampl mani domain annot form opinion label exampl increment eg addit word read document addit minut spent inspect video help inform annot paper investig whether train learn system effici request annot inspect fulli complet eg read 25 word document may reduc overal annot time also introduc risk annot might abl provid label interrupt earli propos anytim activ learn approach optim annot time respons rate simultan conduct user studi subset two document classif dataset develop simul annot mimic user simul experi show anytim activ learn outperform sever baselin two dataset exampl annot budget one hour train classifi annot first 25 word document reduc classif error 17 annot first 100 word document activ learn nonuniform label cost document classif,6,-9.852849,-8.768045
Elimination Ordering in Lifted First-Order Probabilistic Inference,Seyed Mehran Kazemi and David Poole,Reasoning under Uncertainty (RU),"Lifted inference
Elimination orderings
Probabilistic inference
Statistical relational AI","RU: Probabilistic Inference
RU: Relational Probabilistic Models","Various representations and inference methods have been proposed for lifted probabilistic inference in relational models. Many of these methods choose an order to eliminate (or branch on) the parametrized random variables. Similar to such methods for non-relational probabilistic inference, the order of elimination has a significant role in the performance of the algorithm. Since finding the best order is NP-complete even for non-relational models, heuristics have been proposed to find good orderings in the non-relational models. We show that these heuristics are inefficient for relational models, because they fail to consider the population sizes associated with logical variables in the parametrized random variable. In this paper, we extend existing heuristics for non-relational models and propose new heuristics for relational models. We evaluate the existing and new heuristics on a range of generated relational graphs.","Elimination Ordering in Lifted First-Order Probabilistic Inference Various representations and inference methods have been proposed for lifted probabilistic inference in relational models. Many of these methods choose an order to eliminate (or branch on) the parametrized random variables. Similar to such methods for non-relational probabilistic inference, the order of elimination has a significant role in the performance of the algorithm. Since finding the best order is NP-complete even for non-relational models, heuristics have been proposed to find good orderings in the non-relational models. We show that these heuristics are inefficient for relational models, because they fail to consider the population sizes associated with logical variables in the parametrized random variable. In this paper, we extend existing heuristics for non-relational models and propose new heuristics for relational models. We evaluate the existing and new heuristics on a range of generated relational graphs. Lifted inference
Elimination orderings
Probabilistic inference
Statistical relational AI",elimin order lift firstord probabilist infer various represent infer method propos lift probabilist infer relat model mani method choos order elimin branch parametr random variabl similar method nonrel probabilist infer order elimin signific role perform algorithm sinc find best order npcomplet even nonrel model heurist propos find good order nonrel model show heurist ineffici relat model fail consid popul size associ logic variabl parametr random variabl paper extend exist heurist nonrel model propos new heurist relat model evalu exist new heurist rang generat relat graph lift infer elimin order probabilist infer statist relat ai,5,-8.049889,2.8002129
Predicting Postoperative Atrial Fibrillation from Independent ECG Components,"Chih-Chun Chia, James Blum, Zahi Karam, Satinder Singh and Zeeshan Syed","Applications (APP)
Machine Learning Applications (MLA)","atrial fibrillation
independent components
medicine","APP: Biomedical / Bioinformatics
MLA: Bio/Medicine","Postoperative atrial fibrillation (PAF) occurs in 10\% to 65\% of the patients undergoing cardiac surgery. It is associated with increased postoperative mortality and morbidity, and also results in longer and more expensive hospital stays. Accurately stratifying patients for PAF allows for the selective use of prophylactic therapies (e.g., amiodarone) to reduce this burden. Our proposed work addresses this need through the development of novel electrocardiographic (ECG) markers that can be easily deployed in a clinical setting to identify patients at risk of PAF. Specifically, we explore a novel eigen-decomposition approach that first partitions ECG signals into atrial and ventricular components by exploiting knowledge of the underlying cardiac cycle. We then quantify cardiac instability manifesting as probabilistic variations in atrial ECG morphology to assess the risk of PAF. When evaluated on a cohort of 385 patients undergoing cardiac surgery, our proposed approach based on an analysis of decoupled ECG components demonstrated substantial promise in identifying patients at risk of PAF and improved clinical models (both in terms of discrimination and reclassification) relative to the use of existing clinical metrics.","Predicting Postoperative Atrial Fibrillation from Independent ECG Components Postoperative atrial fibrillation (PAF) occurs in 10\% to 65\% of the patients undergoing cardiac surgery. It is associated with increased postoperative mortality and morbidity, and also results in longer and more expensive hospital stays. Accurately stratifying patients for PAF allows for the selective use of prophylactic therapies (e.g., amiodarone) to reduce this burden. Our proposed work addresses this need through the development of novel electrocardiographic (ECG) markers that can be easily deployed in a clinical setting to identify patients at risk of PAF. Specifically, we explore a novel eigen-decomposition approach that first partitions ECG signals into atrial and ventricular components by exploiting knowledge of the underlying cardiac cycle. We then quantify cardiac instability manifesting as probabilistic variations in atrial ECG morphology to assess the risk of PAF. When evaluated on a cohort of 385 patients undergoing cardiac surgery, our proposed approach based on an analysis of decoupled ECG components demonstrated substantial promise in identifying patients at risk of PAF and improved clinical models (both in terms of discrimination and reclassification) relative to the use of existing clinical metrics. atrial fibrillation
independent components
medicine",predict postop atrial fibril independ ecg compon postop atrial fibril paf occur 10 65 patient undergo cardiac surgeri associ increas postop mortal morbid also result longer expens hospit stay accur stratifi patient paf allow select use prophylact therapi eg amiodaron reduc burden propos work address need develop novel electrocardiograph ecg marker easili deploy clinic set identifi patient risk paf specif explor novel eigendecomposit approach first partit ecg signal atrial ventricular compon exploit knowledg under cardiac cycl quantifi cardiac instabl manifest probabilist variat atrial ecg morpholog assess risk paf evalu cohort 385 patient undergo cardiac surgeri propos approach base analysi decoupl ecg compon demonstr substanti promis identifi patient risk paf improv clinic model term discrimin reclassif relat use exist clinic metric atrial fibril independ compon medicin,0,1.5365427,-0.2367677
Online Multi-Task Learning via Sparse Dictionary Optimization,Paul Ruvolo and Eric Eaton,Novel Machine Learning Algorithms (NMLA),"multi-task learning
transfer learning
lifelong learning
sparse coding
k-svd","NMLA: Online Learning
NMLA: Transfer, Adaptation, Multitask Learning","This paper develops an efficient online algorithm for learning multiple consecutive tasks based on the K-SVD algorithm for sparse dictionary optimization.  We first derive a batch multi-task learning method that builds upon K-SVD, and then extend the batch algorithm to train models online in a lifelong learning setting.  The resulting method has lower computational complexity than other current lifelong learning algorithms while maintaining nearly identical performance.  Additionally, the proposed method offers an alternate formulation for lifelong learning that supports both task and feature similarity matrices.","Online Multi-Task Learning via Sparse Dictionary Optimization This paper develops an efficient online algorithm for learning multiple consecutive tasks based on the K-SVD algorithm for sparse dictionary optimization.  We first derive a batch multi-task learning method that builds upon K-SVD, and then extend the batch algorithm to train models online in a lifelong learning setting.  The resulting method has lower computational complexity than other current lifelong learning algorithms while maintaining nearly identical performance.  Additionally, the proposed method offers an alternate formulation for lifelong learning that supports both task and feature similarity matrices. multi-task learning
transfer learning
lifelong learning
sparse coding
k-svd",onlin multitask learn via spars dictionari optim paper develop effici onlin algorithm learn multipl consecut task base ksvd algorithm spars dictionari optim first deriv batch multitask learn method build upon ksvd extend batch algorithm train model onlin lifelong learn set result method lower comput complex current lifelong learn algorithm maintain near ident perform addit propos method offer altern formul lifelong learn support task featur similar matric multitask learn transfer learn lifelong learn spars code ksvd,4,0.31742617,-14.996812
"Betting Strategies, Market Selection, and the Wisdom of Crowds","Willemien Kets, David Pennock, Rajiv Sethi and Nisarg Shah",Game Theory and Economic Paradigms (GTEP),"Prediction market
Market selection
Kelly betting
CRRA utilities",GTEP: Auctions and Market-Based Systems,"We investigate the limiting behavior of trader wealth and  prices in a simple prediction market with a finite set of participants having heterogeneous beliefs. Traders bet repeatedly on the outcome of a binary event with fixed Bernoulli success probability. A class of strategies, including (fractional) Kelly betting and constant relative risk aversion (CRRA) are considered. We show that when traders are willing to risk only a small fraction of their wealth in any period, belief heterogeneity can persist indefinitely; if bets are large in proportion to wealth then only the most accurate belief type survives. The market price is more accurate in the long run when traders with less accurate {beliefs} also survive. That is, the survival of traders with heterogeneous beliefs, some less accurate than others, allows the market price to better reflect the objective probability of the event in the long run.","Betting Strategies, Market Selection, and the Wisdom of Crowds We investigate the limiting behavior of trader wealth and  prices in a simple prediction market with a finite set of participants having heterogeneous beliefs. Traders bet repeatedly on the outcome of a binary event with fixed Bernoulli success probability. A class of strategies, including (fractional) Kelly betting and constant relative risk aversion (CRRA) are considered. We show that when traders are willing to risk only a small fraction of their wealth in any period, belief heterogeneity can persist indefinitely; if bets are large in proportion to wealth then only the most accurate belief type survives. The market price is more accurate in the long run when traders with less accurate {beliefs} also survive. That is, the survival of traders with heterogeneous beliefs, some less accurate than others, allows the market price to better reflect the objective probability of the event in the long run. Prediction market
Market selection
Kelly betting
CRRA utilities",bet strategi market select wisdom crowd investig limit behavior trader wealth price simpl predict market finit set particip heterogen belief trader bet repeat outcom binari event fix bernoulli success probabl class strategi includ fraction kelli bet constant relat risk avers crra consid show trader will risk small fraction wealth period belief heterogen persist indefinit bet larg proport wealth accur belief type surviv market price accur long run trader less accur belief also surviv surviv trader heterogen belief less accur other allow market price better reflect object probabl event long run predict market market select kelli bet crra util,5,5.6384177,5.2784934
"Where and Why Users ""Check In""","Yoon-Sik Cho, Greg Ver Steeg and Aram Galstyan","AI and the Web (AIW)
Applications (APP)","Location Based Social Network
Point Processes
Temporal Clustering
Social Network Analysis","AIW: Social networking and community identification
APP: Computational Social Science
APP: Social Networks","The emergence of location based social network (LBSN) services makes it possible to study individuals’ mobility patterns at a fine-grained level and to see how they are impacted by social factors. In this study we analyze the check-in patterns in LBSN and observe significant temporal clustering of check-in activities. We explore how self-reinforcing behaviors, social factors, and exogenous effects contribute to this clustering and introduce a framework to distinguish these effects at the level of individual check-ins for both users and venues. Using check-in data from three major cities, we show not only that our model can improve prediction of future check-ins, but also that disentangling of different factors allows us to infer meaningful properties of different venues.","Where and Why Users ""Check In"" The emergence of location based social network (LBSN) services makes it possible to study individuals’ mobility patterns at a fine-grained level and to see how they are impacted by social factors. In this study we analyze the check-in patterns in LBSN and observe significant temporal clustering of check-in activities. We explore how self-reinforcing behaviors, social factors, and exogenous effects contribute to this clustering and introduce a framework to distinguish these effects at the level of individual check-ins for both users and venues. Using check-in data from three major cities, we show not only that our model can improve prediction of future check-ins, but also that disentangling of different factors allows us to infer meaningful properties of different venues. Location Based Social Network
Point Processes
Temporal Clustering
Social Network Analysis",user check emerg locat base social network lbsn servic make possibl studi individu mobil pattern finegrain level see impact social factor studi analyz checkin pattern lbsn observ signific tempor cluster checkin activ explor selfreinforc behavior social factor exogen effect contribut cluster introduc framework distinguish effect level individu checkin user venu use checkin data three major citi show model improv predict futur checkin also disentangl differ factor allow us infer meaning properti differ venu locat base social network point process tempor cluster social network analysi,0,13.347373,6.02286
Designing Fast Absorbing Markov Chains,"Stefano Ermon, Carla Gomes, Ashish Sabharwal and Bart Selman","Heuristic Search and Optimization (HSO)
Novel Machine Learning Algorithms (NMLA)","MCMC
Markov Chain
Absorption time","HSO: Evaluation and Analysis (Search and Optimization)
HSO: Search (General/Other)
NMLA: Machine Learning (General/other)","Markov Chains are a fundamental tool for the analysis of real world
phenomena and randomized algorithms. Given a graph with some specified
sink nodes and an initial probability distribution,
we consider the problem of designing an absorbing Markov
Chain that minimizes the time required to reach a sink node, by
selecting transition probabilities subject to some natural regularity
constraints. By exploiting the Markovian structure, we obtain closed
form expressions for the objective function as well as its gradient,
which can be thus evaluated efficiently without any simulation of the
underlying process and fed to a gradient-based optimization
package. For the special case of designing reversible Markov Chains,
we show that global optimum can be efficiently computed by exploiting
convexity. We demonstrate how our method can be used to
evaluate and design local search methods tailored for certain
domains.","Designing Fast Absorbing Markov Chains Markov Chains are a fundamental tool for the analysis of real world
phenomena and randomized algorithms. Given a graph with some specified
sink nodes and an initial probability distribution,
we consider the problem of designing an absorbing Markov
Chain that minimizes the time required to reach a sink node, by
selecting transition probabilities subject to some natural regularity
constraints. By exploiting the Markovian structure, we obtain closed
form expressions for the objective function as well as its gradient,
which can be thus evaluated efficiently without any simulation of the
underlying process and fed to a gradient-based optimization
package. For the special case of designing reversible Markov Chains,
we show that global optimum can be efficiently computed by exploiting
convexity. We demonstrate how our method can be used to
evaluate and design local search methods tailored for certain
domains. MCMC
Markov Chain
Absorption time",design fast absorb markov chain markov chain fundament tool analysi real world phenomena random algorithm given graph specifi sink node initi probabl distribut consid problem design absorb markov chain minim time requir reach sink node select transit probabl subject natur regular constraint exploit markovian structur obtain close form express object function well gradient thus evalu effici without simul under process fed gradientbas optim packag special case design revers markov chain show global optimum effici comput exploit convex demonstr method use evalu design local search method tailor certain domain mcmc markov chain absorpt time,7,-6.5989256,1.1258953
Symbolic Model Checking Epistemic Strategy Logic,Xiaowei Huang and Ron van der Meyden,Multiagent Systems (MAS),"Logic of Knowledge
Strategic Reasoning
Model Checking","MAS: Coordination and Collaboration
MAS: Evaluation and Analysis (Multiagent Systems)","This paper presents a symbolic BDD-based model checking algorithm for an epistemic strategy logic with observational semantics. The logic has been shown to be more expressive than several variants of ATEL and therefore the algorithm can also be used for ATEL model checking. We implement the algorithm in a model checker and apply it to several applications. The performance of the algorithm is also reported, with a comparison with a partially symbolic approach for ATEL model checking.","Symbolic Model Checking Epistemic Strategy Logic This paper presents a symbolic BDD-based model checking algorithm for an epistemic strategy logic with observational semantics. The logic has been shown to be more expressive than several variants of ATEL and therefore the algorithm can also be used for ATEL model checking. We implement the algorithm in a model checker and apply it to several applications. The performance of the algorithm is also reported, with a comparison with a partially symbolic approach for ATEL model checking. Logic of Knowledge
Strategic Reasoning
Model Checking",symbol model check epistem strategi logic paper present symbol bddbase model check algorithm epistem strategi logic observ semant logic shown express sever variant atel therefor algorithm also use atel model check implement algorithm model checker appli sever applic perform algorithm also report comparison partial symbol approach atel model check logic knowledg strateg reason model check,3,0.97927505,2.1495092
A Scheduler for Actions with Iterated Durations,James Paterson and Brian Williams,Planning and Scheduling (PS),"Scheduling
Loops
Preference
Optimization",PS: Scheduling,"A wide range of robotic missions contain actions that exhibit looping behavior. Examples of these actions include picking fruit in agriculture, pick-and-place tasks in manufacturing, or even search patterns in robotic search or survey missions. These looping actions often have a range of acceptable values for the number of loops and a preference function over them. For example, during robotic survey missions, the information gain is expected to increase with the number of loops in a search pattern. Since these looping actions also take time, which is typically bounded, there is a challenge of maximizing utility while respecting time constraints. 

In this paper, we introduce the Looping Temporal Problem with Preference (LTPP) as a formalism for encoding scheduling problems that contain looping actions. In addition, we introduce a scheduling algorithm for LTPPs, which leverages the structure of the problem to find the optimal solution efficiently.","A Scheduler for Actions with Iterated Durations A wide range of robotic missions contain actions that exhibit looping behavior. Examples of these actions include picking fruit in agriculture, pick-and-place tasks in manufacturing, or even search patterns in robotic search or survey missions. These looping actions often have a range of acceptable values for the number of loops and a preference function over them. For example, during robotic survey missions, the information gain is expected to increase with the number of loops in a search pattern. Since these looping actions also take time, which is typically bounded, there is a challenge of maximizing utility while respecting time constraints. 

In this paper, we introduce the Looping Temporal Problem with Preference (LTPP) as a formalism for encoding scheduling problems that contain looping actions. In addition, we introduce a scheduling algorithm for LTPPs, which leverages the structure of the problem to find the optimal solution efficiently. Scheduling
Loops
Preference
Optimization",schedul action iter durat wide rang robot mission contain action exhibit loop behavior exampl action includ pick fruit agricultur pickandplac task manufactur even search pattern robot search survey mission loop action often rang accept valu number loop prefer function exampl robot survey mission inform gain expect increas number loop search pattern sinc loop action also take time typic bound challeng maxim util respect time constraint paper introduc loop tempor problem prefer ltpp formal encod schedul problem contain loop action addit introduc schedul algorithm ltpps leverag structur problem find optim solut effici schedul loop prefer optim,3,-15.841021,-0.40656167
Materials Discovery - Synthetic and Real World Datasets,"John M. Gregoire, Santosh Suram, Ronan Le Bras, Richard Bernstein, Carla Gomes, Bart Selman and R. Bruce Van Dover","Computational Sustainability and AI (CSAI)
Heuristic Search and Optimization (HSO)
Machine Learning Applications (MLA)
Search and Constraint Satisfaction (SCS)","Materials discovery
Dataset
Phase-map identification problem","CSAI: Modeling and control of complex high-dimensional systems
MLA: Applications of Unsupervised Learning
SCS: Constraint Optimization","Newly-discovered materials have been central to recent technological advances. They have contributed significantly to breakthroughs in electronics, renewable energy and green buildings, and overall, have promoted the advancement of global human welfare. Yet, only a fraction of all possible materials have been explored. Accelerating the pace of discovery of materials would foster technological innovations, and would potentially address pressing issues in sustainability, such as energy production or consumption.

The bottleneck of this discovery cycle lies, however, in the analysis of the materials data. As materials scientists have recently devised techniques to efficiently create thousands of materials and experimentalists have developed new methods and tools to characterize these materials, the limiting factor has become the data analysis itself. Hence, the goal of this paper is to stimulate the development of new computational techniques for the analysis of materials data, by bringing together the complimentary expertise of materials scientists and computer scientists.

In collaboration with two major research laboratories in materials science, we provide the first publicly available dataset for the phase map identification problem. In addition, we provide a parameterized synthetic data generator to assess the quality of proposed approaches, as well as tools for data visualization and solution evaluation.","Materials Discovery - Synthetic and Real World Datasets Newly-discovered materials have been central to recent technological advances. They have contributed significantly to breakthroughs in electronics, renewable energy and green buildings, and overall, have promoted the advancement of global human welfare. Yet, only a fraction of all possible materials have been explored. Accelerating the pace of discovery of materials would foster technological innovations, and would potentially address pressing issues in sustainability, such as energy production or consumption.

The bottleneck of this discovery cycle lies, however, in the analysis of the materials data. As materials scientists have recently devised techniques to efficiently create thousands of materials and experimentalists have developed new methods and tools to characterize these materials, the limiting factor has become the data analysis itself. Hence, the goal of this paper is to stimulate the development of new computational techniques for the analysis of materials data, by bringing together the complimentary expertise of materials scientists and computer scientists.

In collaboration with two major research laboratories in materials science, we provide the first publicly available dataset for the phase map identification problem. In addition, we provide a parameterized synthetic data generator to assess the quality of proposed approaches, as well as tools for data visualization and solution evaluation. Materials discovery
Dataset
Phase-map identification problem",materi discoveri synthet real world dataset newlydiscov materi central recent technolog advanc contribut signific breakthrough electron renew energi green build overal promot advanc global human welfar yet fraction possibl materi explor acceler pace discoveri materi would foster technolog innov would potenti address press issu sustain energi product consumpt bottleneck discoveri cycl lie howev analysi materi data materi scientist recent devis techniqu effici creat thousand materi experimentalist develop new method tool character materi limit factor becom data analysi henc goal paper stimul develop new comput techniqu analysi materi data bring togeth complimentari expertis materi scientist comput scientist collabor two major research laboratori materi scienc provid first public avail dataset phase map identif problem addit provid parameter synthet data generat assess qualiti propos approach well tool data visual solut evalu materi discoveri dataset phasemap identif problem,6,-0.60232204,-2.239973
Automatic Synthesis of Geometry Problems for an Intelligent Tutoring System,"Christopher Alvin, Sumit Gulwani, Rupak Majumdar and Supratik Mukhopadhyay",Applications (APP),"Problem Synthesis
Automated Reasoning
Computer-Aided Education
Intelligent Tutor",APP: Computer-Aided Education,"This paper presents an intelligent tutoring system, GeoTutor, for Euclidean Geometry that is automatically able to synthesize proof problems and their respective solutions given a geometric figure together with a set of properties true of it. GeoTutor can provide personalized practice problems that address student deficiencies in the subject matter.","Automatic Synthesis of Geometry Problems for an Intelligent Tutoring System This paper presents an intelligent tutoring system, GeoTutor, for Euclidean Geometry that is automatically able to synthesize proof problems and their respective solutions given a geometric figure together with a set of properties true of it. GeoTutor can provide personalized practice problems that address student deficiencies in the subject matter. Problem Synthesis
Automated Reasoning
Computer-Aided Education
Intelligent Tutor",automat synthesi geometri problem intellig tutor system paper present intellig tutor system geotutor euclidean geometri automat abl synthes proof problem respect solut given geometr figur togeth set properti true geotutor provid person practic problem address student defici subject matter problem synthesi autom reason computeraid educ intellig tutor,3,6.70958,-3.6379948
Modeling Subjective Experience-based Learning under Uncertainty and Frames,Hyung-Il Ahn and Rosalind Picard,"Cognitive Modeling (CM)
Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)","subjective experience-based learning
subjective value function
prospect theory
subjective discriminability
experienced utility
decision utility
gain frame
loss frame","CM: Adaptive Behavior
CM: Simulating Humans
NMLA: Reinforcement Learning
RU: Decision/Utility Theory","In this paper we computationally examine how subjective experience may help or harm the decision maker's learning under uncertain outcomes, frames and their interactions. To model subjective experience, we propose the ``experienced-utility function'' based on a prospect theory (PT)-based parameterized subjective value function. Our analysis and simulations of two-armed bandit tasks present that the task domain (underlying outcome distributions) and framing (reference point selection) influence experienced utilities and in turn, the ``subjective discriminability'' of choices under uncertainty.  Experiments demonstrate that subjective discriminability improves on objective discriminability by the use of the experienced-utility function with appropriate framing for a domain, and that bigger subjective discriminability leads to more optimal decisions in learning under uncertainty.","Modeling Subjective Experience-based Learning under Uncertainty and Frames In this paper we computationally examine how subjective experience may help or harm the decision maker's learning under uncertain outcomes, frames and their interactions. To model subjective experience, we propose the ``experienced-utility function'' based on a prospect theory (PT)-based parameterized subjective value function. Our analysis and simulations of two-armed bandit tasks present that the task domain (underlying outcome distributions) and framing (reference point selection) influence experienced utilities and in turn, the ``subjective discriminability'' of choices under uncertainty.  Experiments demonstrate that subjective discriminability improves on objective discriminability by the use of the experienced-utility function with appropriate framing for a domain, and that bigger subjective discriminability leads to more optimal decisions in learning under uncertainty. subjective experience-based learning
subjective value function
prospect theory
subjective discriminability
experienced utility
decision utility
gain frame
loss frame",model subject experiencebas learn uncertainti frame paper comput examin subject experi may help harm decis maker learn uncertain outcom frame interact model subject experi propos experiencedutil function base prospect theori ptbase parameter subject valu function analysi simul twoarm bandit task present task domain under outcom distribut frame refer point select influenc experienc util turn subject discrimin choic uncertainti experi demonstr subject discrimin improv object discrimin use experiencedutil function appropri frame domain bigger subject discrimin lead optim decis learn uncertainti subject experiencebas learn subject valu function prospect theori subject discrimin experienc util decis util gain frame loss frame,6,0.5597787,-5.827122
Resolving Pronouns by Leveraging English Resources,Chen Chen and Vincent Ng,NLP and Text Mining (NLPTM),"Pronouns
Text Mining
Natural Language Processing",NLPML: Evaluation and Analysis,"Existing approaches to pronoun resolution are monolingual, training and testing a pronoun resolver on the data from original language. In contrast, we propose a bilingual approach to pronoun resolution, aiming to improve the resolution of pronouns by leveraging both the publicly available dictionaries and coreference annotations from a second language. Experiments on the OntoNotes corpus demonstrate that our bilingual approach to pronoun resolution significantly surpasses the performance of state-of-the-art monolingual approaches.","Resolving Pronouns by Leveraging English Resources Existing approaches to pronoun resolution are monolingual, training and testing a pronoun resolver on the data from original language. In contrast, we propose a bilingual approach to pronoun resolution, aiming to improve the resolution of pronouns by leveraging both the publicly available dictionaries and coreference annotations from a second language. Experiments on the OntoNotes corpus demonstrate that our bilingual approach to pronoun resolution significantly surpasses the performance of state-of-the-art monolingual approaches. Pronouns
Text Mining
Natural Language Processing",resolv pronoun leverag english resourc exist approach pronoun resolut monolingu train test pronoun resolv data origin languag contrast propos bilingu approach pronoun resolut aim improv resolut pronoun leverag public avail dictionari corefer annot second languag experi ontonot corpus demonstr bilingu approach pronoun resolut signific surpass perform stateoftheart monolingu approach pronoun text mine natur languag process,6,21.855398,-10.998811
Bagging by design (on the sub-optimality of bagging),"Cao Zhu, Periklis Papakonstantinou and Jia Xu",Novel Machine Learning Algorithms (NMLA),"bagging
bootstrapping
combinatorial design
noise stability
correlation
dependent sampling","NMLA: Classification
NMLA: Ensemble Methods
NMLA: Supervised Learning (Other)
NMLA: Machine Learning (General/other)","Bagging (Breiman 1996) and its variants is one of the most popular methods
in aggregating classifiers and regressors. Originally, its analysis assumed that the bootstraps are built from an unlimited, independent source of samples, therefore we call this form of bagging \emph{ideal-bagging}. However in the real world, base predictors are trained on data subsampled from a limited number of training samples and thus they behave very differently. We analyze the effect of intersections between bootstraps (obtained by subsampling) to train different base predictors. Most importantly, we provide an alternative subsampling method called \emph{design-bagging} based on a new construction of combinatorial designs, and prove it universally better than bagging. Methodologically, we succeed at this level of generality because we compare the bagging and design-bagging on their prediction accuracy each relative to the accuracy ideal-bagging.This can possibly find applications in more involved bagging-based ensemble methods. Our analytical results are backed up by experiments on classification and regression settings.","Bagging by design (on the sub-optimality of bagging) Bagging (Breiman 1996) and its variants is one of the most popular methods
in aggregating classifiers and regressors. Originally, its analysis assumed that the bootstraps are built from an unlimited, independent source of samples, therefore we call this form of bagging \emph{ideal-bagging}. However in the real world, base predictors are trained on data subsampled from a limited number of training samples and thus they behave very differently. We analyze the effect of intersections between bootstraps (obtained by subsampling) to train different base predictors. Most importantly, we provide an alternative subsampling method called \emph{design-bagging} based on a new construction of combinatorial designs, and prove it universally better than bagging. Methodologically, we succeed at this level of generality because we compare the bagging and design-bagging on their prediction accuracy each relative to the accuracy ideal-bagging.This can possibly find applications in more involved bagging-based ensemble methods. Our analytical results are backed up by experiments on classification and regression settings. bagging
bootstrapping
combinatorial design
noise stability
correlation
dependent sampling",bag design suboptim bag bag breiman 1996 variant one popular method aggreg classifi regressor origin analysi assum bootstrap built unlimit independ sourc sampl therefor call form bag emphidealbag howev real world base predictor train data subsampl limit number train sampl thus behav differ analyz effect intersect bootstrap obtain subsampl train differ base predictor import provid altern subsampl method call emphdesignbag base new construct combinatori design prove univers better bag methodolog succeed level general compar bag designbag predict accuraci relat accuraci idealbaggingthi possibl find applic involv baggingbas ensembl method analyt result back experi classif regress set bag bootstrap combinatori design nois stabil correl depend sampl,6,-3.3828702,-3.726893
A Hybrid Grammar-Based Approach for Learning and Recognizing Natural Hand Gestures,Amir Sadeghipour and Stefan Kopp,"Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Iconic Hand Gestures
Stochastic Context-Free Grammar
Machine learning
Classification","MLA: Applications of Supervised Learning
NMLA: Classification
NMLA: Graphical Model Learning
NMLA: Supervised Learning (Other)
NMLA: Machine Learning (General/other)
VIS: Face and Gesture Recognition","In this paper, we present an approach to learn structured models of gesture performances that allow for a compressed representation and robust recognition of natural iconic gestures. We analyze a dataset of iconic gestures and show how the proposed hybrid grammar formalism can generalize over both structural and feature-based variations among different gesture performances.","A Hybrid Grammar-Based Approach for Learning and Recognizing Natural Hand Gestures In this paper, we present an approach to learn structured models of gesture performances that allow for a compressed representation and robust recognition of natural iconic gestures. We analyze a dataset of iconic gestures and show how the proposed hybrid grammar formalism can generalize over both structural and feature-based variations among different gesture performances. Iconic Hand Gestures
Stochastic Context-Free Grammar
Machine learning
Classification",hybrid grammarbas approach learn recogn natur hand gestur paper present approach learn structur model gestur perform allow compress represent robust recognit natur icon gestur analyz dataset icon gestur show propos hybrid grammar formal general structur featurebas variat among differ gestur perform icon hand gestur stochast contextfre grammar machin learn classif,1,2.2930315,-3.6245236
Combining Heterogenous Social and Geographical Information for Event Recommendation,"Zhi Qiao, Peng Zhang, Yanan Cao, Chuan Zhou and Li Guo","AI and the Web (AIW)
Novel Machine Learning Algorithms (NMLA)","heterogeneous social networks
event recommendation
geographical features","AIW: Web-based recommendation systems
NMLA: Recommender Systems","With the rapid growth of event-based social services (EBSSs) like \emph{Meetup}, the demand for event recommendation becomes increasingly urgent. In EBSSs, event recommendation plays a central role in recommending the most relevant events to users who are likely to participate in. Different from traditional recommendation problems, event recommendation encounters three new types of information, \emph{i.e.}, heterogeneous online+offline social relationships, geographical information of events and implicit feedback data from users. Yet combining the three types of data for event recommendation has not been considered. Therefore, we present a Bayesian probability model that can unify these data for event recommendation. Experimental results on real-world data sets show the performance of our method.","Combining Heterogenous Social and Geographical Information for Event Recommendation With the rapid growth of event-based social services (EBSSs) like \emph{Meetup}, the demand for event recommendation becomes increasingly urgent. In EBSSs, event recommendation plays a central role in recommending the most relevant events to users who are likely to participate in. Different from traditional recommendation problems, event recommendation encounters three new types of information, \emph{i.e.}, heterogeneous online+offline social relationships, geographical information of events and implicit feedback data from users. Yet combining the three types of data for event recommendation has not been considered. Therefore, we present a Bayesian probability model that can unify these data for event recommendation. Experimental results on real-world data sets show the performance of our method. heterogeneous social networks
event recommendation
geographical features",combin heterogen social geograph inform event recommend rapid growth eventbas social servic ebsss like emphmeetup demand event recommend becom increas urgent ebsss event recommend play central role recommend relev event user like particip differ tradit recommend problem event recommend encount three new type inform emphi heterogen onlineofflin social relationship geograph inform event implicit feedback data user yet combin three type data event recommend consid therefor present bayesian probabl model unifi data event recommend experiment result realworld data set show perform method heterogen social network event recommend geograph featur,0,16.416487,6.6563253
"Leveraging Fee-Based, Imperfect Advisors in Human-Agent Games of Trust","Cody Buntain, Sarit Kraus and Amos Azaria",Humans and AI (HAI),"advisors
trust games
investment game
bribery",HAI: Human-Computer Interaction,"This paper explores whether the addition of costly, imperfect, and exploitable advisors to Berg's investment game enhances or detracts from investor performance in both one-shot and multi-round interactions.
We then leverage our findings to develop an automated investor agent that performs as well as or better than humans in these games.
To gather this data, we extended Berg's game and conducted a series of experiments using Amazon's Mechanical Turk to determine how humans behave in these potentially adversarial conditions.
Our results indicate that, in games of short duration, advisors do not stimulate positive behavior and are not useful in providing actionable advice.
In long-term interactions, however, advisors do stimulate positive behavior with significantly increased investments and returns.
By modeling human behavior across several hundred participants, we were then able to develop agent strategies that maximized return on investment and performed as well as or significantly better than humans.
In one-shot games, we identified an ideal investment value that, on average, resulted in positive returns as long as advisor exploitation was not allowed.
For the multi-round games, our agents relied on the corrective presence of advisors to stimulate positive returns on maximum investment.","Leveraging Fee-Based, Imperfect Advisors in Human-Agent Games of Trust This paper explores whether the addition of costly, imperfect, and exploitable advisors to Berg's investment game enhances or detracts from investor performance in both one-shot and multi-round interactions.
We then leverage our findings to develop an automated investor agent that performs as well as or better than humans in these games.
To gather this data, we extended Berg's game and conducted a series of experiments using Amazon's Mechanical Turk to determine how humans behave in these potentially adversarial conditions.
Our results indicate that, in games of short duration, advisors do not stimulate positive behavior and are not useful in providing actionable advice.
In long-term interactions, however, advisors do stimulate positive behavior with significantly increased investments and returns.
By modeling human behavior across several hundred participants, we were then able to develop agent strategies that maximized return on investment and performed as well as or significantly better than humans.
In one-shot games, we identified an ideal investment value that, on average, resulted in positive returns as long as advisor exploitation was not allowed.
For the multi-round games, our agents relied on the corrective presence of advisors to stimulate positive returns on maximum investment. advisors
trust games
investment game
bribery",leverag feebas imperfect advisor humanag game trust paper explor whether addit cost imperfect exploit advisor berg invest game enhanc detract investor perform oneshot multiround interact leverag find develop autom investor agent perform well better human game gather data extend berg game conduct seri experi use amazon mechan turk determin human behav potenti adversari condit result indic game short durat advisor stimul posit behavior use provid action advic longterm interact howev advisor stimul posit behavior signific increas invest return model human behavior across sever hundr particip abl develop agent strategi maxim return invest perform well signific better human oneshot game identifi ideal invest valu averag result posit return long advisor exploit allow multiround game agent reli correct presenc advisor stimul posit return maximum invest advisor trust game invest game briberi,2,6.641469,18.751286
GenEth: A General Ethical Dilemma Analyzer,Michael Anderson and Susan Leigh Anderson,"Applications (APP)
Humans and AI (HAI)
Machine Learning Applications (MLA)","machine ethics
concept learning
application","APP: Philosophical and Ethical Issues
HAI: Human-Computer Interaction
HAI: Understanding People, Theories, Concepts and Methods
KRR: Logic Programming
MLA: Applications of Supervised Learning
MLA: Machine Learning Applications (General/other)","We contend that ethically significant behavior of autonomous systems should be guided by explicit ethical principles determined through a consensus of ethicists. As it is likely that in many particular cases of ethical dilemmas ethicists agree on the ethically relevant features and the right course of action, generalization of such cases can be used to help discover principles needed for ethical guidance of the behavior of autonomous systems. Such principles help ensure the ethical behavior of complex and dynamic systems and further serve as a basis for justification of their actions as well as a control abstraction for managing unanticipated behavior. To provide assistance in developing ethical principles, we have developed GENETH, a general ethical dilemma analyzer that, through a dialog with ethicists, codifies ethical principles in any given domain.  GENETH has been used to codify principles in a number of domains pertinent to the behavior of autonomous systems and these principles have been verified using an Ethical Turing Test.","GenEth: A General Ethical Dilemma Analyzer We contend that ethically significant behavior of autonomous systems should be guided by explicit ethical principles determined through a consensus of ethicists. As it is likely that in many particular cases of ethical dilemmas ethicists agree on the ethically relevant features and the right course of action, generalization of such cases can be used to help discover principles needed for ethical guidance of the behavior of autonomous systems. Such principles help ensure the ethical behavior of complex and dynamic systems and further serve as a basis for justification of their actions as well as a control abstraction for managing unanticipated behavior. To provide assistance in developing ethical principles, we have developed GENETH, a general ethical dilemma analyzer that, through a dialog with ethicists, codifies ethical principles in any given domain.  GENETH has been used to codify principles in a number of domains pertinent to the behavior of autonomous systems and these principles have been verified using an Ethical Turing Test. machine ethics
concept learning
application",geneth general ethic dilemma analyz contend ethic signific behavior autonom system guid explicit ethic principl determin consensus ethicist like mani particular case ethic dilemma ethicist agre ethic relev featur right cours action general case use help discov principl need ethic guidanc behavior autonom system principl help ensur ethic behavior complex dynam system serv basi justif action well control abstract manag unanticip behavior provid assist develop ethic principl develop geneth general ethic dilemma analyz dialog ethicist codifi ethic principl given domain geneth use codifi principl number domain pertin behavior autonom system principl verifi use ethic ture test machin ethic concept learn applic,6,3.258627,2.9202332
Solving the Traveling Tournament Problem by Packing Three-Vertex Paths,"Richard Hoshino, Ken-Ichi Kawarabayashi, Marc Goerigk and Stephan Westphal","Heuristic Search and Optimization (HSO)
Planning and Scheduling (PS)","traveling tournament problem
sports scheduling
scheduling optimization
graph theory","HSO: Heuristic Search
HSO: Optimization
PS: Scheduling","The Traveling Tournament Problem (TTP) is a complex problem in sports scheduling whose solution is a schedule of home and away games meeting specific feasibility requirements, while minimizing the total distance traveled by all the teams.  A recently-developed ""hybrid"" algorithm, combining local search and integer programming, has resulted in best-known solutions for many TTP instances.  In this paper, we tackle the TTP from a graph-theoretic perspective, by generating a new ""canonical"" schedule in which each team's three-game road trips match up with the underlying graph's minimum-weight P_3-packing.  By using this new schedule as the initial input for the hybrid algorithm, we develop tournament schedules for five benchmark TTP instances that beat all previously-known solutions.","Solving the Traveling Tournament Problem by Packing Three-Vertex Paths The Traveling Tournament Problem (TTP) is a complex problem in sports scheduling whose solution is a schedule of home and away games meeting specific feasibility requirements, while minimizing the total distance traveled by all the teams.  A recently-developed ""hybrid"" algorithm, combining local search and integer programming, has resulted in best-known solutions for many TTP instances.  In this paper, we tackle the TTP from a graph-theoretic perspective, by generating a new ""canonical"" schedule in which each team's three-game road trips match up with the underlying graph's minimum-weight P_3-packing.  By using this new schedule as the initial input for the hybrid algorithm, we develop tournament schedules for five benchmark TTP instances that beat all previously-known solutions. traveling tournament problem
sports scheduling
scheduling optimization
graph theory",solv travel tournament problem pack threevertex path travel tournament problem ttp complex problem sport schedul whose solut schedul home away game meet specif feasibl requir minim total distanc travel team recentlydevelop hybrid algorithm combin local search integ program result bestknown solut mani ttp instanc paper tackl ttp graphtheoret perspect generat new canon schedul team threegam road trip match under graph minimumweight p3pack use new schedul initi input hybrid algorithm develop tournament schedul five benchmark ttp instanc beat previouslyknown solut travel tournament problem sport schedul schedul optim graph theori,5,-15.135548,-3.102885
Gradient Descent Method with Proximal Average for Nonconvex and Composite Regularization,Wenliang Zhong and James Kwok,Novel Machine Learning Algorithms (NMLA),"non-convex optimization
composite regularization
proximal average","NMLA: Big Data / Scalability
NMLA: Dimension Reduction/Feature Selection","Sparse modeling has been highly successful on high-dimensional data. While a lot of interests have been on convex regularization, recent studies show that nonconvex regularizers can outperform their convex counterparts in many situations. However, the resulting nonconvex optimization problems are often challenging, especially for composite regularizers such as the nonconvex overlapping group lasso. In this paper,  by using a recent tool known as the proximal average, we propose a novel proximal gradient descent method for a wide class of composite and nonconvex problems. Instead of directly solving the proximal step with a composite regularizer, we average the solutions from the proximal problems of the individual regularizers. This simple strategy has similar convergence guarantee as existing nonconvex optimization approaches,but its per-iteration complexity is much lower. Experimental results on synthetic and real-world data sets demonstrate the effectiveness and efficiency of the proposed optimization algorithm, and also the improved prediction performance resulting from the nonconvex regularizers.","Gradient Descent Method with Proximal Average for Nonconvex and Composite Regularization Sparse modeling has been highly successful on high-dimensional data. While a lot of interests have been on convex regularization, recent studies show that nonconvex regularizers can outperform their convex counterparts in many situations. However, the resulting nonconvex optimization problems are often challenging, especially for composite regularizers such as the nonconvex overlapping group lasso. In this paper,  by using a recent tool known as the proximal average, we propose a novel proximal gradient descent method for a wide class of composite and nonconvex problems. Instead of directly solving the proximal step with a composite regularizer, we average the solutions from the proximal problems of the individual regularizers. This simple strategy has similar convergence guarantee as existing nonconvex optimization approaches,but its per-iteration complexity is much lower. Experimental results on synthetic and real-world data sets demonstrate the effectiveness and efficiency of the proposed optimization algorithm, and also the improved prediction performance resulting from the nonconvex regularizers. non-convex optimization
composite regularization
proximal average",gradient descent method proxim averag nonconvex composit regular spars model high success highdimension data lot interest convex regular recent studi show nonconvex regular outperform convex counterpart mani situat howev result nonconvex optim problem often challeng especi composit regular nonconvex overlap group lasso paper use recent tool known proxim averag propos novel proxim gradient descent method wide class composit nonconvex problem instead direct solv proxim step composit regular averag solut proxim problem individu regular simpl strategi similar converg guarante exist nonconvex optim approachesbut periter complex much lower experiment result synthet realworld data set demonstr effect effici propos optim algorithm also improv predict perform result nonconvex regular nonconvex optim composit regular proxim averag,4,3.4587822,-19.361628
Multilabel Classification with Label Correlations and Missing Labels,Wei Bi and James Kwok,Novel Machine Learning Algorithms (NMLA),"multilabel classification
label correlation
missing label",NMLA: Classification,"Many real-world applications involve multilabel classification, in which the labels can have strong inter-dependencies and some of them may even be missing. Existing multilabel algorithms are unable to deal with both issues simultaneously. In this paper, we propose a probabilistic model that can automatically learn and exploit multilabel correlations. By integrating out the missing information, it also provides a disciplined approach to the handling of missing labels. The inference procedure is simple, and the optimization subproblems are convex. Experiments on a number of real-world data sets with both complete and missing labels demonstrate that the proposed algorithm can consistently outperform the state-of-the-art multilabel classification algorithms.","Multilabel Classification with Label Correlations and Missing Labels Many real-world applications involve multilabel classification, in which the labels can have strong inter-dependencies and some of them may even be missing. Existing multilabel algorithms are unable to deal with both issues simultaneously. In this paper, we propose a probabilistic model that can automatically learn and exploit multilabel correlations. By integrating out the missing information, it also provides a disciplined approach to the handling of missing labels. The inference procedure is simple, and the optimization subproblems are convex. Experiments on a number of real-world data sets with both complete and missing labels demonstrate that the proposed algorithm can consistently outperform the state-of-the-art multilabel classification algorithms. multilabel classification
label correlation
missing label",multilabel classif label correl miss label mani realworld applic involv multilabel classif label strong interdepend may even miss exist multilabel algorithm unabl deal issu simultan paper propos probabilist model automat learn exploit multilabel correl integr miss inform also provid disciplin approach handl miss label infer procedur simpl optim subproblem convex experi number realworld data set complet miss label demonstr propos algorithm consist outperform stateoftheart multilabel classif algorithm multilabel classif label correl miss label,6,-7.472754,-12.41129
Double Configuration Checking in Stochastic Local Search for Satisfiability,"Chuan Luo, Shaowei Cai, Wei Wu and Kaile Su","Heuristic Search and Optimization (HSO)
Search and Constraint Satisfaction (SCS)","Double Configuration Checking
Stochastic Local Search
Satisfiability
Phase Transition","HSO: Heuristic Search
SCS: Constraint Satisfaction
SCS: SAT and CSP: Solvers and Tools","Stochastic local search (SLS) algorithms have shown effectiveness on satisfiable instances of the Boolean satisfiability (SAT) problem. However, their performance is still unsatisfactory on random k-SAT at the phase transition, which is of significance and is one of the most empirically hardest distributions of SAT instances. In this paper, we propose a new heuristic called DCCA, which combines two configuration checking (CC) strategies with different definitions of configuration in a novel way. We use the DCCA heuristic to design an efficient SLS solver for SAT dubbed DCCASat. The experiments show that the DCCASat solver significantly outperforms a number of state-of-the-art solvers on extensive random k-SAT benchmarks at the phase transition. Moreover, further empirical analyses on structured benchmarks indicate the robustness of DCCASat.","Double Configuration Checking in Stochastic Local Search for Satisfiability Stochastic local search (SLS) algorithms have shown effectiveness on satisfiable instances of the Boolean satisfiability (SAT) problem. However, their performance is still unsatisfactory on random k-SAT at the phase transition, which is of significance and is one of the most empirically hardest distributions of SAT instances. In this paper, we propose a new heuristic called DCCA, which combines two configuration checking (CC) strategies with different definitions of configuration in a novel way. We use the DCCA heuristic to design an efficient SLS solver for SAT dubbed DCCASat. The experiments show that the DCCASat solver significantly outperforms a number of state-of-the-art solvers on extensive random k-SAT benchmarks at the phase transition. Moreover, further empirical analyses on structured benchmarks indicate the robustness of DCCASat. Double Configuration Checking
Stochastic Local Search
Satisfiability
Phase Transition",doubl configur check stochast local search satisfi stochast local search sls algorithm shown effect satisfi instanc boolean satisfi sat problem howev perform still unsatisfactori random ksat phase transit signific one empir hardest distribut sat instanc paper propos new heurist call dcca combin two configur check cc strategi differ definit configur novel way use dcca heurist design effici sls solver sat dub dccasat experi show dccasat solver signific outperform number stateoftheart solver extens random ksat benchmark phase transit moreov empir analys structur benchmark indic robust dccasat doubl configur check stochast local search satisfi phase transit,4,-11.054791,15.187159
Direct Semantic Analysis for Social Image Classification,"Zhiwu Lu, Liwei Wang and Ji-Rong Wen","Machine Learning Applications (MLA)
Vision (VIS)","social image classification
latent semantic analysis
graph-based learning
bag-of-words","MLA: Machine Learning Applications (General/other)
VIS: Image and Video Retrieval
VIS: Statistical Methods and Learning","This paper presents a direct semantic analysis method for learning the correlation matrix between visual and textual words from socially tagged images. In the literature, to improve the traditional visual bag-of-words (BOW) representation, latent semantic analysis has been studied extensively for learning a compact visual representation, where each visual word may be related to multiple latent topics. However, these latent topics do not convey any true semantic information which can be understood by human. In fact, it remains a challenging problem how to recover the relationships between visual and textual words. Motivated by the recent advances in dealing with socially tagged images, we develop a direct semantic analysis method which can explicitly learn the correlation matrix between visual and textual words for social image classification. To this end, we formulate our direct semantic analysis from a graph-based learning viewpoint. Once the correlation matrix is learnt, we can readily first obtain a semantically refined visual BOW representation and then apply it to social image classification. Experimental results on two benchmark image datasets show the promising performance of the proposed method.","Direct Semantic Analysis for Social Image Classification This paper presents a direct semantic analysis method for learning the correlation matrix between visual and textual words from socially tagged images. In the literature, to improve the traditional visual bag-of-words (BOW) representation, latent semantic analysis has been studied extensively for learning a compact visual representation, where each visual word may be related to multiple latent topics. However, these latent topics do not convey any true semantic information which can be understood by human. In fact, it remains a challenging problem how to recover the relationships between visual and textual words. Motivated by the recent advances in dealing with socially tagged images, we develop a direct semantic analysis method which can explicitly learn the correlation matrix between visual and textual words for social image classification. To this end, we formulate our direct semantic analysis from a graph-based learning viewpoint. Once the correlation matrix is learnt, we can readily first obtain a semantically refined visual BOW representation and then apply it to social image classification. Experimental results on two benchmark image datasets show the promising performance of the proposed method. social image classification
latent semantic analysis
graph-based learning
bag-of-words",direct semant analysi social imag classif paper present direct semant analysi method learn correl matrix visual textual word social tag imag literatur improv tradit visual bagofword bow represent latent semant analysi studi extens learn compact visual represent visual word may relat multipl latent topic howev latent topic convey true semant inform understood human fact remain challeng problem recov relationship visual textual word motiv recent advanc deal social tag imag develop direct semant analysi method explicit learn correl matrix visual textual word social imag classif end formul direct semant analysi graphbas learn viewpoint correl matrix learnt readili first obtain semant refin visual bow represent appli social imag classif experiment result two benchmark imag dataset show promis perform propos method social imag classif latent semant analysi graphbas learn bagofword,1,-23.897371,-6.405628
On the Structure of Synergies in Cooperative Games,"Ariel Procaccia, Nisarg Shah and Max Tucker",Game Theory and Economic Paradigms (GTEP),"Cooperative game theory
Shapley value
Weighted voting games",GTEP: Game Theory,"We investigate synergy, or lack thereof, between agents in cooperative games, building on the popular notion of Shapley value. We think of a pair of agents as synergistic (resp., antagonistic) if the Shapley value of one agent when the other agent participates in a joint effort is higher (resp. lower) than when the other agent does not participate. Our main theoretical result is that any graph specifying synergistic and antagonistic pairs can arise even from a restricted class of cooperative games. We also study the computational complexity of determining whether a given pair of agents is synergistic. Finally, we use the concepts developed in the paper to uncover the structure of synergies in two real-world organizations, the European Union and the International Monetary Fund.","On the Structure of Synergies in Cooperative Games We investigate synergy, or lack thereof, between agents in cooperative games, building on the popular notion of Shapley value. We think of a pair of agents as synergistic (resp., antagonistic) if the Shapley value of one agent when the other agent participates in a joint effort is higher (resp. lower) than when the other agent does not participate. Our main theoretical result is that any graph specifying synergistic and antagonistic pairs can arise even from a restricted class of cooperative games. We also study the computational complexity of determining whether a given pair of agents is synergistic. Finally, we use the concepts developed in the paper to uncover the structure of synergies in two real-world organizations, the European Union and the International Monetary Fund. Cooperative game theory
Shapley value
Weighted voting games",structur synergi cooper game investig synergi lack thereof agent cooper game build popular notion shapley valu think pair agent synergist resp antagonist shapley valu one agent agent particip joint effort higher resp lower agent particip main theoret result graph specifi synergist antagonist pair aris even restrict class cooper game also studi comput complex determin whether given pair agent synergist final use concept develop paper uncov structur synergi two realworld organ european union intern monetari fund cooper game theori shapley valu weight vote game,2,5.750485,14.560884
Similarity-preserving binary signature for linear subspace,"Jianqiu Ji, Jianmin Li, Shuicheng Yan, Qi Tian and Bo Zhang",Novel Machine Learning Algorithms (NMLA),"linear subspace
angular distance
binary signature
Hamming distance",NMLA: Data Mining and Knowledge Discovery,"Linear subspace is an important representation for many kinds of real-world data in computer vision and pattern recognition, e.g. faces, motion videos, speech. In this paper, first we define pairwise angular similarity and angular distance for linear subspaces. The angular distance satisfies non-negativity, identity of indiscernibles, symmetry and triangle inequality, and thus it is a metric. Then we propose a method to compress linear subspaces into compact similarity-preserving binary signatures, between which the normalized Hamming distance is an unbiased estimator of the angular distance. We provide a lower bound on the length of binary signatures which suffices to guarantee a uniform distance-preservation within a set of subspaces. Experiments on face recognition demonstrate the effectiveness of this binary signature in terms of recognition accuracy, speed and storage requirement. The results show that, compared with the exact method, the approximation with binary signatures achieves an order of magnitude speed-up, while requiring significantly smaller amount of storage space, yet it still accurately preserves the similarity, and achieves high recognition accuracy comparable to the exact method in face recognition.","Similarity-preserving binary signature for linear subspace Linear subspace is an important representation for many kinds of real-world data in computer vision and pattern recognition, e.g. faces, motion videos, speech. In this paper, first we define pairwise angular similarity and angular distance for linear subspaces. The angular distance satisfies non-negativity, identity of indiscernibles, symmetry and triangle inequality, and thus it is a metric. Then we propose a method to compress linear subspaces into compact similarity-preserving binary signatures, between which the normalized Hamming distance is an unbiased estimator of the angular distance. We provide a lower bound on the length of binary signatures which suffices to guarantee a uniform distance-preservation within a set of subspaces. Experiments on face recognition demonstrate the effectiveness of this binary signature in terms of recognition accuracy, speed and storage requirement. The results show that, compared with the exact method, the approximation with binary signatures achieves an order of magnitude speed-up, while requiring significantly smaller amount of storage space, yet it still accurately preserves the similarity, and achieves high recognition accuracy comparable to the exact method in face recognition. linear subspace
angular distance
binary signature
Hamming distance",similaritypreserv binari signatur linear subspac linear subspac import represent mani kind realworld data comput vision pattern recognit eg face motion video speech paper first defin pairwis angular similar angular distanc linear subspac angular distanc satisfi nonneg ident indiscern symmetri triangl inequ thus metric propos method compress linear subspac compact similaritypreserv binari signatur normal ham distanc unbias estim angular distanc provid lower bound length binari signatur suffic guarante uniform distancepreserv within set subspac experi face recognit demonstr effect binari signatur term recognit accuraci speed storag requir result show compar exact method approxim binari signatur achiev order magnitud speedup requir signific smaller amount storag space yet still accur preserv similar achiev high recognit accuraci compar exact method face recognit linear subspac angular distanc binari signatur ham distanc,1,2.4178073,-5.648685
Sample-Adaptive Multiple Kernel Learning,"Xinwang Liu, Lei Wang, Jian Zhang and Jianping Yin",Novel Machine Learning Algorithms (NMLA),"Latent Support Vector Machine
Multiple Kernel Learning
Inference","NMLA: Classification
NMLA: Ensemble Methods
NMLA: Kernel Methods
NMLA: Supervised Learning (Other)","Existing multiple kernel learning (MKL) algorithms indiscriminately apply a same set of kernel combination weights to all samples. However, the utility of base kernels could vary across samples and a base kernel useful for one sample could become noisy for another. In this case, rigidly applying a same set of kernel combination weights could adversely affect the learning performance. To improve this situation, we propose a sample-adaptive MKL algorithm, in which base kernels are allowed to be adaptively switched on/off with respect to each sample. We achieve this goal by assigning a latent binary variable to each base kernel when it is applied to a sample. The kernel combination weights and the latent variables are jointly optimized via margin maximization principle. As demonstrated on five benchmark data sets, the proposed algorithm consistently outperforms the comparable ones in the literature.","Sample-Adaptive Multiple Kernel Learning Existing multiple kernel learning (MKL) algorithms indiscriminately apply a same set of kernel combination weights to all samples. However, the utility of base kernels could vary across samples and a base kernel useful for one sample could become noisy for another. In this case, rigidly applying a same set of kernel combination weights could adversely affect the learning performance. To improve this situation, we propose a sample-adaptive MKL algorithm, in which base kernels are allowed to be adaptively switched on/off with respect to each sample. We achieve this goal by assigning a latent binary variable to each base kernel when it is applied to a sample. The kernel combination weights and the latent variables are jointly optimized via margin maximization principle. As demonstrated on five benchmark data sets, the proposed algorithm consistently outperforms the comparable ones in the literature. Latent Support Vector Machine
Multiple Kernel Learning
Inference",sampleadapt multipl kernel learn exist multipl kernel learn mkl algorithm indiscrimin appli set kernel combin weight sampl howev util base kernel could vari across sampl base kernel use one sampl could becom noisi anoth case rigid appli set kernel combin weight could advers affect learn perform improv situat propos sampleadapt mkl algorithm base kernel allow adapt switch onoff respect sampl achiev goal assign latent binari variabl base kernel appli sampl kernel combin weight latent variabl joint optim via margin maxim principl demonstr five benchmark data set propos algorithm consist outperform compar one literatur latent support vector machin multipl kernel learn infer,4,-1.5361727,-23.544388
Learning Temporal Dynamics of Behavior Propagation in Social Networks,"Jun Zhang, Chaokun Wang and Jianmin Wang","AI and the Web (AIW)
Applications (APP)","Temporal Dynamics
Social Influence
Behavior Propagation
Behavior Prediction
Social Networks","AIW: Exploiting Linked Open Data
AIW: Social networking and community identification
AIW: Web personalization and user modeling
AIW: Web-based recommendation systems
APP: Computational Social Science
APP: Social Networks","Social influence has been widely accepted to explain people's cascade behaviors and further utilized to benefit many applications. However, few of existing work studied the direct, microscopic and temporal impact of social influence on people's behaviors in detail. In this paper we engage in the investigation of behavior propagation based on social influence and its temporal dynamics over continuous time. We formalize the static behavior models including BP and IBP, and the discrete DBP and DIBP models. We introduce continuous-temporal functions (CTFs) to model the fully-continuous dynamic variance of social influence over time. Upon that we propose the continuous-temporal interest-aware behavior propagation model, called CIBP, and present effective inference algorithm. Experimental studies on real-world datasets evaluated the family of behavior propagation models (BPMs) and demonstrated the effectiveness of our proposed models.","Learning Temporal Dynamics of Behavior Propagation in Social Networks Social influence has been widely accepted to explain people's cascade behaviors and further utilized to benefit many applications. However, few of existing work studied the direct, microscopic and temporal impact of social influence on people's behaviors in detail. In this paper we engage in the investigation of behavior propagation based on social influence and its temporal dynamics over continuous time. We formalize the static behavior models including BP and IBP, and the discrete DBP and DIBP models. We introduce continuous-temporal functions (CTFs) to model the fully-continuous dynamic variance of social influence over time. Upon that we propose the continuous-temporal interest-aware behavior propagation model, called CIBP, and present effective inference algorithm. Experimental studies on real-world datasets evaluated the family of behavior propagation models (BPMs) and demonstrated the effectiveness of our proposed models. Temporal Dynamics
Social Influence
Behavior Propagation
Behavior Prediction
Social Networks",learn tempor dynam behavior propag social network social influenc wide accept explain peopl cascad behavior util benefit mani applic howev exist work studi direct microscop tempor impact social influenc peopl behavior detail paper engag investig behavior propag base social influenc tempor dynam continu time formal static behavior model includ bp ibp discret dbp dibp model introduc continuoustempor function ctfs model fullycontinu dynam varianc social influenc time upon propos continuoustempor interestawar behavior propag model call cibp present effect infer algorithm experiment studi realworld dataset evalu famili behavior propag model bpms demonstr effect propos model tempor dynam social influenc behavior propag behavior predict social network,0,13.95649,3.2208903
Delivering Guaranteed Display Ads under Reach and Frequency Requirements,"Ali Hojjat, John Turner, Suleyman Cetintas and Jian Yang","AI and the Web (AIW)
Applications (APP)
Heuristic Search and Optimization (HSO)
Planning and Scheduling (PS)","Online Advertising
Math programming
Optimization
Column Generation
Guaranteed Targeted Display Advertising
Reach
Frequency
Uniform Delivery","AIW: AI for web services: semantic descriptions, planning, matching, and coordination
APP: Other Applications
HSO: Optimization
PS: Deterministic Planning
PS: Planning (General/Other)","We propose a new idea in the allocation and serving of online advertising.  We show that by using predetermined fixed-length streams of ads (which we call patterns) to
serve advertising, we can incorporate a variety of interesting features into the ad allocation optimization problem.  In particular, our formulation optimizes for representativeness as well as user-level diversity and pacing of ads, under
reach and frequency requirements.  We show how the problem can be solved efficiently using a column generation scheme in which only a small set of best patterns are kept in the optimization problem.  Our numerical tests show that with parallelization of the pattern generation process, the algorithm has a promising run time and memory usage.","Delivering Guaranteed Display Ads under Reach and Frequency Requirements We propose a new idea in the allocation and serving of online advertising.  We show that by using predetermined fixed-length streams of ads (which we call patterns) to
serve advertising, we can incorporate a variety of interesting features into the ad allocation optimization problem.  In particular, our formulation optimizes for representativeness as well as user-level diversity and pacing of ads, under
reach and frequency requirements.  We show how the problem can be solved efficiently using a column generation scheme in which only a small set of best patterns are kept in the optimization problem.  Our numerical tests show that with parallelization of the pattern generation process, the algorithm has a promising run time and memory usage. Online Advertising
Math programming
Optimization
Column Generation
Guaranteed Targeted Display Advertising
Reach
Frequency
Uniform Delivery",deliv guarante display ad reach frequenc requir propos new idea alloc serv onlin advertis show use predetermin fixedlength stream ad call pattern serv advertis incorpor varieti interest featur ad alloc optim problem particular formul optim repres well userlevel divers pace ad reach frequenc requir show problem solv effici use column generat scheme small set best pattern kept optim problem numer test show parallel pattern generat process algorithm promis run time memori usag onlin advertis math program optim column generat guarante target display advertis reach frequenc uniform deliveri,4,11.132468,9.844902
Video Recovery via Low-Rank Tensor Completion with Spatio-Temporal Consistency,"Hua Wang, Feiping Nie and Heng Huang",Vision (VIS),"Video completion
Tensor completion
Augmented Langrange Multiplier Method",VIS: Videos,"Video completion is a computer vision technique to recover the missing values in video sequences by filling the unknown regions with the known information. In recent research, tensor completion, a generalization of matrix completion for higher order data, emerges as a new solution to estimate the missing information in video with the assumption that the video frames are homogenous and correlated. However, each video clip often stores the heterogeneous episodes and the correlations among all video frames are not high. Thus, the regular tenor completion methods are not suitable to recover the video missing values in practical applications.

To solve this problem, we propose a novel spatially-temporally consistent tensor completion method for recovering the video missing data. Instead of minimizing the average of the trace norms of all matrices unfolded along each mode in a tensor data, we introduce a new smoothness regularization along video time direction to utilize the temporal information between consecutive video frames. Meanwhile, we also minimize the trace norm of each individual video frame to employ the spatial correlations among pixels. Different to previous tensor completion approaches, our new method can keep the spatio-temporal consistency in video and do not assume the global correlation in video frames. Thus, the proposed method can be applied to the general and practical video completion applications. Our method shows promising results in all evaluations on 3D biomedical image sequence and video benchmark data sets.","Video Recovery via Low-Rank Tensor Completion with Spatio-Temporal Consistency Video completion is a computer vision technique to recover the missing values in video sequences by filling the unknown regions with the known information. In recent research, tensor completion, a generalization of matrix completion for higher order data, emerges as a new solution to estimate the missing information in video with the assumption that the video frames are homogenous and correlated. However, each video clip often stores the heterogeneous episodes and the correlations among all video frames are not high. Thus, the regular tenor completion methods are not suitable to recover the video missing values in practical applications.

To solve this problem, we propose a novel spatially-temporally consistent tensor completion method for recovering the video missing data. Instead of minimizing the average of the trace norms of all matrices unfolded along each mode in a tensor data, we introduce a new smoothness regularization along video time direction to utilize the temporal information between consecutive video frames. Meanwhile, we also minimize the trace norm of each individual video frame to employ the spatial correlations among pixels. Different to previous tensor completion approaches, our new method can keep the spatio-temporal consistency in video and do not assume the global correlation in video frames. Thus, the proposed method can be applied to the general and practical video completion applications. Our method shows promising results in all evaluations on 3D biomedical image sequence and video benchmark data sets. Video completion
Tensor completion
Augmented Langrange Multiplier Method",video recoveri via lowrank tensor complet spatiotempor consist video complet comput vision techniqu recov miss valu video sequenc fill unknown region known inform recent research tensor complet general matrix complet higher order data emerg new solut estim miss inform video assumpt video frame homogen correl howev video clip often store heterogen episod correl among video frame high thus regular tenor complet method suitabl recov video miss valu practic applic solv problem propos novel spatiallytempor consist tensor complet method recov video miss data instead minim averag trace norm matric unfold along mode tensor data introduc new smooth regular along video time direct util tempor inform consecut video frame meanwhil also minim trace norm individu video frame employ spatial correl among pixel differ previous tensor complet approach new method keep spatiotempor consist video assum global correl video frame thus propos method appli general practic video complet applic method show promis result evalu 3d biomed imag sequenc video benchmark data set video complet tensor complet augment langrang multipli method,7,-21.698544,1.2272301
Robust Multi-View Spectral Clustering via Low-Rank and Sparse Decomposition,"Rongkai Xia, Yan Pan, Lei Du and Jian Yin",Novel Machine Learning Algorithms (NMLA),"multi-view clustering
spectral clustering
low-rank matrices
Markov chains
Augmented Lagrangian Multiplier method",NMLA: Clustering,"Multi-view clustering, which seeks a partition of the data in
multiple views that often provide complementary information to each
other, has received considerable attention in recent years. In real
life clustering problems, the data in each view may have
considerable noise. However, existing clustering methods blindly
combine the information from multi-view data with possibly
considerable noise, which often degrades their performance. In this
paper, we propose a novel Markov chain method for \textit{Robust
Multi-view Spectral Clustering} (RMSC). Our method has a flavor of
low-rank and sparse decomposition, where we firstly construct a
transition probability matrix from each single view, and then use
these matrices to recover a shared low-rank transition probability
matrix as a crucial input to the standard Markov chain method
for clustering. The optimization problem of RMSC has a low-rank
constraint on the transition probability matrix, and simultaneously
a probabilistic simplex constraint on each of its rows. To solve
this challenging optimization problem, we propose an optimization procedure
based on the Augmented Lagrangian Multiplier scheme. Experimental
results on various real world datasets show that the
proposed method has superior performance over several
state-of-the-art methods for multi-view clustering.","Robust Multi-View Spectral Clustering via Low-Rank and Sparse Decomposition Multi-view clustering, which seeks a partition of the data in
multiple views that often provide complementary information to each
other, has received considerable attention in recent years. In real
life clustering problems, the data in each view may have
considerable noise. However, existing clustering methods blindly
combine the information from multi-view data with possibly
considerable noise, which often degrades their performance. In this
paper, we propose a novel Markov chain method for \textit{Robust
Multi-view Spectral Clustering} (RMSC). Our method has a flavor of
low-rank and sparse decomposition, where we firstly construct a
transition probability matrix from each single view, and then use
these matrices to recover a shared low-rank transition probability
matrix as a crucial input to the standard Markov chain method
for clustering. The optimization problem of RMSC has a low-rank
constraint on the transition probability matrix, and simultaneously
a probabilistic simplex constraint on each of its rows. To solve
this challenging optimization problem, we propose an optimization procedure
based on the Augmented Lagrangian Multiplier scheme. Experimental
results on various real world datasets show that the
proposed method has superior performance over several
state-of-the-art methods for multi-view clustering. multi-view clustering
spectral clustering
low-rank matrices
Markov chains
Augmented Lagrangian Multiplier method",robust multiview spectral cluster via lowrank spars decomposit multiview cluster seek partit data multipl view often provid complementari inform receiv consider attent recent year real life cluster problem data view may consider nois howev exist cluster method blind combin inform multiview data possibl consider nois often degrad perform paper propos novel markov chain method textitrobust multiview spectral cluster rmsc method flavor lowrank spars decomposit first construct transit probabl matrix singl view use matric recov share lowrank transit probabl matrix crucial input standard markov chain method cluster optim problem rmsc lowrank constraint transit probabl matrix simultan probabilist simplex constraint row solv challeng optim problem propos optim procedur base augment lagrangian multipli scheme experiment result various real world dataset show propos method superior perform sever stateoftheart method multiview cluster multiview cluster spectral cluster lowrank matric markov chain augment lagrangian multipli method,7,-5.0351233,-16.403715
Ranking Tweets by Labeled and Collaboratively Selected Pairs with Transitive Closure,"Shenghua Liu, Xueqi Cheng and Fangtao Li","Heuristic Search and Optimization (HSO)
Machine Learning Applications (MLA)
NLP and Machine Learning (NLPML)
Novel Machine Learning Algorithms (NMLA)","Microblog search
semi-supervised learning
transitive closure
learning to rank
SVM","APP: Social Networks
NMLA: Semisupervised Learning","Tweets ranking is important for information acquisition in Microblog. Due to the content sparsity and lack of labeled data, it is better to employ semi-supervised learning methods to utilize the unlabeled data. However, most of previous semi-supervised learning methods do not consider the pair conflict problem, which means that the new selected unlabeled data may conflict with the labeled and previously selected data. It will hurt the learning performance a lot, if the training data contains many conflict pairs. In this paper, we propose a new collaborative semi-supervised SVM ranking model (CSR-TC) with consideration of the order conflict. The unlabeled data is selected based on a dynamically maintained transitive closure graph to avoid pair conflict. We also investigate the two views of features, intrinsic and content-relevant features, for the proposed model. Extensive experiments are conducted on TREC Microblogging corpus. The results demonstrate that our proposed method achieves significant improvement, compared to several state-of-the-art models.","Ranking Tweets by Labeled and Collaboratively Selected Pairs with Transitive Closure Tweets ranking is important for information acquisition in Microblog. Due to the content sparsity and lack of labeled data, it is better to employ semi-supervised learning methods to utilize the unlabeled data. However, most of previous semi-supervised learning methods do not consider the pair conflict problem, which means that the new selected unlabeled data may conflict with the labeled and previously selected data. It will hurt the learning performance a lot, if the training data contains many conflict pairs. In this paper, we propose a new collaborative semi-supervised SVM ranking model (CSR-TC) with consideration of the order conflict. The unlabeled data is selected based on a dynamically maintained transitive closure graph to avoid pair conflict. We also investigate the two views of features, intrinsic and content-relevant features, for the proposed model. Extensive experiments are conducted on TREC Microblogging corpus. The results demonstrate that our proposed method achieves significant improvement, compared to several state-of-the-art models. Microblog search
semi-supervised learning
transitive closure
learning to rank
SVM",rank tweet label collabor select pair transit closur tweet rank import inform acquisit microblog due content sparsiti lack label data better employ semisupervis learn method util unlabel data howev previous semisupervis learn method consid pair conflict problem mean new select unlabel data may conflict label previous select data hurt learn perform lot train data contain mani conflict pair paper propos new collabor semisupervis svm rank model csrtc consider order conflict unlabel data select base dynam maintain transit closur graph avoid pair conflict also investig two view featur intrins contentrelev featur propos model extens experi conduct trec microblog corpus result demonstr propos method achiev signific improv compar sever stateoftheart model microblog search semisupervis learn transit closur learn rank svm,6,-8.493017,-18.528929
Globally and Locally Consistent Unsupervised Projection,"Hua Wang, Feiping Nie and Heng Huang",Novel Machine Learning Algorithms (NMLA),"Unsupervised learning
Dimension reduction
L1-norm minimization and maximization",NMLA: Unsupervised Learning (Other),"In this paper, we propose an unsupervised projection method for feature extraction to preserve both global and local consistencies of the input data in the projected space. Traditional unsupervised feature extraction methods, such as principal component analysis and locality preserving projections, can only explore either the global or local geometric structures of the input data, but not the both. In our new method, we introduce a new measurement using the neighborhood data variances to assess the data locality, by which we propose to learn an optimal projection by rewarding both the global and local structures of the input data. Moreover, to improve the robustness of the proposed learning model against outlier data samples and outlier features, which is of particular importance in unsupervised learning, we propose a new objective that simultaneously minimizes and maximizes (minmax) the L1-norm distances instead of the traditional squared L2-norm distances. Solving the formulated optimization problem is very challenging, because it minimizes and maximizes a number of non-smooth L1- norm terms at the same time. In this paper, as an important theoretical contribution, we propose a simple yet effective optimization method to solve the L1-norm minmax problem, and theoretically prove its convergence and correctness. To the best of our knowledge, our paper makes the first attempt to solve the general L1-norm minmax problem with orthogonal constraints. Extensive experiments have been performed on six benchmark data sets, where the promising results validate the proposed method.","Globally and Locally Consistent Unsupervised Projection In this paper, we propose an unsupervised projection method for feature extraction to preserve both global and local consistencies of the input data in the projected space. Traditional unsupervised feature extraction methods, such as principal component analysis and locality preserving projections, can only explore either the global or local geometric structures of the input data, but not the both. In our new method, we introduce a new measurement using the neighborhood data variances to assess the data locality, by which we propose to learn an optimal projection by rewarding both the global and local structures of the input data. Moreover, to improve the robustness of the proposed learning model against outlier data samples and outlier features, which is of particular importance in unsupervised learning, we propose a new objective that simultaneously minimizes and maximizes (minmax) the L1-norm distances instead of the traditional squared L2-norm distances. Solving the formulated optimization problem is very challenging, because it minimizes and maximizes a number of non-smooth L1- norm terms at the same time. In this paper, as an important theoretical contribution, we propose a simple yet effective optimization method to solve the L1-norm minmax problem, and theoretically prove its convergence and correctness. To the best of our knowledge, our paper makes the first attempt to solve the general L1-norm minmax problem with orthogonal constraints. Extensive experiments have been performed on six benchmark data sets, where the promising results validate the proposed method. Unsupervised learning
Dimension reduction
L1-norm minimization and maximization",global local consist unsupervis project paper propos unsupervis project method featur extract preserv global local consist input data project space tradit unsupervis featur extract method princip compon analysi local preserv project explor either global local geometr structur input data new method introduc new measur use neighborhood data varianc assess data local propos learn optim project reward global local structur input data moreov improv robust propos learn model outlier data sampl outlier featur particular import unsupervis learn propos new object simultan minim maxim minmax l1norm distanc instead tradit squar l2norm distanc solv formul optim problem challeng minim maxim number nonsmooth l1 norm term time paper import theoret contribut propos simpl yet effect optim method solv l1norm minmax problem theoret prove converg correct best knowledg paper make first attempt solv general l1norm minmax problem orthogon constraint extens experi perform six benchmark data set promis result valid propos method unsupervis learn dimens reduct l1norm minim maxim,5,7.4137664,-10.999998
On the Incompatibility of Efficiency and Strategyproofness in Randomized Social Choice,"Haris Aziz, Florian Brandl and Felix Brandt",Game Theory and Economic Paradigms (GTEP),"social decision schemes
lotteries
efficiency
strategyproofness
randomized social choice","GTEP: Game Theory
GTEP: Social Choice / Voting","Efficiency--no agent can be made better off without making another one worse off--and strategyproofness--no agent can obtain a more preferred outcome by misrepresenting his preferences--are two cornerstones of economics and ubiquitous in important areas such as voting, auctions, or matching markets. Within the context of randomized social choice, Bogomolnaia and Moulin have shown that two particular notions of efficiency and strategyproofness based on stochastic dominance are incompatible. However, there are various other possibilities of lifting preferences over alternatives to preferences over lotteries apart from stochastic dominance. In this paper, we give an overview of common preference extensions, propose two new ones, and show that the above-mentioned incompatibility can be extended to various other notions of strategyproofness and efficiency.","On the Incompatibility of Efficiency and Strategyproofness in Randomized Social Choice Efficiency--no agent can be made better off without making another one worse off--and strategyproofness--no agent can obtain a more preferred outcome by misrepresenting his preferences--are two cornerstones of economics and ubiquitous in important areas such as voting, auctions, or matching markets. Within the context of randomized social choice, Bogomolnaia and Moulin have shown that two particular notions of efficiency and strategyproofness based on stochastic dominance are incompatible. However, there are various other possibilities of lifting preferences over alternatives to preferences over lotteries apart from stochastic dominance. In this paper, we give an overview of common preference extensions, propose two new ones, and show that the above-mentioned incompatibility can be extended to various other notions of strategyproofness and efficiency. social decision schemes
lotteries
efficiency
strategyproofness
randomized social choice",incompat effici strategyproof random social choic efficiencyno agent made better without make anoth one wors offand strategyproofnessno agent obtain prefer outcom misrepres preferencesar two cornerston econom ubiquit import area vote auction match market within context random social choic bogomolnaia moulin shown two particular notion effici strategyproof base stochast domin incompat howev various possibl lift prefer altern prefer lotteri apart stochast domin paper give overview common prefer extens propos two new one show abovement incompat extend various notion strategyproof effici social decis scheme lotteri effici strategyproof random social choic,9,17.878592,1.0551071
Context-aware Collaborative Topic Regression with Social Matrix Factorization for Recommender Systems,"Chaochao Chen, Xiaolin Zheng, Yan Wang, Fuxing Hong and Zhen Lin",AI and the Web (AIW),"Context-awareness
Topic modeling
Matrix Factorization
Social Networks",AIW: Web-based recommendation systems,"Online social networking sites have become popular platforms on which users can link with each other and share information, not only basic rating information but also information such as contexts, social relationships, and item contents. However, as far as we know, no existing works systematically combine diverse types of information to build more accurate recommender systems. In this paper, we propose a novel context-aware hierarchical Bayesian method. First, we propose the use of spectral clustering for user-item subgrouping, so that users and items in similar contexts are grouped. We then propose a novel hierarchical Bayesian model that can make predictions for each user-item subgroup, our model incorporate not only topic modeling to mine item content but also social matrix factorization to handle ratings and social relationships. Experiments on an Epinions dataset show that our method significantly improves recommendation performance compared with six categories of state-of-the-art recommendation methods in terms of both prediction accuracy and recall. We have also conducted experiments to study the extent to which ratings, contexts, social relationships, and item contents contribute to recommendation performance in terms of prediction accuracy and recall.","Context-aware Collaborative Topic Regression with Social Matrix Factorization for Recommender Systems Online social networking sites have become popular platforms on which users can link with each other and share information, not only basic rating information but also information such as contexts, social relationships, and item contents. However, as far as we know, no existing works systematically combine diverse types of information to build more accurate recommender systems. In this paper, we propose a novel context-aware hierarchical Bayesian method. First, we propose the use of spectral clustering for user-item subgrouping, so that users and items in similar contexts are grouped. We then propose a novel hierarchical Bayesian model that can make predictions for each user-item subgroup, our model incorporate not only topic modeling to mine item content but also social matrix factorization to handle ratings and social relationships. Experiments on an Epinions dataset show that our method significantly improves recommendation performance compared with six categories of state-of-the-art recommendation methods in terms of both prediction accuracy and recall. We have also conducted experiments to study the extent to which ratings, contexts, social relationships, and item contents contribute to recommendation performance in terms of prediction accuracy and recall. Context-awareness
Topic modeling
Matrix Factorization
Social Networks",contextawar collabor topic regress social matrix factor recommend system onlin social network site becom popular platform user link share inform basic rate inform also inform context social relationship item content howev far know exist work systemat combin divers type inform build accur recommend system paper propos novel contextawar hierarch bayesian method first propos use spectral cluster useritem subgroup user item similar context group propos novel hierarch bayesian model make predict useritem subgroup model incorpor topic model mine item content also social matrix factor handl rate social relationship experi epinion dataset show method signific improv recommend perform compar six categori stateoftheart recommend method term predict accuraci recal also conduct experi studi extent rate context social relationship item content contribut recommend perform term predict accuraci recal contextawar topic model matrix factor social network,0,15.87639,7.325821
Deep Salience: Visual Salience Modeling via Deep Belief Propagation,Richard Jiang,"Robotics (ROB)
Vision (VIS)","Visual Salience
Gaze Modelling
Deep Belief Propagation
Random Field","CM: Cognitive Architectures
ROB: Cognitive Robotics
VIS: Object Detection
VIS: Perception","Visual salience has been considered as an intriguing phenomenon observed in biologic neural systems. Numerous efforts have been made on mathematical modeling of visual salience using various feature contrasts, either locally or at global range. However, these algorithmic models treat this biologic phenomenon more like a mathematic problem and somehow ignores its biological instinct that visual salience arouses from the deep propagation of visual stimuli along the visual cortex. In this paper, we present a Deep Salience model that emulates this bio-inspired task, where a multi-layer successive Markov random fields (sMRF) is proposed to analyze the input image successively through its deep belief propagation. As its outcome, the foreground object can be automatically separated from the background in a fully unsupervised way. Experimental evaluation on benchmark datasets validated that our model can consistently outperform state-of-the-art salience models, yielding the highest recall rates, precision and F-measure scores in object detection. With this experimental validation, it is shown that the proposed bio-plausible deep belief network as an emulation of successive visual signal propagation along human visual cortex can functionally work well on solving real-world computational problems.","Deep Salience: Visual Salience Modeling via Deep Belief Propagation Visual salience has been considered as an intriguing phenomenon observed in biologic neural systems. Numerous efforts have been made on mathematical modeling of visual salience using various feature contrasts, either locally or at global range. However, these algorithmic models treat this biologic phenomenon more like a mathematic problem and somehow ignores its biological instinct that visual salience arouses from the deep propagation of visual stimuli along the visual cortex. In this paper, we present a Deep Salience model that emulates this bio-inspired task, where a multi-layer successive Markov random fields (sMRF) is proposed to analyze the input image successively through its deep belief propagation. As its outcome, the foreground object can be automatically separated from the background in a fully unsupervised way. Experimental evaluation on benchmark datasets validated that our model can consistently outperform state-of-the-art salience models, yielding the highest recall rates, precision and F-measure scores in object detection. With this experimental validation, it is shown that the proposed bio-plausible deep belief network as an emulation of successive visual signal propagation along human visual cortex can functionally work well on solving real-world computational problems. Visual Salience
Gaze Modelling
Deep Belief Propagation
Random Field",deep salienc visual salienc model via deep belief propag visual salienc consid intrigu phenomenon observ biolog neural system numer effort made mathemat model visual salienc use various featur contrast either local global rang howev algorithm model treat biolog phenomenon like mathemat problem somehow ignor biolog instinct visual salienc arous deep propag visual stimuli along visual cortex paper present deep salienc model emul bioinspir task multilay success markov random field smrf propos analyz input imag success deep belief propag outcom foreground object automat separ background fulli unsupervis way experiment evalu benchmark dataset valid model consist outperform stateoftheart salienc model yield highest recal rate precis fmeasur score object detect experiment valid shown propos bioplaus deep belief network emul success visual signal propag along human visual cortex function work well solv realworld comput problem visual salienc gaze model deep belief propag random field,0,6.9103274,-6.206406
A Local Non-negative Pursuit Method for Intrinsic Manifold Structure Preservation,"Dongdong Chen, Jian Cheng Lv and Yi Zhang",Novel Machine Learning Algorithms (NMLA),"neighborhood selection
non-negative representation learning
intrinsic manifold structure preservation","NMLA: Clustering
NMLA: Dimension Reduction/Feature Selection
NMLA: Unsupervised Learning (Other)","The local neighborhood  selection plays a crucial role for most representation based manifold learning algorithms. This paper reveals that an improper selection of neighborhood for learning representation will introduce negative components in the learnt representations. Importantly, the representations with negative components will affect the intrinsic manifold structure preservation. In this paper, a local non-negative pursuit (LNP) method is proposed for neighborhood selection and non-negative representations are learnt. Moreover, it is proved that the learnt representations are sparse and convex. Theoretical analysis and experimental results show that the proposed method achieves or outperforms the state-of-the-art results on various manifold learning problems.","A Local Non-negative Pursuit Method for Intrinsic Manifold Structure Preservation The local neighborhood  selection plays a crucial role for most representation based manifold learning algorithms. This paper reveals that an improper selection of neighborhood for learning representation will introduce negative components in the learnt representations. Importantly, the representations with negative components will affect the intrinsic manifold structure preservation. In this paper, a local non-negative pursuit (LNP) method is proposed for neighborhood selection and non-negative representations are learnt. Moreover, it is proved that the learnt representations are sparse and convex. Theoretical analysis and experimental results show that the proposed method achieves or outperforms the state-of-the-art results on various manifold learning problems. neighborhood selection
non-negative representation learning
intrinsic manifold structure preservation",local nonneg pursuit method intrins manifold structur preserv local neighborhood select play crucial role represent base manifold learn algorithm paper reveal improp select neighborhood learn represent introduc negat compon learnt represent import represent negat compon affect intrins manifold structur preserv paper local nonneg pursuit lnp method propos neighborhood select nonneg represent learnt moreov prove learnt represent spars convex theoret analysi experiment result show propos method achiev outperform stateoftheart result various manifold learn problem neighborhood select nonneg represent learn intrins manifold structur preserv,1,6.8077383,-11.018115
Regret-based Optimization and Preference Elicitation for Stackelberg Security Games with Uncertainty,"Thanh Nguyen, Amulya Yadav, Bo An, Milind Tambe and Craig Boutilier",Game Theory and Economic Paradigms (GTEP),"security game
robust optimization
minimax regret
preference elicitation
payoff uncertainty",GTEP: Game Theory,"Stackelberg security games (SSGs) have been deployed a number of real-world security domains. One key challenge in these applications is the assessment of attacker payoffs which may not be perfectly known. Previous work has studied SSGs with uncertain payoffs modeled by interval uncertainty and maximin-based robust optimization. In contrast, this paper is the first to propose the use of the less conservative minimax regret as a decision criterion for payoff-uncertain SSGs
and to present several algorithms for computing minimax regret for such games. This paper also for the first time addresses the challenge of preference elicitation in SSGs, providing novel regret-based solution strategies. Experimental results validate the runtime performance and solution quality of our approaches.","Regret-based Optimization and Preference Elicitation for Stackelberg Security Games with Uncertainty Stackelberg security games (SSGs) have been deployed a number of real-world security domains. One key challenge in these applications is the assessment of attacker payoffs which may not be perfectly known. Previous work has studied SSGs with uncertain payoffs modeled by interval uncertainty and maximin-based robust optimization. In contrast, this paper is the first to propose the use of the less conservative minimax regret as a decision criterion for payoff-uncertain SSGs
and to present several algorithms for computing minimax regret for such games. This paper also for the first time addresses the challenge of preference elicitation in SSGs, providing novel regret-based solution strategies. Experimental results validate the runtime performance and solution quality of our approaches. security game
robust optimization
minimax regret
preference elicitation
payoff uncertainty",regretbas optim prefer elicit stackelberg secur game uncertainti stackelberg secur game ssgs deploy number realworld secur domain one key challeng applic assess attack payoff may perfect known previous work studi ssgs uncertain payoff model interv uncertainti maximinbas robust optim contrast paper first propos use less conserv minimax regret decis criterion payoffuncertain ssgs present sever algorithm comput minimax regret game paper also first time address challeng prefer elicit ssgs provid novel regretbas solut strategi experiment result valid runtim perform solut qualiti approach secur game robust optim minimax regret prefer elicit payoff uncertainti,2,4.2902083,21.033245
Adding Local Exploration to Greedy Best-First Search for Satisficing Planning,"Fan Xie, Martin Mueller and Robert Holte","Heuristic Search and Optimization (HSO)
Planning and Scheduling (PS)","Heuristic Search
Satisficing Planning
Greedy Best First Search","HSO: Heuristic Search
HSO: Search (General/Other)
PS: Deterministic Planning","Greedy Best-First Search (GBFS) is a powerful algorithm at the heart of many state of the art satisficing planners. One major weakness of GBFS is its behavior in so-called uninformative heuristic regions (UHR) - parts of the search space in which no heuristic provides guidance towards states with improved heuristic values. In such regions, GBFS degenerates into an inefficient breadth-first type search.
This work analyzes the problem of UHR in planning in detail, and proposes a two level search framework as a solution. In Greedy Best-First Search with Local Exploration (GBFS-LE), local exploration is started from within a global GBFS whenever the search seems stuck in UHRs.

Two different local exploration strategies are developed and evaluated experimentally: Local GBFS (LS) and Local Random Walk Search (LRW). The two new planners LAMA-LS and LAMA-LRW integrate these strategies into the GBFS component of LAMA-2011. Both are shown to yield clear improvements in terms of both coverage and search time on standard International Planning Competition benchmarks, especially for domains that are proven to have large or unbounded UHRs.","Adding Local Exploration to Greedy Best-First Search for Satisficing Planning Greedy Best-First Search (GBFS) is a powerful algorithm at the heart of many state of the art satisficing planners. One major weakness of GBFS is its behavior in so-called uninformative heuristic regions (UHR) - parts of the search space in which no heuristic provides guidance towards states with improved heuristic values. In such regions, GBFS degenerates into an inefficient breadth-first type search.
This work analyzes the problem of UHR in planning in detail, and proposes a two level search framework as a solution. In Greedy Best-First Search with Local Exploration (GBFS-LE), local exploration is started from within a global GBFS whenever the search seems stuck in UHRs.

Two different local exploration strategies are developed and evaluated experimentally: Local GBFS (LS) and Local Random Walk Search (LRW). The two new planners LAMA-LS and LAMA-LRW integrate these strategies into the GBFS component of LAMA-2011. Both are shown to yield clear improvements in terms of both coverage and search time on standard International Planning Competition benchmarks, especially for domains that are proven to have large or unbounded UHRs. Heuristic Search
Satisficing Planning
Greedy Best First Search",ad local explor greedi bestfirst search satisf plan greedi bestfirst search gbfs power algorithm heart mani state art satisf planner one major weak gbfs behavior socal uninform heurist region uhr part search space heurist provid guidanc toward state improv heurist valu region gbfs degener ineffici breadthfirst type search work analyz problem uhr plan detail propos two level search framework solut greedi bestfirst search local explor gbfsle local explor start within global gbfs whenev search seem stuck uhr two differ local explor strategi develop evalu experiment local gbfs ls local random walk search lrw two new planner lamal lamalrw integr strategi gbfs compon lama2011 shown yield clear improv term coverag search time standard intern plan competit benchmark especi domain proven larg unbound uhr heurist search satisf plan greedi best first search,5,-14.891597,7.5551186
Pre-trained Multi-view Word Embedding using Two-side Neural Network,"Yong Luo, Jian Tang, Jun Yan, Chao Xu and Zheng Chen","AI and the Web (AIW)
Novel Machine Learning Algorithms (NMLA)","word embedding
neural network
pre-train
multiple data sources","AIW: Machine learning and the web
NMLA: Neural Networks/Deep Learning","Word embedding aims to learn a continuous representation for each word. It attracts increasing attention due to its effectiveness in various tasks such as named entity recognition and language modeling. Most existing word embedding results are generally trained on one individual data source such as news pages or Wikipedia articles. However, when we apply them to other tasks such as web search, the performance suffers. To obtain a robust word embedding for different applications, multiple data sources could be leveraged. In this paper, we proposed a two-side multimodal neural network to learn a robust word embedding from multiple data sources including free text, user search queries and search click-through data. This framework takes the word embeddings learned from different data sources as pre-train, and then uses a two-side neural network to unify these embeddings. The pre-trained embeddings are obtained by adapting the recently proposed CBOW algorithm. Since the proposed neural network does not need to re-train word embeddings for a new task, it is highly scalable in real world problem solving. Besides, the network allows weighting different sources differently when applied to different application tasks. Experiments on two real-world applications including web search ranking and word similarity measuring show that our neural network with multiple sources outperforms state-of-the-art word embedding algorithm with each individual source. It also outperforms other competitive baselines using multiple sources.","Pre-trained Multi-view Word Embedding using Two-side Neural Network Word embedding aims to learn a continuous representation for each word. It attracts increasing attention due to its effectiveness in various tasks such as named entity recognition and language modeling. Most existing word embedding results are generally trained on one individual data source such as news pages or Wikipedia articles. However, when we apply them to other tasks such as web search, the performance suffers. To obtain a robust word embedding for different applications, multiple data sources could be leveraged. In this paper, we proposed a two-side multimodal neural network to learn a robust word embedding from multiple data sources including free text, user search queries and search click-through data. This framework takes the word embeddings learned from different data sources as pre-train, and then uses a two-side neural network to unify these embeddings. The pre-trained embeddings are obtained by adapting the recently proposed CBOW algorithm. Since the proposed neural network does not need to re-train word embeddings for a new task, it is highly scalable in real world problem solving. Besides, the network allows weighting different sources differently when applied to different application tasks. Experiments on two real-world applications including web search ranking and word similarity measuring show that our neural network with multiple sources outperforms state-of-the-art word embedding algorithm with each individual source. It also outperforms other competitive baselines using multiple sources. word embedding
neural network
pre-train
multiple data sources",pretrain multiview word embed use twosid neural network word embed aim learn continu represent word attract increas attent due effect various task name entiti recognit languag model exist word embed result general train one individu data sourc news page wikipedia articl howev appli task web search perform suffer obtain robust word embed differ applic multipl data sourc could leverag paper propos twosid multimod neural network learn robust word embed multipl data sourc includ free text user search queri search clickthrough data framework take word embed learn differ data sourc pretrain use twosid neural network unifi embed pretrain embed obtain adapt recent propos cbow algorithm sinc propos neural network need retrain word embed new task high scalabl real world problem solv besid network allow weight differ sourc differ appli differ applic task experi two realworld applic includ web search rank word similar measur show neural network multipl sourc outperform stateoftheart word embed algorithm individu sourc also outperform competit baselin use multipl sourc word embed neural network pretrain multipl data sourc,6,11.420809,-9.127654
Trust Prediction with Propagation and Similarity Regularization,"Xiaoming Zheng, Yan Wang, Mehmet Orgun, Youliang Zhong and Guanfeng Liu","AI and the Web (AIW)
Applications (APP)","Trust prediction
Social network
Trust propagation
Trust tendency","AIW: Representing, reasoning, and using provenance, trust, privacy, and security on the web
APP: Social Networks","Online social networks have been used for a variety of rich activities in recent years, such as investigating potential employees or seeking recommendations of high quality services and service providers. In such activities trust is one of the most vital factors for users' decision-making. In the literature, the state-of-the-art trust prediction approaches either focus on dispositional bias and propagated trust value of the pair-wise trust relationship along a path or on the similarity of trust rating values.  However, another factor, the distribution of trust ratings, also affects the trust between users. In addition, bias, propagated trust and similarity are of different types,  but were treated the same. Therefore, how to utilize the factors needs further improvement. In this paper we propose a new trust prediction model based on trust decomposition and matrix factorization, considering all the above essential factors to predict the trust between two users who are not directly connected. In this model, we firstly decompose trust into biased trust and bias-reduced trust. Then based on bias-reduced trust ratings, matrix factorization with a similarity regularization term, which takes advantages of both users' rating habits and propagated trust, is proposed to predict missing trust values. In the end, the missing trust is recomposed with predicted trust values and bias. Experiments conducted on a real-world dataset illustrate significantly improved prediction accuracy over the state-of-the-art approaches.","Trust Prediction with Propagation and Similarity Regularization Online social networks have been used for a variety of rich activities in recent years, such as investigating potential employees or seeking recommendations of high quality services and service providers. In such activities trust is one of the most vital factors for users' decision-making. In the literature, the state-of-the-art trust prediction approaches either focus on dispositional bias and propagated trust value of the pair-wise trust relationship along a path or on the similarity of trust rating values.  However, another factor, the distribution of trust ratings, also affects the trust between users. In addition, bias, propagated trust and similarity are of different types,  but were treated the same. Therefore, how to utilize the factors needs further improvement. In this paper we propose a new trust prediction model based on trust decomposition and matrix factorization, considering all the above essential factors to predict the trust between two users who are not directly connected. In this model, we firstly decompose trust into biased trust and bias-reduced trust. Then based on bias-reduced trust ratings, matrix factorization with a similarity regularization term, which takes advantages of both users' rating habits and propagated trust, is proposed to predict missing trust values. In the end, the missing trust is recomposed with predicted trust values and bias. Experiments conducted on a real-world dataset illustrate significantly improved prediction accuracy over the state-of-the-art approaches. Trust prediction
Social network
Trust propagation
Trust tendency",trust predict propag similar regular onlin social network use varieti rich activ recent year investig potenti employe seek recommend high qualiti servic servic provid activ trust one vital factor user decisionmak literatur stateoftheart trust predict approach either focus disposit bias propag trust valu pairwis trust relationship along path similar trust rate valu howev anoth factor distribut trust rate also affect trust user addit bias propag trust similar differ type treat therefor util factor need improv paper propos new trust predict model base trust decomposit matrix factor consid essenti factor predict trust two user direct connect model first decompos trust bias trust biasreduc trust base biasreduc trust rate matrix factor similar regular term take advantag user rate habit propag trust propos predict miss trust valu end miss trust recompos predict trust valu bias experi conduct realworld dataset illustr signific improv predict accuraci stateoftheart approach trust predict social network trust propag trust tendenc,0,15.720243,-10.892364
Recommendation by Mining Multiple User Behaviors with Group Sparsity,"Ting Yuan, Jian Cheng, Xi Zhang, Shuang Qiu and Hanqing Lu",AI and the Web (AIW),"Recommender System
Collaborative Filtering
Matrix Factorization",AIW: Web-based recommendation systems,"Recently, some recommendation methods try to improve the prediction results by integrating information from user’s multiple types of behaviors. How to model the dependence and independence between different behaviors is critical for them. In this paper, we propose a novel recommendation model, the Group-Sparse Matrix Factorization (GSMF), which factorizes the rating matrices for multiple behaviors into the user and item latent factor space with group sparsity regularization. It can (1) select out the different subsets of latent factors for different behaviors, addressing that users’ decisions on different behaviors are determined by different sets of factors; (2) model the dependence and independence between behaviors by learning the shared and private factors for multiple behaviors automatically; (3) allow the shared factors between different behaviors to be different, instead of all the behaviors sharing the same set of factors. Experiments on the real-world dataset demonstrate that our model can integrate users’multiple types of behaviors into recommendation better, compared with other state-of-the-arts.","Recommendation by Mining Multiple User Behaviors with Group Sparsity Recently, some recommendation methods try to improve the prediction results by integrating information from user’s multiple types of behaviors. How to model the dependence and independence between different behaviors is critical for them. In this paper, we propose a novel recommendation model, the Group-Sparse Matrix Factorization (GSMF), which factorizes the rating matrices for multiple behaviors into the user and item latent factor space with group sparsity regularization. It can (1) select out the different subsets of latent factors for different behaviors, addressing that users’ decisions on different behaviors are determined by different sets of factors; (2) model the dependence and independence between behaviors by learning the shared and private factors for multiple behaviors automatically; (3) allow the shared factors between different behaviors to be different, instead of all the behaviors sharing the same set of factors. Experiments on the real-world dataset demonstrate that our model can integrate users’multiple types of behaviors into recommendation better, compared with other state-of-the-arts. Recommender System
Collaborative Filtering
Matrix Factorization",recommend mine multipl user behavior group sparsiti recent recommend method tri improv predict result integr inform user multipl type behavior model depend independ differ behavior critic paper propos novel recommend model groupspars matrix factor gsmf factor rate matric multipl behavior user item latent factor space group sparsiti regular 1 select differ subset latent factor differ behavior address user decis differ behavior determin differ set factor 2 model depend independ behavior learn share privat factor multipl behavior automat 3 allow share factor differ behavior differ instead behavior share set factor experi realworld dataset demonstr model integr users'multipl type behavior recommend better compar stateoftheart recommend system collabor filter matrix factor,0,15.836998,9.285224
Modal Ranking: A Uniquely Robust Voting Rule,"Ioannis Caragiannis, Ariel Procaccia and Nisarg Shah",Game Theory and Economic Paradigms (GTEP),"Computational social choice
Crowdsourcing
Noise models",GTEP: Social Choice / Voting,"Motivated by applications to crowdsourcing, we study voting rules that output a correct ranking of alternatives by quality from a large collection of noisy input rankings. We seek voting rules that are supremely robust to noise, in the sense of being correct in the face of any ""reasonable"" type of noise. We show that there is such a voting rule, which we call the modal ranking rule. Moreover, we establish that the modal ranking rule is the unique rule with the preceding robustness property within a large family of voting rules, which includes a slew of well-studied rules.","Modal Ranking: A Uniquely Robust Voting Rule Motivated by applications to crowdsourcing, we study voting rules that output a correct ranking of alternatives by quality from a large collection of noisy input rankings. We seek voting rules that are supremely robust to noise, in the sense of being correct in the face of any ""reasonable"" type of noise. We show that there is such a voting rule, which we call the modal ranking rule. Moreover, we establish that the modal ranking rule is the unique rule with the preceding robustness property within a large family of voting rules, which includes a slew of well-studied rules. Computational social choice
Crowdsourcing
Noise models",modal rank uniqu robust vote rule motiv applic crowdsourc studi vote rule output correct rank altern qualiti larg collect noisi input rank seek vote rule suprem robust nois sens correct face reason type nois show vote rule call modal rank rule moreov establish modal rank rule uniqu rule preced robust properti within larg famili vote rule includ slew wellstudi rule comput social choic crowdsourc nois model,9,16.549185,-17.3591
Proximal Iteratively Reweighted Algorithm with Multiple Splitting for Nonconvex Sparsity Optimization,"Canyi Lu, Yunchao Wei, Zhouchen Lin and Shuicheng Yan","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Nonconvex Sparsity Optimization
General iterative solver
multiple splitting for multi-variable problem",NMLA: Machine Learning (General/other),"This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm for solving a general problem, which involves a large body of nonconvex sparse and structured sparse related problems. Comparing with previous iterative solvers for nonconvex sparse problem, PIRE is much more general and efficient. The computational cost of PIRE in each iteration is usually as low as the state-of-the-art convex solvers. We further propose the PIRE algorithm with Parallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating (PIRE-AU) to handle the multi-variable problems. In theory, we prove that our proposed methods converge and any limit solution is a stationary point. Extensive experiments on both synthesis and real data sets demonstrate that our methods achieve comparative learning performance, but are much more efficient, by comparing with previous nonconvex solvers.","Proximal Iteratively Reweighted Algorithm with Multiple Splitting for Nonconvex Sparsity Optimization This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm for solving a general problem, which involves a large body of nonconvex sparse and structured sparse related problems. Comparing with previous iterative solvers for nonconvex sparse problem, PIRE is much more general and efficient. The computational cost of PIRE in each iteration is usually as low as the state-of-the-art convex solvers. We further propose the PIRE algorithm with Parallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating (PIRE-AU) to handle the multi-variable problems. In theory, we prove that our proposed methods converge and any limit solution is a stationary point. Extensive experiments on both synthesis and real data sets demonstrate that our methods achieve comparative learning performance, but are much more efficient, by comparing with previous nonconvex solvers. Nonconvex Sparsity Optimization
General iterative solver
multiple splitting for multi-variable problem",proxim iter reweight algorithm multipl split nonconvex sparsiti optim paper propos proxim iter reweight pire algorithm solv general problem involv larg bodi nonconvex spars structur spars relat problem compar previous iter solver nonconvex spars problem pire much general effici comput cost pire iter usual low stateoftheart convex solver propos pire algorithm parallel split pirep pire algorithm altern updat pireau handl multivari problem theori prove propos method converg limit solut stationari point extens experi synthesi real data set demonstr method achiev compar learn perform much effici compar previous nonconvex solver nonconvex sparsiti optim general iter solver multipl split multivari problem,4,3.0547287,-19.124512
Extending Tournament Solutions,"Felix Brandt, Markus Brill and Paul Harrenstein",Game Theory and Economic Paradigms (GTEP),"Social Choice Theory
Tournament Solutions
Possible Winners",GTEP: Social Choice / Voting,"An important subclass of social choice functions, so-called majoritarian (or C1) functions, only take into account the pairwise majority relation between alternatives. In the absence of majority ties--e.g., when there is an odd number of agents with linear preferences--the majority relation is antisymmetric and complete and can thus conveniently be represented by a tournament. Tournaments have a rich mathematical theory and many formal results for majoritarian functions assume that the majority relation constitutes a tournament. Moreover, most majoritarian functions have only been defined for tournaments and allow for a variety of generalizations to unrestricted preference profiles, none of which can be seen as the unequivocal extension of the original function. In this paper, we argue that restricting attention to tournaments is justified by the existence of a conservative extension, which inherits most of the commonly considered properties from its underlying tournament solution.","Extending Tournament Solutions An important subclass of social choice functions, so-called majoritarian (or C1) functions, only take into account the pairwise majority relation between alternatives. In the absence of majority ties--e.g., when there is an odd number of agents with linear preferences--the majority relation is antisymmetric and complete and can thus conveniently be represented by a tournament. Tournaments have a rich mathematical theory and many formal results for majoritarian functions assume that the majority relation constitutes a tournament. Moreover, most majoritarian functions have only been defined for tournaments and allow for a variety of generalizations to unrestricted preference profiles, none of which can be seen as the unequivocal extension of the original function. In this paper, we argue that restricting attention to tournaments is justified by the existence of a conservative extension, which inherits most of the commonly considered properties from its underlying tournament solution. Social Choice Theory
Tournament Solutions
Possible Winners",extend tournament solut import subclass social choic function socal majoritarian c1 function take account pairwis major relat altern absenc major tieseg odd number agent linear preferencesth major relat antisymmetr complet thus conveni repres tournament tournament rich mathemat theori mani formal result majoritarian function assum major relat constitut tournament moreov majoritarian function defin tournament allow varieti general unrestrict prefer profil none seen unequivoc extens origin function paper argu restrict attent tournament justifi exist conserv extens inherit common consid properti under tournament solut social choic theori tournament solut possibl winner,9,-15.496242,-3.7447033
Optimal Neighborhood Preserving Visualization by Maximum Satisfiability,"Kerstin Bunte, Matti Järvisalo, Jeremias Berg, Petri Myllymäki, Jaakko Peltonen and Samuel Kaski","Novel Machine Learning Algorithms (NMLA)
Search and Constraint Satisfaction (SCS)","visualization
boolean optimization
maximum satisfiability
nonlinear dimensionality reduction
neighbor embedding","NMLA: Dimension Reduction/Feature Selection
NMLA: Unsupervised Learning (Other)
SCS: Constraint Optimization
SCS: SAT and CSP: Modeling/Formulations","We present a novel approach to low-dimensional neighbor embedding for visualization: we formulate an information retrieval based neighborhood preservation cost function as Maximum satisfiability on a discretized output display, and maximize the number of clauses preserved. The method has a rigorous interpretation as optimal visualization for neighbor retrieval. Unlike previous low-dimensional neighbor embedding methods, our satisfiability formulation is guaranteed to yield a global optimum and does so reasonably fast. Unlike previous manifold learning methods yielding global optima of their cost functions, our cost function and method are designed for low-dimensional visualization where evaluation and minimization of visualization errors are crucial. Our method performs well in experiments, yielding clean embeddings of data sets where a state-of-the-art comparison method yields poor arrangements. In a real-world case study for semi-supervised WLAN positioning in buildings we outperform state-of-the-art methods, especially when having few measurements.","Optimal Neighborhood Preserving Visualization by Maximum Satisfiability We present a novel approach to low-dimensional neighbor embedding for visualization: we formulate an information retrieval based neighborhood preservation cost function as Maximum satisfiability on a discretized output display, and maximize the number of clauses preserved. The method has a rigorous interpretation as optimal visualization for neighbor retrieval. Unlike previous low-dimensional neighbor embedding methods, our satisfiability formulation is guaranteed to yield a global optimum and does so reasonably fast. Unlike previous manifold learning methods yielding global optima of their cost functions, our cost function and method are designed for low-dimensional visualization where evaluation and minimization of visualization errors are crucial. Our method performs well in experiments, yielding clean embeddings of data sets where a state-of-the-art comparison method yields poor arrangements. In a real-world case study for semi-supervised WLAN positioning in buildings we outperform state-of-the-art methods, especially when having few measurements. visualization
boolean optimization
maximum satisfiability
nonlinear dimensionality reduction
neighbor embedding",optim neighborhood preserv visual maximum satisfi present novel approach lowdimension neighbor embed visual formul inform retriev base neighborhood preserv cost function maximum satisfi discret output display maxim number claus preserv method rigor interpret optim visual neighbor retriev unlik previous lowdimension neighbor embed method satisfi formul guarante yield global optimum reason fast unlik previous manifold learn method yield global optima cost function cost function method design lowdimension visual evalu minim visual error crucial method perform well experi yield clean embed data set stateoftheart comparison method yield poor arrang realworld case studi semisupervis wlan posit build outperform stateoftheart method especi measur visual boolean optim maximum satisfi nonlinear dimension reduct neighbor embed,5,7.278406,-8.412217
Dynamic Bayesian Probabilistic Matrix Factorization,Sotirios Chatzis,"Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)","Probabilistic Matrix Factorization
Dynamic Hierarchical Dirichlet Process
Bayesian Nonparametrics
Collaborative Filtering","NMLA: Bayesian Learning
NMLA: Preferences/Ranking Learning
NMLA: Recommender Systems
RU: Probabilistic Inference
RU: Relational Probabilistic Models","Collaborative filtering algorithms generally rely on the assumption that user preference patterns remain stationary. However, real-world relational data are seldom stationary. User preference patterns may change over time, giving rise to the requirement of designing collaborative filtering systems capable of detecting and adapting to preference pattern shifts. Motivated by this observation, in this paper we propose a dynamic Bayesian probabilistic matrix factorization model, designed for modeling time-varying distributions. Formulation of our model is based on imposition of a dynamic hierarchical Dirichlet process (dHDP) prior over the space of probabilistic matrix factorization models to capture the time-evolving statistical properties of modeled sequential relational datasets. We develop a simple Markov Chain Monte Carlo sampler to perform inference. We present experimental results to demonstrate the superiority of our temporal model.","Dynamic Bayesian Probabilistic Matrix Factorization Collaborative filtering algorithms generally rely on the assumption that user preference patterns remain stationary. However, real-world relational data are seldom stationary. User preference patterns may change over time, giving rise to the requirement of designing collaborative filtering systems capable of detecting and adapting to preference pattern shifts. Motivated by this observation, in this paper we propose a dynamic Bayesian probabilistic matrix factorization model, designed for modeling time-varying distributions. Formulation of our model is based on imposition of a dynamic hierarchical Dirichlet process (dHDP) prior over the space of probabilistic matrix factorization models to capture the time-evolving statistical properties of modeled sequential relational datasets. We develop a simple Markov Chain Monte Carlo sampler to perform inference. We present experimental results to demonstrate the superiority of our temporal model. Probabilistic Matrix Factorization
Dynamic Hierarchical Dirichlet Process
Bayesian Nonparametrics
Collaborative Filtering",dynam bayesian probabilist matrix factor collabor filter algorithm general reli assumpt user prefer pattern remain stationari howev realworld relat data seldom stationari user prefer pattern may chang time give rise requir design collabor filter system capabl detect adapt prefer pattern shift motiv observ paper propos dynam bayesian probabilist matrix factor model design model timevari distribut formul model base imposit dynam hierarch dirichlet process dhdp prior space probabilist matrix factor model captur timeevolv statist properti model sequenti relat dataset develop simpl markov chain mont carlo sampler perform infer present experiment result demonstr superior tempor model probabilist matrix factor dynam hierarch dirichlet process bayesian nonparametr collabor filter,0,6.3221765,1.8993073
Echo-State Conditional Restricted Boltzmann Machines,Sotirios Chatzis,Novel Machine Learning Algorithms (NMLA),"Conditional Restricted Boltzmann Machine
Echo-State Network
Contrastive Divergence","NMLA: Neural Networks/Deep Learning
NMLA: Time-Series/Data Streams
NMLA: Structured Prediction","Restricted Boltzmann machines (RBMs) are a powerful generative modeling technique, based on a complex graphical model of hidden (latent) variables. Conditional RBMs (CRBMs) are an extension of RBMs tailored to modeling temporal data. A drawback of CRBMs is their consideration of linear temporal dependencies, which limits their capability to capture complex temporal structure. They also require many variables to model long temporal dependencies, a fact that might provoke overfitting proneness. To resolve these issues, in this paper we propose the echo-state CRBM (ES-CRBM): our model uses an echo-state network reservoir in the context of CRBMs to efficiently capture long and complex temporal dynamics, with much fewer trainable parameters compared to conventional CRBMs. In addition, we introduce an (implicit) mixture of ES-CRBM experts (im-ES-CRBM) to enhance even further the capabilities of our ES-CRBM model. The introduced im-ES-CRBM allows for better modeling temporal observations which might comprise a number of latent or observable subpatterns that alternate in a dynamic fashion. It also allows for performing sequence segmentation using our framework. We apply our methods to sequential data modeling and classification experiments using public datasets. As we show, our approach outperforms both existing RBM-based approaches as well as related state-of-the-art methods, such as conditional random fields.","Echo-State Conditional Restricted Boltzmann Machines Restricted Boltzmann machines (RBMs) are a powerful generative modeling technique, based on a complex graphical model of hidden (latent) variables. Conditional RBMs (CRBMs) are an extension of RBMs tailored to modeling temporal data. A drawback of CRBMs is their consideration of linear temporal dependencies, which limits their capability to capture complex temporal structure. They also require many variables to model long temporal dependencies, a fact that might provoke overfitting proneness. To resolve these issues, in this paper we propose the echo-state CRBM (ES-CRBM): our model uses an echo-state network reservoir in the context of CRBMs to efficiently capture long and complex temporal dynamics, with much fewer trainable parameters compared to conventional CRBMs. In addition, we introduce an (implicit) mixture of ES-CRBM experts (im-ES-CRBM) to enhance even further the capabilities of our ES-CRBM model. The introduced im-ES-CRBM allows for better modeling temporal observations which might comprise a number of latent or observable subpatterns that alternate in a dynamic fashion. It also allows for performing sequence segmentation using our framework. We apply our methods to sequential data modeling and classification experiments using public datasets. As we show, our approach outperforms both existing RBM-based approaches as well as related state-of-the-art methods, such as conditional random fields. Conditional Restricted Boltzmann Machine
Echo-State Network
Contrastive Divergence",echost condit restrict boltzmann machin restrict boltzmann machin rbms power generat model techniqu base complex graphic model hidden latent variabl condit rbms crbms extens rbms tailor model tempor data drawback crbms consider linear tempor depend limit capabl captur complex tempor structur also requir mani variabl model long tempor depend fact might provok overfit prone resolv issu paper propos echost crbm escrbm model use echost network reservoir context crbms effici captur long complex tempor dynam much fewer trainabl paramet compar convent crbms addit introduc implicit mixtur escrbm expert imescrbm enhanc even capabl escrbm model introduc imescrbm allow better model tempor observ might compris number latent observ subpattern altern dynam fashion also allow perform sequenc segment use framework appli method sequenti data model classif experi use public dataset show approach outperform exist rbmbase approach well relat stateoftheart method condit random field condit restrict boltzmann machin echost network contrast diverg,3,5.7659497,0.5215402
A Joint Optimization Model for Image Summarization Based on Image Content and Tags,"Hongliang Yu, Zhi-Hong Deng, Yunlun Yang and Tao Xiong","AI and the Web (AIW)
Machine Learning Applications (MLA)","image summarization
image tags
optimization model
similarity-inducing regularizer","AIW: AI for multimedia and multimodal web applications
MLA: Machine Learning Applications (General/other)","As an effective technology for navigating a large number of images, image summarization is becoming a promising task with the rapid development of image sharing sites and social networks. Most existing summarization approaches use the visual-based features for image representation without considering tag information. In this paper, we propose a novel framework, named JOINT, which employs both image content and tag information to summarize images. Our model generates the summary images which can best reconstruct the original collection. Based on the assumption that an image with representative content should also have typical tags, we introduce a similarity-inducing regularizer to our model. Furthermore, we impose the lasso penalty on the objective function to yield a concise summary set. Extensive experiments demonstrate our model outperforms the state-of-the-art approaches.","A Joint Optimization Model for Image Summarization Based on Image Content and Tags As an effective technology for navigating a large number of images, image summarization is becoming a promising task with the rapid development of image sharing sites and social networks. Most existing summarization approaches use the visual-based features for image representation without considering tag information. In this paper, we propose a novel framework, named JOINT, which employs both image content and tag information to summarize images. Our model generates the summary images which can best reconstruct the original collection. Based on the assumption that an image with representative content should also have typical tags, we introduce a similarity-inducing regularizer to our model. Furthermore, we impose the lasso penalty on the objective function to yield a concise summary set. Extensive experiments demonstrate our model outperforms the state-of-the-art approaches. image summarization
image tags
optimization model
similarity-inducing regularizer",joint optim model imag summar base imag content tag effect technolog navig larg number imag imag summar becom promis task rapid develop imag share site social network exist summar approach use visualbas featur imag represent without consid tag inform paper propos novel framework name joint employ imag content tag inform summar imag model generat summari imag best reconstruct origin collect base assumpt imag repres content also typic tag introduc similarityinduc regular model furthermor impos lasso penalti object function yield concis summari set extens experi demonstr model outperform stateoftheart approach imag summar imag tag optim model similarityinduc regular,1,-22.64539,-6.0535784
"A Computational Method for (MSS,CoMSS) Partitioning","Jean Marie Lagniez, Eric Gregoire and Bertrand Mazure","Heuristic Search and Optimization (HSO)
Search and Constraint Satisfaction (SCS)","SAT
CoMSS
MSS
Maximal Satisﬁable Subset","HSO: Search (General/Other)
SCS: Constraint Satisfaction
SCS: SAT and CSP: Solvers and Tools","MSS (Maximal Satisﬁable Subset) and CoMSS (also called Minimal Correction Subset) concepts play a key role in many A.I. approaches and techniques. In this paper, a novel algorithm for partitioning a Boolean CNF into one MSS and its corresponding CoMSS is introduced. Extensive empirical evaluation shows that it is more robust and more efﬁcient on most instances than currently available techniques.","A Computational Method for (MSS,CoMSS) Partitioning MSS (Maximal Satisﬁable Subset) and CoMSS (also called Minimal Correction Subset) concepts play a key role in many A.I. approaches and techniques. In this paper, a novel algorithm for partitioning a Boolean CNF into one MSS and its corresponding CoMSS is introduced. Extensive empirical evaluation shows that it is more robust and more efﬁcient on most instances than currently available techniques. SAT
CoMSS
MSS
Maximal Satisﬁable Subset",comput method msscomss partit mss maxim satisﬁ subset comss also call minim correct subset concept play key role mani ai approach techniqu paper novel algorithm partit boolean cnf one mss correspond comss introduc extens empir evalu show robust efﬁcient instanc current avail techniqu sat comss mss maxim satisﬁ subset,5,0.47280478,-0.69487303
Feature Selection at the Discrete Limit,"Miao Zhang, Chris Ding and Ya Zhang","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","feature selection
sparse
L2p norm",NMLA: Dimension Reduction/Feature Selection,"Feature selection plays an important role in many machine
learning and data mining applications. In this paper,
we propose to use L2,p norm for feature selection
with emphasis on small p. As p appoaches 0, feature selection
becomes discrete feature selection problem.We provide
two algorithms, proximal gradient algorithm and rank one
update algorithm, which is more efficient at large
regularization . Experiments on real life data sets show
that features selected at small p consistently outperform
features selected at p = 1, the standard L2,1 approach
and other popular feature selection methods.","Feature Selection at the Discrete Limit Feature selection plays an important role in many machine
learning and data mining applications. In this paper,
we propose to use L2,p norm for feature selection
with emphasis on small p. As p appoaches 0, feature selection
becomes discrete feature selection problem.We provide
two algorithms, proximal gradient algorithm and rank one
update algorithm, which is more efficient at large
regularization . Experiments on real life data sets show
that features selected at small p consistently outperform
features selected at p = 1, the standard L2,1 approach
and other popular feature selection methods. feature selection
sparse
L2p norm",featur select discret limit featur select play import role mani machin learn data mine applic paper propos use l2p norm featur select emphasi small p p appoach 0 featur select becom discret featur select problemw provid two algorithm proxim gradient algorithm rank one updat algorithm effici larg regular experi real life data set show featur select small p consist outperform featur select p 1 standard l21 approach popular featur select method featur select spars l2p norm,6,-8.299024,-21.226244
Computing General First-order Parallel and Prioritized Circumscription,"Hai Wan, Zhanhao Xiao, Yuan Zhenfeng, Heng Zhang and Yan Zhang",Knowledge Representation and Reasoning (KRR),"first-order parallel and prioritized circumscription
first-order stable model semantics
translation
optimization","KRR: Common-Sense Reasoning
KRR: Nonmonotonic Reasoning","This paper focuses on computing general first-order parallel and prioritized circumscription with varied constants. We propose polynomial translations from general first-order circumscription to first-order stable model semantics over arbitrary structures, including $Tr_v$ for parallel circumscription and $Tr^s_v$ for several parallel circumscriptions (further for prioritized circumscription). To improve the efficiency, we give an optimization, called $\Gamma_{\exists}$, to reduce auxiliary predicates in number and logic programs in size when eliminating existential quantifiers during the translations. Based on these results, a general first-order circumscription solver, named cfo2lp, is developed by calling answer set programming solvers. Using circuit diagnosis problem and extended stable marriage problem as benchmarks, we compare cfo2lp with a propositional circumscription solver circ2dlp on efficiency. Experimental results demonstrate that for problems represented by general first-order circumscription naturally and intuitively, cfo2lp can compute all solutions over finite structures. We also apply our approach to description logics with circumscription and repairs in inconsistent databases, which can be handled effectively.","Computing General First-order Parallel and Prioritized Circumscription This paper focuses on computing general first-order parallel and prioritized circumscription with varied constants. We propose polynomial translations from general first-order circumscription to first-order stable model semantics over arbitrary structures, including $Tr_v$ for parallel circumscription and $Tr^s_v$ for several parallel circumscriptions (further for prioritized circumscription). To improve the efficiency, we give an optimization, called $\Gamma_{\exists}$, to reduce auxiliary predicates in number and logic programs in size when eliminating existential quantifiers during the translations. Based on these results, a general first-order circumscription solver, named cfo2lp, is developed by calling answer set programming solvers. Using circuit diagnosis problem and extended stable marriage problem as benchmarks, we compare cfo2lp with a propositional circumscription solver circ2dlp on efficiency. Experimental results demonstrate that for problems represented by general first-order circumscription naturally and intuitively, cfo2lp can compute all solutions over finite structures. We also apply our approach to description logics with circumscription and repairs in inconsistent databases, which can be handled effectively. first-order parallel and prioritized circumscription
first-order stable model semantics
translation
optimization",comput general firstord parallel priorit circumscript paper focus comput general firstord parallel priorit circumscript vari constant propos polynomi translat general firstord circumscript firstord stabl model semant arbitrari structur includ trv parallel circumscript trsv sever parallel circumscript priorit circumscript improv effici give optim call gammaexist reduc auxiliari predic number logic program size elimin existenti quantifi translat base result general firstord circumscript solver name cfo2lp develop call answer set program solver use circuit diagnosi problem extend stabl marriag problem benchmark compar cfo2lp proposit circumscript solver circ2dlp effici experiment result demonstr problem repres general firstord circumscript natur intuit cfo2lp comput solut finit structur also appli approach descript logic circumscript repair inconsist databas handl effect firstord parallel priorit circumscript firstord stabl model semant translat optim,5,-10.9081335,4.4504104
Learning Word Representation Considering Proximity and Ambiguity,"Lin Qiu, Yong Cao, Zaiqing Nie and Yong Rui","AI and the Web (AIW)
Machine Learning Applications (MLA)
NLP and Knowledge Representation (NLPKR)
NLP and Machine Learning (NLPML)","word proximity
word ambiguity
neural networks","AIW: Knowledge acquisition from the web
AIW: Machine learning and the web
MLA: Applications of Unsupervised Learning
NLPKR: Natural Language Processing (General/Other)
NLPML: Natural Language Processing (General/Other)
NMLA: Neural Networks/Deep Learning","Distributed representations of words (aka word embedding) have been proven helpful in solving NLP tasks. Training distributed representations of words with neural networks has received much attention of late. Especially, the most recent work on word embedding, the Continuous Bag-of-Words (CBOW) model and the Continuous Skip-gram (Skip-gram) model proposed by Google, shows very impressive results by significantly speeding up the training process to enable word representation learning from very large-scale data. However, both CBOW and Skip-gram do not pay enough attention to the word proximity in terms of model or the word ambiguity in terms of linguistics. In this paper, we propose Proximity-Ambiguity Sensitive (PAS) models (i.e. PAS CBOW and PAS Skip-gram) for producing high quality distributed representations of words considering both word proximity and ambiguity. From the model perspective, we introduce proximity weights as parameters to be learned in PAS CBOW and used in PAS Skip-gram. By better modeling word proximity, we reveal the real strength of the pooling-structured neural networks in word representation learning. The proximity-sensitive pooling layer can also be applied to other neural network applications that employ pooling layers. From the linguistics perspective, we train multiple representation vectors per word. Each representation vector corresponds to a particular sense of the word. By using PAS models, we achieved a maximum accuracy increase of 16.9% over the state-of-the-art models on the word representation test set.","Learning Word Representation Considering Proximity and Ambiguity Distributed representations of words (aka word embedding) have been proven helpful in solving NLP tasks. Training distributed representations of words with neural networks has received much attention of late. Especially, the most recent work on word embedding, the Continuous Bag-of-Words (CBOW) model and the Continuous Skip-gram (Skip-gram) model proposed by Google, shows very impressive results by significantly speeding up the training process to enable word representation learning from very large-scale data. However, both CBOW and Skip-gram do not pay enough attention to the word proximity in terms of model or the word ambiguity in terms of linguistics. In this paper, we propose Proximity-Ambiguity Sensitive (PAS) models (i.e. PAS CBOW and PAS Skip-gram) for producing high quality distributed representations of words considering both word proximity and ambiguity. From the model perspective, we introduce proximity weights as parameters to be learned in PAS CBOW and used in PAS Skip-gram. By better modeling word proximity, we reveal the real strength of the pooling-structured neural networks in word representation learning. The proximity-sensitive pooling layer can also be applied to other neural network applications that employ pooling layers. From the linguistics perspective, we train multiple representation vectors per word. Each representation vector corresponds to a particular sense of the word. By using PAS models, we achieved a maximum accuracy increase of 16.9% over the state-of-the-art models on the word representation test set. word proximity
word ambiguity
neural networks",learn word represent consid proxim ambigu distribut represent word aka word embed proven help solv nlp task train distribut represent word neural network receiv much attent late especi recent work word embed continu bagofword cbow model continu skipgram skipgram model propos googl show impress result signific speed train process enabl word represent learn largescal data howev cbow skipgram pay enough attent word proxim term model word ambigu term linguist paper propos proximityambigu sensit pas model ie pas cbow pas skipgram produc high qualiti distribut represent word consid word proxim ambigu model perspect introduc proxim weight paramet learn pas cbow use pas skipgram better model word proxim reveal real strength poolingstructur neural network word represent learn proximitysensit pool layer also appli neural network applic employ pool layer linguist perspect train multipl represent vector per word represent vector correspond particular sens word use pas model achiev maximum accuraci increas 169 stateoftheart model word represent test set word proxim word ambigu neural network,1,11.378169,-9.041464
Planning as Model Checking in Hybrid Domains,"Sergiy Bogomolov, Daniele Magazzeni, Andreas Podelski and Martin Wehrle",Planning and Scheduling (PS),"Planning in Mixed Discrete-Continuous Domains
PDDL+
Model Checking
Hybrid Automata
Planning as Model Checking","PS: Mixed Discrete/Continuous Planning
PS: Temporal Planning","Planning in hybrid domains is an important and challenging task, and
various planning algorithms have been proposed in the last years.
From an abstract point of view, hybrid planning domains are based on
hybrid automata, which have been studied intensively in the model
checking community. In particular, powerful model checking
algorithms and tools have emerged for this formalism. However,
despite the quest for more scalable planning approaches, model
checking algorithms have not been applied to planning in hybrid
domains so far.
                                      
In this paper, we make a first step in bridging the gap between
these two worlds. We provide a formal translation scheme from PDDL+
to the standard formalism of hybrid automata, as a solid basis for
using hybrid system model-checking tools for dealing with hybrid
planning domains. As a case study, we use the SpaceEx model checker,
showing how we can address PDDL+ domains that are out of the scope
of state-of-the-art planners.","Planning as Model Checking in Hybrid Domains Planning in hybrid domains is an important and challenging task, and
various planning algorithms have been proposed in the last years.
From an abstract point of view, hybrid planning domains are based on
hybrid automata, which have been studied intensively in the model
checking community. In particular, powerful model checking
algorithms and tools have emerged for this formalism. However,
despite the quest for more scalable planning approaches, model
checking algorithms have not been applied to planning in hybrid
domains so far.
                                      
In this paper, we make a first step in bridging the gap between
these two worlds. We provide a formal translation scheme from PDDL+
to the standard formalism of hybrid automata, as a solid basis for
using hybrid system model-checking tools for dealing with hybrid
planning domains. As a case study, we use the SpaceEx model checker,
showing how we can address PDDL+ domains that are out of the scope
of state-of-the-art planners. Planning in Mixed Discrete-Continuous Domains
PDDL+
Model Checking
Hybrid Automata
Planning as Model Checking",plan model check hybrid domain plan hybrid domain import challeng task various plan algorithm propos last year abstract point view hybrid plan domain base hybrid automata studi intens model check communiti particular power model check algorithm tool emerg formal howev despit quest scalabl plan approach model check algorithm appli plan hybrid domain far paper make first step bridg gap two world provid formal translat scheme pddl standard formal hybrid automata solid basi use hybrid system modelcheck tool deal hybrid plan domain case studi use spaceex model checker show address pddl domain scope stateoftheart planner plan mix discretecontinu domain pddl model check hybrid automata plan model check,3,-2.6238809,16.224546
Novel Density-based Clustering Algorithms for Uncertain Data,"Xianchao Zhang, Han Liu, Xiaotong Zhang and Xinyue Liu",Novel Machine Learning Algorithms (NMLA),"Uncertain data
Clustering
Density-based algorithm","NMLA: Clustering
RU: Uncertainty in AI (General/Other)","Density-based techniques seem promising for handling data
uncertainty in uncertain data clustering. Nevertheless, some
issues have not been addressed well in existing algorithms.
In this paper, we firstly propose a novel density-based uncertain
data clustering algorithm, which improves upon existing
algorithms from the following two aspects: (1) it employs
an exact method to compute the probability that the distance
between two uncertain objects is less than or equal to
a boundary value, instead of the sampling-based method in
previous work; (2) it introduces new definitions of core object
probability and direct reachability probability, thus reducing
the complexity and avoiding sampling. We then further improve
the algorithm by using a novel assignment strategy to
ensure that every object will be assigned to the most appropriate
cluster. Experimental results show the superiority of
our proposed algorithms over existing ones.","Novel Density-based Clustering Algorithms for Uncertain Data Density-based techniques seem promising for handling data
uncertainty in uncertain data clustering. Nevertheless, some
issues have not been addressed well in existing algorithms.
In this paper, we firstly propose a novel density-based uncertain
data clustering algorithm, which improves upon existing
algorithms from the following two aspects: (1) it employs
an exact method to compute the probability that the distance
between two uncertain objects is less than or equal to
a boundary value, instead of the sampling-based method in
previous work; (2) it introduces new definitions of core object
probability and direct reachability probability, thus reducing
the complexity and avoiding sampling. We then further improve
the algorithm by using a novel assignment strategy to
ensure that every object will be assigned to the most appropriate
cluster. Experimental results show the superiority of
our proposed algorithms over existing ones. Uncertain data
Clustering
Density-based algorithm",novel densitybas cluster algorithm uncertain data densitybas techniqu seem promis handl data uncertainti uncertain data cluster nevertheless issu address well exist algorithm paper first propos novel densitybas uncertain data cluster algorithm improv upon exist algorithm follow two aspect 1 employ exact method comput probabl distanc two uncertain object less equal boundari valu instead samplingbas method previous work 2 introduc new definit core object probabl direct reachabl probabl thus reduc complex avoid sampl improv algorithm use novel assign strategi ensur everi object assign appropri cluster experiment result show superior propos algorithm exist one uncertain data cluster densitybas algorithm,7,-4.799061,-18.091793
Parallel Restarted Search,"Andre Cire, Serdar Kadioglu and Meinolf Sellmann",Heuristic Search and Optimization (HSO),"Restarts
Parallel Search
Deterministic Parallelization",HSO: Search (General/Other),"We consider the problem of parallelizing restarted backtrack search. With few notable exceptions, most commercial and academic constraint programming solvers do not learn no-goods during search. Depending on the branching heuristics used, this means that there are little to no side-effects between restarts, making them an excellent target for parallelization. We develop a simple technique for parallelizing restarted search deterministically and demonstrate experimentally that we can achieve near-linear speed-ups in practice.","Parallel Restarted Search We consider the problem of parallelizing restarted backtrack search. With few notable exceptions, most commercial and academic constraint programming solvers do not learn no-goods during search. Depending on the branching heuristics used, this means that there are little to no side-effects between restarts, making them an excellent target for parallelization. We develop a simple technique for parallelizing restarted search deterministically and demonstrate experimentally that we can achieve near-linear speed-ups in practice. Restarts
Parallel Search
Deterministic Parallelization",parallel restart search consid problem parallel restart backtrack search notabl except commerci academ constraint program solver learn nogood search depend branch heurist use mean littl sideeffect restart make excel target parallel develop simpl techniqu parallel restart search determinist demonstr experiment achiev nearlinear speedup practic restart parallel search determinist parallel,5,-11.712083,5.7070513
MaxSAT Portfolio,"Carlos Ansotegui-Gil, Yuri Malitsky and Meinolf Sellmann",Search and Constraint Satisfaction (SCS),"MaxSAT
Algorithm Portfolios
Algorithm Tuning",SCS: Satisfiability (General/Other),"Our objective is to boost the state-of-the-art performance in MaxSAT solving. To this end, we employ the instance-specific algorithm configurator ISAC and improve it by combining it with the latest in portfolio technology. Experimental results on SAT show that this combination marks a significant step forward in our ability to tune algorithms instance-specifically. We then apply the new methodology to a number of MaxSAT problem domains and show that the resulting solvers consistently outperform the best existing solvers on the respective problem families. In fact, the solvers presented here were independently evaluated at the 2013 MaxSAT Evaluation where they won six out of eleven categories.","MaxSAT Portfolio Our objective is to boost the state-of-the-art performance in MaxSAT solving. To this end, we employ the instance-specific algorithm configurator ISAC and improve it by combining it with the latest in portfolio technology. Experimental results on SAT show that this combination marks a significant step forward in our ability to tune algorithms instance-specifically. We then apply the new methodology to a number of MaxSAT problem domains and show that the resulting solvers consistently outperform the best existing solvers on the respective problem families. In fact, the solvers presented here were independently evaluated at the 2013 MaxSAT Evaluation where they won six out of eleven categories. MaxSAT
Algorithm Portfolios
Algorithm Tuning",maxsat portfolio object boost stateoftheart perform maxsat solv end employ instancespecif algorithm configur isac improv combin latest portfolio technolog experiment result sat show combin mark signific step forward abil tune algorithm instancespecif appli new methodolog number maxsat problem domain show result solver consist outperform best exist solver respect problem famili fact solver present independ evalu 2013 maxsat evalu six eleven categori maxsat algorithm portfolio algorithm tune,4,-10.492063,16.823309
Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization,Dongqing Zhang and Wu-Jun Li,"Novel Machine Learning Algorithms (NMLA)
Vision (VIS)","Multimodal Hashing
Cross-view Similarity Search
Image Retrieval
Scalability","NMLA: Big Data / Scalability
NMLA: Supervised Learning (Other)
VIS: Image and Video Retrieval","Due to its low storage cost and fast query speed, hashing has been widely adopted for similarity search in multimedia data. In particular, more and more attentions have been payed to multimodal hashing for search in multimedia data with multiple modalities, such as images with tags. Typically, supervised information of semantic
labels is also available for the data points in many real applications. Hence, many supervised multimodal hashing~(SMH) methods have been proposed to utilize such semantic labels to further improve the search accuracy. However, the training time complexity of most existing SMH methods is too high, which makes them unscalable to large-scale datasets. In this paper, a novel SMH method, called semantic
correlation maximization~(SCM), is proposed to seamlessly integrate semantic labels into the hashing learning procedure for large-scale data modeling. Experimental results on two real-world datasets show
that SCM can significantly outperform the state-of-the-art SMH methods, in terms of both accuracy and scalability.","Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization Due to its low storage cost and fast query speed, hashing has been widely adopted for similarity search in multimedia data. In particular, more and more attentions have been payed to multimodal hashing for search in multimedia data with multiple modalities, such as images with tags. Typically, supervised information of semantic
labels is also available for the data points in many real applications. Hence, many supervised multimodal hashing~(SMH) methods have been proposed to utilize such semantic labels to further improve the search accuracy. However, the training time complexity of most existing SMH methods is too high, which makes them unscalable to large-scale datasets. In this paper, a novel SMH method, called semantic
correlation maximization~(SCM), is proposed to seamlessly integrate semantic labels into the hashing learning procedure for large-scale data modeling. Experimental results on two real-world datasets show
that SCM can significantly outperform the state-of-the-art SMH methods, in terms of both accuracy and scalability. Multimodal Hashing
Cross-view Similarity Search
Image Retrieval
Scalability",largescal supervis multimod hash semant correl maxim due low storag cost fast queri speed hash wide adopt similar search multimedia data particular attent pay multimod hash search multimedia data multipl modal imag tag typic supervis inform semant label also avail data point mani real applic henc mani supervis multimod hashingsmh method propos util semant label improv search accuraci howev train time complex exist smh method high make unscal largescal dataset paper novel smh method call semant correl maximizationscm propos seamless integr semant label hash learn procedur largescal data model experiment result two realworld dataset show scm signific outperform stateoftheart smh method term accuraci scalabl multimod hash crossview similar search imag retriev scalabl,6,-21.509247,-7.8379107
Role-aware Conformity Modeling and Analysis in Social Networks,"Jing Zhang, Jie Tang, Honglei Zhuang, Cane Leung and Juanzi Li","Applications (APP)
Novel Machine Learning Algorithms (NMLA)","social conformity
role base conformity
probabilistic model","APP: Computational Social Science
NMLA: Data Mining and Knowledge Discovery","Conformity is the inclination of a person to be influenced by others. In this paper, we study how the conformity tendency of a person changes with her {\em role}, as defined by her structural properties in a social network. We first formalize conformity using a utility function based on the conformity theory from social psychology, and validate the proposed utility function by proving the existence of Nash Equilibria when all users in a network behave according to it. We then extend and incorporate the utility function into a probabilistic topic model, called the Role-Conformity Model (RCM), for modeling user behaviors under the effect of conformity. We apply the proposed RCM to several academic research networks, and discover that people with higher degree and lower clustering coefficient are more likely to conform to others. We also evaluate RCM through the task of word usage prediction in academic publications, and show significant improvements over the performance of baselines.","Role-aware Conformity Modeling and Analysis in Social Networks Conformity is the inclination of a person to be influenced by others. In this paper, we study how the conformity tendency of a person changes with her {\em role}, as defined by her structural properties in a social network. We first formalize conformity using a utility function based on the conformity theory from social psychology, and validate the proposed utility function by proving the existence of Nash Equilibria when all users in a network behave according to it. We then extend and incorporate the utility function into a probabilistic topic model, called the Role-Conformity Model (RCM), for modeling user behaviors under the effect of conformity. We apply the proposed RCM to several academic research networks, and discover that people with higher degree and lower clustering coefficient are more likely to conform to others. We also evaluate RCM through the task of word usage prediction in academic publications, and show significant improvements over the performance of baselines. social conformity
role base conformity
probabilistic model",roleawar conform model analysi social network conform inclin person influenc other paper studi conform tendenc person chang em role defin structur properti social network first formal conform use util function base conform theori social psycholog valid propos util function prove exist nash equilibria user network behav accord extend incorpor util function probabilist topic model call roleconform model rcm model user behavior effect conform appli propos rcm sever academ research network discov peopl higher degre lower cluster coeffici like conform other also evalu rcm task word usag predict academ public show signific improv perform baselin social conform role base conform probabilist model,0,12.083142,5.43673
Data Quality in Ontology-based Data Access: The Case of Consistency,Marco Console and Maurizio Lenzerini,Knowledge Representation and Reasoning (KRR),"Ontology-based data access
Description Logics
Data Quality
Data Management","APP: Other Applications
KRR: Ontologies
KRR: Description Logics
KRR: Knowledge Representation (General/Other)","Ontology-based data access (OBDA) is a new paradigm aiming at
  accessing and managing data by means of an ontology, i.e., a
  conceptual representation of the domain of interest in the
  underlying information system. In the last years, this new paradigm
  has been used for providing users with abstract (independent from
  technological and system-oriented aspects), effective, and
  reasoning-intensive mechanisms for querying the data residing at the
  information system sources. In this paper we argue that OBDA,
  besides querying data, provides the right principles for devising a
  formal approach to data quality. In particular, we concentrate on
  one of the most important dimensions considered both in the
  literature and in the practice of data quality, namely
  consistency. We define a general framework for data consistency in
  OBDA, and present algorithms and complexity analysis for several
  relevant tasks related to the problem of checking data quality under
  this dimension, both at the extensional level (content of the data
  sources), and at the intensional level (schema of the data sources).","Data Quality in Ontology-based Data Access: The Case of Consistency Ontology-based data access (OBDA) is a new paradigm aiming at
  accessing and managing data by means of an ontology, i.e., a
  conceptual representation of the domain of interest in the
  underlying information system. In the last years, this new paradigm
  has been used for providing users with abstract (independent from
  technological and system-oriented aspects), effective, and
  reasoning-intensive mechanisms for querying the data residing at the
  information system sources. In this paper we argue that OBDA,
  besides querying data, provides the right principles for devising a
  formal approach to data quality. In particular, we concentrate on
  one of the most important dimensions considered both in the
  literature and in the practice of data quality, namely
  consistency. We define a general framework for data consistency in
  OBDA, and present algorithms and complexity analysis for several
  relevant tasks related to the problem of checking data quality under
  this dimension, both at the extensional level (content of the data
  sources), and at the intensional level (schema of the data sources). Ontology-based data access
Description Logics
Data Quality
Data Management",data qualiti ontologybas data access case consist ontologybas data access obda new paradigm aim access manag data mean ontolog ie conceptu represent domain interest under inform system last year new paradigm use provid user abstract independ technolog systemori aspect effect reasoningintens mechan queri data resid inform system sourc paper argu obda besid queri data provid right principl devis formal approach data qualiti particular concentr one import dimens consid literatur practic data qualiti name consist defin general framework data consist obda present algorithm complex analysi sever relev task relat problem check data qualiti dimens extension level content data sourc intension level schema data sourc ontologybas data access descript logic data qualiti data manag,6,-10.809305,-3.6428583
Sparse Compositional Metric Learning,"Yuan Shi, Aurélien Bellet and Fei Sha",Novel Machine Learning Algorithms (NMLA),"Metric Learning
Sparse Methods
Local Metric Learning
Multi-task Learning","NMLA: Classification
NMLA: Dimension Reduction/Feature Selection
NMLA: Transfer, Adaptation, Multitask Learning","We propose a new approach for metric learning by framing it as learning a sparse combination of locally discriminative metrics that are inexpensive to generate from the training data. This flexible framework allows us to naturally derive formulations for global, multi-task and local metric learning. These new algorithms have several advantages over existing methods in the literature: a much smaller number of parameters to be estimated and a principled way to generalize learned metrics to new testing data points. To analyze the approach theoretically, we derive a generalization bound that justifies the sparse combination. Empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. The results are consistent with our theoretical findings and demonstrate the superiority of our approach in terms of classification performance and scalability.","Sparse Compositional Metric Learning We propose a new approach for metric learning by framing it as learning a sparse combination of locally discriminative metrics that are inexpensive to generate from the training data. This flexible framework allows us to naturally derive formulations for global, multi-task and local metric learning. These new algorithms have several advantages over existing methods in the literature: a much smaller number of parameters to be estimated and a principled way to generalize learned metrics to new testing data points. To analyze the approach theoretically, we derive a generalization bound that justifies the sparse combination. Empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. The results are consistent with our theoretical findings and demonstrate the superiority of our approach in terms of classification performance and scalability. Metric Learning
Sparse Methods
Local Metric Learning
Multi-task Learning",spars composit metric learn propos new approach metric learn frame learn spars combin local discrimin metric inexpens generat train data flexibl framework allow us natur deriv formul global multitask local metric learn new algorithm sever advantag exist method literatur much smaller number paramet estim principl way general learn metric new test data point analyz approach theoret deriv general bound justifi spars combin empir evalu algorithm sever dataset stateoftheart metric learn method result consist theoret find demonstr superior approach term classif perform scalabl metric learn spars method local metric learn multitask learn,4,1.3456378,-15.419342
Improving Context and Category Matching for Entity Search,"Yueguo Chen, Lexi Gao, Shuming Shi, Xiaoyong Du and Ji-Rong Wen",AI and the Web (AIW),"entity search
language model
category matching
context matching
result re-ranking","AIW: Enhancing web search and information retrieval
AIW: Question answering on the web","Entity search is to retrieve a ranked list of named entities of target types to a given query. In this paper, we propose an approach of entity search by formalizing both context matching and category matching. In addition, we propose a result re-ranking strategy that can be easily adapted to achieve a hybrid of two context matching strategies. Experiments on the INEX 2009 entity ranking task show that the proposed approach achieves a significant improvement of the entity search performance (xinfAP from 0.27 to 0.39) over the existing solutions.","Improving Context and Category Matching for Entity Search Entity search is to retrieve a ranked list of named entities of target types to a given query. In this paper, we propose an approach of entity search by formalizing both context matching and category matching. In addition, we propose a result re-ranking strategy that can be easily adapted to achieve a hybrid of two context matching strategies. Experiments on the INEX 2009 entity ranking task show that the proposed approach achieves a significant improvement of the entity search performance (xinfAP from 0.27 to 0.39) over the existing solutions. entity search
language model
category matching
context matching
result re-ranking",improv context categori match entiti search entiti search retriev rank list name entiti target type given queri paper propos approach entiti search formal context match categori match addit propos result rerank strategi easili adapt achiev hybrid two context match strategi experi inex 2009 entiti rank task show propos approach achiev signific improv entiti search perform xinfap 027 039 exist solut entiti search languag model categori match context match result rerank,0,-0.3869883,5.913342
ARIA: Asymmetry Resistant Instance Alignment,Sanghoon Lee and Seung-Won Hwang,AI and the Web (AIW),"instance alignment
entity matching
resolution
deduplication
linkage
linked data","AIW: AI for web services: semantic descriptions, planning, matching, and coordination","This paper studies the problem of instance alignment between knowledge bases (KBs). Existing approaches, exploiting the ""symmetry"" of structure and information across KBs, suffer in the presence of asymmetry, which is frequent as KBs are independently built. Specifically, we observe three types of asymmetry -- concept, feature, and structural asymmetry. The goal of this paper is to identify key techniques for overcoming each type of asymmetry, then build them into a framework that robustly aligns matches over asymmetry. In particular, we propose an Asymmetry-Resistant Instance Alignment framework (ARIA), implementing two-phased blocking methods considering concept and feature asymmetry, with a novel similarity measure overcoming structural asymmetry. Our evaluation results validate that this framework outperforms state-of-the-arts in terms of both response time and accuracy, by increasing 18% in precision and 2% in recall in matching large-scale real-life KBs.","ARIA: Asymmetry Resistant Instance Alignment This paper studies the problem of instance alignment between knowledge bases (KBs). Existing approaches, exploiting the ""symmetry"" of structure and information across KBs, suffer in the presence of asymmetry, which is frequent as KBs are independently built. Specifically, we observe three types of asymmetry -- concept, feature, and structural asymmetry. The goal of this paper is to identify key techniques for overcoming each type of asymmetry, then build them into a framework that robustly aligns matches over asymmetry. In particular, we propose an Asymmetry-Resistant Instance Alignment framework (ARIA), implementing two-phased blocking methods considering concept and feature asymmetry, with a novel similarity measure overcoming structural asymmetry. Our evaluation results validate that this framework outperforms state-of-the-arts in terms of both response time and accuracy, by increasing 18% in precision and 2% in recall in matching large-scale real-life KBs. instance alignment
entity matching
resolution
deduplication
linkage
linked data",aria asymmetri resist instanc align paper studi problem instanc align knowledg base kbs exist approach exploit symmetri structur inform across kbs suffer presenc asymmetri frequent kbs independ built specif observ three type asymmetri concept featur structur asymmetri goal paper identifi key techniqu overcom type asymmetri build framework robust align match asymmetri particular propos asymmetryresist instanc align framework aria implement twophas block method consid concept featur asymmetri novel similar measur overcom structur asymmetri evalu result valid framework outperform stateoftheart term respons time accuraci increas 18 precis 2 recal match largescal reallif kbs instanc align entiti match resolut dedupl linkag link data,0,-0.61058134,0.109911166
Diagnosing Analogue Linear Systems Using Dynamic Topological Reconfiguration,Alexander Feldman and Gregory Provan,Knowledge Representation and Reasoning (KRR),"model-based diagnosis
analogue systems
modeling and analysis",KRR: Diagnosis and Abductive Reasoning,"Fault diagnosis of analogue linear systems is a challenging task and no fully
automated solution exists. Two challenges in this diagnosis task are the size of the search space that much be explored and the possibility of simulation instabilities introduced by particular fault classes. We study a novel algorithm that addresses both problems. This algorithm dynamically modifies the simulation model during  diagnosis by pruning parametrized components that cause discontinuity in the model. We provide a theoretical framework for predicting the speedups, which depends on the topology of the model. We empirically validate the theoretical predictions through extensive experimentation on a benchmark of circuits.","Diagnosing Analogue Linear Systems Using Dynamic Topological Reconfiguration Fault diagnosis of analogue linear systems is a challenging task and no fully
automated solution exists. Two challenges in this diagnosis task are the size of the search space that much be explored and the possibility of simulation instabilities introduced by particular fault classes. We study a novel algorithm that addresses both problems. This algorithm dynamically modifies the simulation model during  diagnosis by pruning parametrized components that cause discontinuity in the model. We provide a theoretical framework for predicting the speedups, which depends on the topology of the model. We empirically validate the theoretical predictions through extensive experimentation on a benchmark of circuits. model-based diagnosis
analogue systems
modeling and analysis",diagnos analogu linear system use dynam topolog reconfigur fault diagnosi analogu linear system challeng task fulli autom solut exist two challeng diagnosi task size search space much explor possibl simul instabl introduc particular fault class studi novel algorithm address problem algorithm dynam modifi simul model diagnosi prune parametr compon caus discontinu model provid theoret framework predict speedup depend topolog model empir valid theoret predict extens experiment benchmark circuit modelbas diagnosi analogu system model analysi,5,2.6227186,4.813128
Simpler Bounded Suboptimal Search,Matthew Hatem and Wheeler Ruml,Heuristic Search and Optimization (HSO),"bounded suboptimal search
distance estimates
additional heuristics
iterative deepening
implementation","HSO: Heuristic Search
HSO: Evaluation and Analysis (Search and Optimization)","It is commonly appreciated that solving search problems optimally can take too long. Bounded suboptimal search algorithms trade increased solution cost for reduced solving time.  Explicit Estimation Search (EES) is a recent state-of-the-art algorithm specifically designed for bounded suboptimal search.  Although it tends to expand fewer nodes than alternative algorithms, such as weighted A* (WA*), its per-node expansion overhead is much higher, causing it to sometimes take longer.  In this paper, we present simplified variants of EES (SEES) and an earlier algorithm, A*epsilon (SA*epsilon), that use different implementations of the same motivating ideas to significantly reduce search overhead and implementation complexity.  In an empirical evaluation, we find that SEES, like EES, outperforms classic bounded suboptimal search algorithms, such as WA*, on domains tested where distance-to-go estimates enable better search guidance.  We also confirm that, while SEES and SA*epsilon expand roughly the same number of nodes as their progenitors, they solve problems significantly faster and are much easier to implement.  This work widens the applicability of state-of the-art bounded suboptimal search by making it easier to deploy.","Simpler Bounded Suboptimal Search It is commonly appreciated that solving search problems optimally can take too long. Bounded suboptimal search algorithms trade increased solution cost for reduced solving time.  Explicit Estimation Search (EES) is a recent state-of-the-art algorithm specifically designed for bounded suboptimal search.  Although it tends to expand fewer nodes than alternative algorithms, such as weighted A* (WA*), its per-node expansion overhead is much higher, causing it to sometimes take longer.  In this paper, we present simplified variants of EES (SEES) and an earlier algorithm, A*epsilon (SA*epsilon), that use different implementations of the same motivating ideas to significantly reduce search overhead and implementation complexity.  In an empirical evaluation, we find that SEES, like EES, outperforms classic bounded suboptimal search algorithms, such as WA*, on domains tested where distance-to-go estimates enable better search guidance.  We also confirm that, while SEES and SA*epsilon expand roughly the same number of nodes as their progenitors, they solve problems significantly faster and are much easier to implement.  This work widens the applicability of state-of the-art bounded suboptimal search by making it easier to deploy. bounded suboptimal search
distance estimates
additional heuristics
iterative deepening
implementation",simpler bound suboptim search common appreci solv search problem optim take long bound suboptim search algorithm trade increas solut cost reduc solv time explicit estim search ee recent stateoftheart algorithm specif design bound suboptim search although tend expand fewer node altern algorithm weight wa pernod expans overhead much higher caus sometim take longer paper present simplifi variant ee see earlier algorithm aepsilon saepsilon use differ implement motiv idea signific reduc search overhead implement complex empir evalu find see like ee outperform classic bound suboptim search algorithm wa domain test distancetogo estim enabl better search guidanc also confirm see saepsilon expand rough number node progenitor solv problem signific faster much easier implement work widen applic stateof theart bound suboptim search make easier deploy bound suboptim search distanc estim addit heurist iter deepen implement,4,-13.6861,8.905909
PAC Rank Elicitation through Adaptive Sampling of Stochastic Pairwise Preferences,"Róbert Busa-Fekete, Balazs Szorenyi and Eyke Huellermeier",Novel Machine Learning Algorithms (NMLA),"Preference learning
Online learning
Ranking models
Rank elicitation
Sample complexity","NMLA: Online Learning
NMLA: Preferences/Ranking Learning","We introduce the problem of PAC rank elicitation, which consists of sorting a given set of options based on adaptive sampling of stochastic pairwise preferences. More specifically, the goal is to predict a ranking that is sufficiently close to a target order with high probability. We instantiate this setting with combinations of two different distance measures and ranking procedures for determining the target order. For these instantiations, we devise efficient sampling strategies and analyze the corresponding sample complexity. We also present first experiments to illustrate the practical performance of our methods.","PAC Rank Elicitation through Adaptive Sampling of Stochastic Pairwise Preferences We introduce the problem of PAC rank elicitation, which consists of sorting a given set of options based on adaptive sampling of stochastic pairwise preferences. More specifically, the goal is to predict a ranking that is sufficiently close to a target order with high probability. We instantiate this setting with combinations of two different distance measures and ranking procedures for determining the target order. For these instantiations, we devise efficient sampling strategies and analyze the corresponding sample complexity. We also present first experiments to illustrate the practical performance of our methods. Preference learning
Online learning
Ranking models
Rank elicitation
Sample complexity",pac rank elicit adapt sampl stochast pairwis prefer introduc problem pac rank elicit consist sort given set option base adapt sampl stochast pairwis prefer specif goal predict rank suffici close target order high probabl instanti set combin two differ distanc measur rank procedur determin target order instanti devis effici sampl strategi analyz correspond sampl complex also present first experi illustr practic perform method prefer learn onlin learn rank model rank elicit sampl complex,9,9.0400915,7.3713098
Elementary Loops Revisited,"Jianmin Ji, Hai Wan, Peng Xiao, Ziwei Huo and Zhanhao Xiao",Knowledge Representation and Reasoning (KRR),"elementary loops
proper loops
positive body-head dependency graph",KRR: Logic Programming,"The notions of loops and loop formulas play an important role in answer set computation. However, there would be an exponential number of loops in the worst case. Recently, Gebser and Schaub characterized a subclass elementary loops and showed that they are sufficient for selecting answer sets from models of a logic program. In this paper, we propose an alternative definition of elementary loops. Based on the new perspective, we identify a subclass of elementary loops, called proper loops, and show that, by applying a special form of their loop formulas, they are also sufficient for the SAT-based answer set computation. We also provide a polynomial algorithm to recognize a proper loop and show that for certain logic programs, identifying all proper loops of a program is more efficient than identifying all elementary loops. Furthermore, we prove that, by considering the structure of the positive body-head dependency graph of a program, a large number of loops could be ignored for identifying proper loops. Based on the observation, we provide another algorithm for identifying all proper loops of a program. The experiments show that, for certain programs whose dependency graphs consisting of sets of components that are densely connected inside and sparsely connected outside, the new algorithm is more efficient.","Elementary Loops Revisited The notions of loops and loop formulas play an important role in answer set computation. However, there would be an exponential number of loops in the worst case. Recently, Gebser and Schaub characterized a subclass elementary loops and showed that they are sufficient for selecting answer sets from models of a logic program. In this paper, we propose an alternative definition of elementary loops. Based on the new perspective, we identify a subclass of elementary loops, called proper loops, and show that, by applying a special form of their loop formulas, they are also sufficient for the SAT-based answer set computation. We also provide a polynomial algorithm to recognize a proper loop and show that for certain logic programs, identifying all proper loops of a program is more efficient than identifying all elementary loops. Furthermore, we prove that, by considering the structure of the positive body-head dependency graph of a program, a large number of loops could be ignored for identifying proper loops. Based on the observation, we provide another algorithm for identifying all proper loops of a program. The experiments show that, for certain programs whose dependency graphs consisting of sets of components that are densely connected inside and sparsely connected outside, the new algorithm is more efficient. elementary loops
proper loops
positive body-head dependency graph",elementari loop revisit notion loop loop formula play import role answer set comput howev would exponenti number loop worst case recent gebser schaub character subclass elementari loop show suffici select answer set model logic program paper propos altern definit elementari loop base new perspect identifi subclass elementari loop call proper loop show appli special form loop formula also suffici satbas answer set comput also provid polynomi algorithm recogn proper loop show certain logic program identifi proper loop program effici identifi elementari loop furthermor prove consid structur posit bodyhead depend graph program larg number loop could ignor identifi proper loop base observ provid anoth algorithm identifi proper loop program experi show certain program whose depend graph consist set compon dens connect insid spars connect outsid new algorithm effici elementari loop proper loop posit bodyhead depend graph,3,-15.779919,-0.20252262
On Computing Optimal Strategies in Open List Proportional Representation: the Two Parties Case,Ning Ding and Fangzhen Lin,"Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","pure Nash equilibrium
open list proportional representation
computational social choice","GTEP: Game Theory
GTEP: Social Choice / Voting
GTEP: Equilibrium
MAS: Evaluation and Analysis (Multiagent Systems)","Open list proportional representation is an election mechanism used in many
elections including the 2012 Hong Kong Legislative Council
Geographical Constituencies election. In this paper, assuming that there
are just two parties in the election, and that the number of votes that a
list would get is the sum of the numbers of votes
that the candidates in the list would get if each of them would go alone in the election,
we formulate the election as a mostly zero-sum game, and show that while the
game always has a pure Nash equilibrium, it is NP-hard to compute it.","On Computing Optimal Strategies in Open List Proportional Representation: the Two Parties Case Open list proportional representation is an election mechanism used in many
elections including the 2012 Hong Kong Legislative Council
Geographical Constituencies election. In this paper, assuming that there
are just two parties in the election, and that the number of votes that a
list would get is the sum of the numbers of votes
that the candidates in the list would get if each of them would go alone in the election,
we formulate the election as a mostly zero-sum game, and show that while the
game always has a pure Nash equilibrium, it is NP-hard to compute it. pure Nash equilibrium
open list proportional representation
computational social choice",comput optim strategi open list proport represent two parti case open list proport represent elect mechan use mani elect includ 2012 hong kong legisl council geograph constitu elect paper assum two parti elect number vote list would get sum number vote candid list would get would go alon elect formul elect most zerosum game show game alway pure nash equilibrium nphard comput pure nash equilibrium open list proport represent comput social choic,2,19.72698,-3.6703029
Joint Morphological Generation and Syntactic Linearization,"Linfeng Song, Yue Zhang, Kai Song and Qun Liu",NLP and Knowledge Representation (NLPKR),"joint method
natural language generation
meaning text theory
syntactic linearization
morphological generation",NLPKR: Natural Language Processing (General/Other),"There has been a growing interest in stochastic methods
to natural language generation (NLG). While most NL-
G pipelines separate morphological generation and syn-
tactic linearization, the two tasks are closely related to
each other. In this paper, we study joint morphological
generation and linearization, making use of word order
and inflections information for both tasks and reducing
error propagation. Our experiments show that the join-
t method significantly outperforms a strong pipelined
baseline (by 1.0 BLEU points). It also achieves the
best reported result on the Generation Challenge 2011
shared task.","Joint Morphological Generation and Syntactic Linearization There has been a growing interest in stochastic methods
to natural language generation (NLG). While most NL-
G pipelines separate morphological generation and syn-
tactic linearization, the two tasks are closely related to
each other. In this paper, we study joint morphological
generation and linearization, making use of word order
and inflections information for both tasks and reducing
error propagation. Our experiments show that the join-
t method significantly outperforms a strong pipelined
baseline (by 1.0 BLEU points). It also achieves the
best reported result on the Generation Challenge 2011
shared task. joint method
natural language generation
meaning text theory
syntactic linearization
morphological generation",joint morpholog generat syntact linear grow interest stochast method natur languag generat nlg nl g pipelin separ morpholog generat syn tactic linear two task close relat paper studi joint morpholog generat linear make use word order inflect inform task reduc error propag experi show join method signific outperform strong pipelin baselin 10 bleu point also achiev best report result generat challeng 2011 share task joint method natur languag generat mean text theori syntact linear morpholog generat,4,-0.562403,4.108971
Machine Translation with Real-time Web Search,"Lei Cui, Ming Zhou, Qiming Chen, Dongdong Zhang and Mu Li",AI and the Web (AIW),"machine translation
real-time web search
web-based machine translation
phrase-level translation
sentence-level translation
search snippets","AIW: Human language technologies for web systems, including text summarization and machine translation","Contemporary machine translation systems usually rely on offline data retrieved from the web for individual model training, such as translation models and language models. Distinct from existing methods, we propose a novel approach that treats machine translation as a web search task and utilizes the web on the fly to acquire translation knowledge. This end-to-end approach takes advantage of fresh web search results that are capable of leveraging tremendous web knowledge to obtain phrase-level candidates on demand and then compose sentence-level translations. Experimental results show that our web-based machine translation method demonstrates very promising performance in leveraging fresh translation knowledge and making translation decisions. Furthermore, when combined with offline models, it significantly outperforms a state-of-the-art phrase-based statistical machine translation system.","Machine Translation with Real-time Web Search Contemporary machine translation systems usually rely on offline data retrieved from the web for individual model training, such as translation models and language models. Distinct from existing methods, we propose a novel approach that treats machine translation as a web search task and utilizes the web on the fly to acquire translation knowledge. This end-to-end approach takes advantage of fresh web search results that are capable of leveraging tremendous web knowledge to obtain phrase-level candidates on demand and then compose sentence-level translations. Experimental results show that our web-based machine translation method demonstrates very promising performance in leveraging fresh translation knowledge and making translation decisions. Furthermore, when combined with offline models, it significantly outperforms a state-of-the-art phrase-based statistical machine translation system. machine translation
real-time web search
web-based machine translation
phrase-level translation
sentence-level translation
search snippets",machin translat realtim web search contemporari machin translat system usual reli offlin data retriev web individu model train translat model languag model distinct exist method propos novel approach treat machin translat web search task util web fli acquir translat knowledg endtoend approach take advantag fresh web search result capabl leverag tremend web knowledg obtain phraselevel candid demand compos sentencelevel translat experiment result show webbas machin translat method demonstr promis perform leverag fresh translat knowledg make translat decis furthermor combin offlin model signific outperform stateoftheart phrasebas statist machin translat system machin translat realtim web search webbas machin translat phraselevel translat sentencelevel translat search snippet,5,-16.988672,15.21321
Latent Low-Rank Bi-Directional Transfer Subspace Learning,"Zhengming Ding, Ming Shao and Yun Fu",Novel Machine Learning Algorithms (NMLA),"transfer learning
latent low-rank
subspace learning","NMLA: Classification
NMLA: Dimension Reduction/Feature Selection
NMLA: Transfer, Adaptation, Multitask Learning","We consider an interesting problem in this paper that using transfer learning in two directions to compensate missing knowledge from the target domain. Transfer learning is usually exploited as a powerful tool to mitigate the discrepancy between different databases for knowledge transfer; it can also be used for knowledge transfer between different modalities within one database. However, in either case, transfer learning will fail if the target data are missing. To overcome this, we consider knowledge transfer between different databases and modalities simultaneously in a single framework, where missing target data from one database are recovered to facilitate recognition task. We call this framework Latent Low-rank Bi-Directional Transfer Subspace Learning method (L2BTSL). First, we propose to use low-rank constraint as well as dictionary learning in a learned subspace to guide the knowledge transfer between and within different databases. Second, a latent factor is introduced to uncover the underlying structure of the missing target data. Third, bi-directional transfer learning is proposed to integrate auxiliary
database for transfer learning with missing target data. Experimental results of multi-modalities knowledge transfer with missing target data demonstrate that our method can successfully inherit knowledge from the auxiliary database to complete the target domain, and therefore enhance the performance when recognizing data from the modality without any training data.","Latent Low-Rank Bi-Directional Transfer Subspace Learning We consider an interesting problem in this paper that using transfer learning in two directions to compensate missing knowledge from the target domain. Transfer learning is usually exploited as a powerful tool to mitigate the discrepancy between different databases for knowledge transfer; it can also be used for knowledge transfer between different modalities within one database. However, in either case, transfer learning will fail if the target data are missing. To overcome this, we consider knowledge transfer between different databases and modalities simultaneously in a single framework, where missing target data from one database are recovered to facilitate recognition task. We call this framework Latent Low-rank Bi-Directional Transfer Subspace Learning method (L2BTSL). First, we propose to use low-rank constraint as well as dictionary learning in a learned subspace to guide the knowledge transfer between and within different databases. Second, a latent factor is introduced to uncover the underlying structure of the missing target data. Third, bi-directional transfer learning is proposed to integrate auxiliary
database for transfer learning with missing target data. Experimental results of multi-modalities knowledge transfer with missing target data demonstrate that our method can successfully inherit knowledge from the auxiliary database to complete the target domain, and therefore enhance the performance when recognizing data from the modality without any training data. transfer learning
latent low-rank
subspace learning",latent lowrank bidirect transfer subspac learn consid interest problem paper use transfer learn two direct compens miss knowledg target domain transfer learn usual exploit power tool mitig discrep differ databas knowledg transfer also use knowledg transfer differ modal within one databas howev either case transfer learn fail target data miss overcom consid knowledg transfer differ databas modal simultan singl framework miss target data one databas recov facilit recognit task call framework latent lowrank bidirect transfer subspac learn method l2btsl first propos use lowrank constraint well dictionari learn learn subspac guid knowledg transfer within differ databas second latent factor introduc uncov under structur miss target data third bidirect transfer learn propos integr auxiliari databas transfer learn miss target data experiment result multimod knowledg transfer miss target data demonstr method success inherit knowledg auxiliari databas complet target domain therefor enhanc perform recogn data modal without train data transfer learn latent lowrank subspac learn,6,-14.998468,-9.09758
Adaptive Knowledge Transfer for Multiple Instance Learning in Image Classification,"Qifan Wang, Lingyun Ruan and Luo Si",Novel Machine Learning Algorithms (NMLA),"Multiple Instance Learning
Image Classification
Transfer Learning","NMLA: Classification
NMLA: Transfer, Adaptation, Multitask Learning
VIS: Categorization","Multiple Instance Learning (MIL) is a popular learning technique in various
vision tasks including image classification.
However, most existing MIL methods do not consider the problem of insufficient examples in the given target category. In this case, it is difficult for traditional MIL methods to build an accurate classifier due to the lack of training examples. Motivated by the empirical success of transfer learning, this paper proposes a novel approach of Adaptive Knowledge Transfer for Multiple Instance Learning (AKT-MIL) in image classification. The new method transfers cross-category knowledge from source categories under multiple instance setting for boosting the learning process. A unified learning framework with a data-dependent mixture model is designed to adaptively combine the transferred knowledge from sources with a weak classifier built in the target domain.
Based on this framework, an iterative coordinate descent method with Constraint
Concave-Convex Programming (CCCP) is proposed as the optimization procedure. An extensive set of experimental results demonstrate that the proposed AKT-MIL approach substantially outperforms several state-of-the-art algorithms on two benchmark datasets, especially in the scenario when very few training examples are available in the target domain.","Adaptive Knowledge Transfer for Multiple Instance Learning in Image Classification Multiple Instance Learning (MIL) is a popular learning technique in various
vision tasks including image classification.
However, most existing MIL methods do not consider the problem of insufficient examples in the given target category. In this case, it is difficult for traditional MIL methods to build an accurate classifier due to the lack of training examples. Motivated by the empirical success of transfer learning, this paper proposes a novel approach of Adaptive Knowledge Transfer for Multiple Instance Learning (AKT-MIL) in image classification. The new method transfers cross-category knowledge from source categories under multiple instance setting for boosting the learning process. A unified learning framework with a data-dependent mixture model is designed to adaptively combine the transferred knowledge from sources with a weak classifier built in the target domain.
Based on this framework, an iterative coordinate descent method with Constraint
Concave-Convex Programming (CCCP) is proposed as the optimization procedure. An extensive set of experimental results demonstrate that the proposed AKT-MIL approach substantially outperforms several state-of-the-art algorithms on two benchmark datasets, especially in the scenario when very few training examples are available in the target domain. Multiple Instance Learning
Image Classification
Transfer Learning",adapt knowledg transfer multipl instanc learn imag classif multipl instanc learn mil popular learn techniqu various vision task includ imag classif howev exist mil method consid problem insuffici exampl given target categori case difficult tradit mil method build accur classifi due lack train exampl motiv empir success transfer learn paper propos novel approach adapt knowledg transfer multipl instanc learn aktmil imag classif new method transfer crosscategori knowledg sourc categori multipl instanc set boost learn process unifi learn framework datadepend mixtur model design adapt combin transfer knowledg sourc weak classifi built target domain base framework iter coordin descent method constraint concaveconvex program cccp propos optim procedur extens set experiment result demonstr propos aktmil approach substanti outperform sever stateoftheart algorithm two benchmark dataset especi scenario train exampl avail target domain multipl instanc learn imag classif transfer learn,6,-15.680979,-9.748113
Finding the k-best Equivalence Classes of Bayesian Network Structures for Model Averaging,Yetian Chen and Jin Tian,"Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)","Bayesian network
Equivalence class
Model averaging
Dynamic programming","NMLA: Bayesian Learning
NMLA: Graphical Model Learning
RU: Bayesian Networks
RU: Graphical Models (Other)
RU: Probabilistic Inference
RU: Uncertainty Representations","In this paper we develop an algorithm to find the k-best equivalence classes of Bayesian networks. Our algorithm is capable of finding much more best DAGs than the previous algorithm that directly finds the k-best DAGs (Tian, He and Ram 2010). We demonstrate our algorithm in the task of Bayesian model averaging. Empirical results show that our algorithm significantly outperforms the k-best DAG algorithm in both time and space to achieve the same quality of approximation. Our algorithm goes beyond the maximum-a-posteriori (MAP) model by listing the most likely network structures and their relative likelihood and therefore has important applications in causal structure discovery.","Finding the k-best Equivalence Classes of Bayesian Network Structures for Model Averaging In this paper we develop an algorithm to find the k-best equivalence classes of Bayesian networks. Our algorithm is capable of finding much more best DAGs than the previous algorithm that directly finds the k-best DAGs (Tian, He and Ram 2010). We demonstrate our algorithm in the task of Bayesian model averaging. Empirical results show that our algorithm significantly outperforms the k-best DAG algorithm in both time and space to achieve the same quality of approximation. Our algorithm goes beyond the maximum-a-posteriori (MAP) model by listing the most likely network structures and their relative likelihood and therefore has important applications in causal structure discovery. Bayesian network
Equivalence class
Model averaging
Dynamic programming",find kbest equival class bayesian network structur model averag paper develop algorithm find kbest equival class bayesian network algorithm capabl find much best dag previous algorithm direct find kbest dag tian ram 2010 demonstr algorithm task bayesian model averag empir result show algorithm signific outperform kbest dag algorithm time space achiev qualiti approxim algorithm goe beyond maximumaposteriori map model list like network structur relat likelihood therefor import applic causal structur discoveri bayesian network equival class model averag dynam program,5,10.154577,1.8423724
On the Challenges of Physical Implementations of RBMs,"Vincent Dumoulin, Ian J. Goodfellow, Aaron Courville and Yoshua Bengio",Machine Learning Applications (MLA),"Restricted Boltzmann Machine
RBM
Hardware
Neuromorphic
Empirical
Deep Learning",MLA: Applications of Unsupervised Learning,"Restricted Boltzmann machines (RBMs) are powerful machine learning models, but
learning and some kinds of inference in the model require sampling-based
approximations, which, in classical digital computers, are implemented using
expensive MCMC.  Physical computation offers the opportunity to reduce the cost
of sampling by building physical systems whose natural dynamics correspond to
drawing samples from the desired RBM distribution. Such a system avoids the
burn-in and mixing cost of a Markov chain. However, hardware implementations of
this variety usually entail limitations such as low-precision and limited range
of the parameters and restrictions on the size and topology of the RBM.  We
conduct software simulations to determine how harmful each of these restrictions
is. Our simulations are designed to reproduce aspects of the D-Wave Two
computer, but the issues we investigate arise in most forms of physical
computation.
Our findings suggest that designers of new physical computing hardware and
algorithms for physical computers should concentrate their efforts on overcoming
the limitations imposed by the topology restrictions of currently existing
physical computers.","On the Challenges of Physical Implementations of RBMs Restricted Boltzmann machines (RBMs) are powerful machine learning models, but
learning and some kinds of inference in the model require sampling-based
approximations, which, in classical digital computers, are implemented using
expensive MCMC.  Physical computation offers the opportunity to reduce the cost
of sampling by building physical systems whose natural dynamics correspond to
drawing samples from the desired RBM distribution. Such a system avoids the
burn-in and mixing cost of a Markov chain. However, hardware implementations of
this variety usually entail limitations such as low-precision and limited range
of the parameters and restrictions on the size and topology of the RBM.  We
conduct software simulations to determine how harmful each of these restrictions
is. Our simulations are designed to reproduce aspects of the D-Wave Two
computer, but the issues we investigate arise in most forms of physical
computation.
Our findings suggest that designers of new physical computing hardware and
algorithms for physical computers should concentrate their efforts on overcoming
the limitations imposed by the topology restrictions of currently existing
physical computers. Restricted Boltzmann Machine
RBM
Hardware
Neuromorphic
Empirical
Deep Learning",challeng physic implement rbms restrict boltzmann machin rbms power machin learn model learn kind infer model requir samplingbas approxim classic digit comput implement use expens mcmc physic comput offer opportun reduc cost sampl build physic system whose natur dynam correspond draw sampl desir rbm distribut system avoid burnin mix cost markov chain howev hardwar implement varieti usual entail limit lowprecis limit rang paramet restrict size topolog rbm conduct softwar simul determin harm restrict simul design reproduc aspect dwave two comput issu investig aris form physic comput find suggest design new physic comput hardwar algorithm physic comput concentr effort overcom limit impos topolog restrict current exist physic comput restrict boltzmann machin rbm hardwar neuromorph empir deep learn,5,5.2319,-1.2241255
SLE: Signed Laplacian Embedding for Supervised Dimension Reduction,"Chen Gong, Dacheng Tao, Jie Yang and Keren Fu",Novel Machine Learning Algorithms (NMLA),"Dimension reduction
Manifold learning
Signed graph Laplacian",NMLA: Dimension Reduction/Feature Selection,"Manifold learning is a powerful tool for solving nonlinear dimension reduction problems. By assuming that the high-dimensional data usually lie on a low-dimensional manifold, many algorithms have been proposed. However, most algorithms simply adopt the traditional graph Laplacian to encode the data locality, so the discriminative ability is limited and the embedding results are not always suitable for the subsequent classification. Instead, this paper deploys the signed graph Laplacian and proposes Signed Laplacian Embedding (SLE) for supervised dimension reduction. By exploring the label information, SLE comprehensively transfers the discrimination carried by the original data to the embedded low-dimensional space. Without perturbing the discrimination structure, SLE also retains the locality. Theoretically, we prove the immersion property by computing the rank of projection, and relate SLE to existing algorithms in the frame of patch alignment. Thorough empirical studies on synthetic and real datasets demonstrate the effectiveness of SLE.","SLE: Signed Laplacian Embedding for Supervised Dimension Reduction Manifold learning is a powerful tool for solving nonlinear dimension reduction problems. By assuming that the high-dimensional data usually lie on a low-dimensional manifold, many algorithms have been proposed. However, most algorithms simply adopt the traditional graph Laplacian to encode the data locality, so the discriminative ability is limited and the embedding results are not always suitable for the subsequent classification. Instead, this paper deploys the signed graph Laplacian and proposes Signed Laplacian Embedding (SLE) for supervised dimension reduction. By exploring the label information, SLE comprehensively transfers the discrimination carried by the original data to the embedded low-dimensional space. Without perturbing the discrimination structure, SLE also retains the locality. Theoretically, we prove the immersion property by computing the rank of projection, and relate SLE to existing algorithms in the frame of patch alignment. Thorough empirical studies on synthetic and real datasets demonstrate the effectiveness of SLE. Dimension reduction
Manifold learning
Signed graph Laplacian",sle sign laplacian embed supervis dimens reduct manifold learn power tool solv nonlinear dimens reduct problem assum highdimension data usual lie lowdimension manifold mani algorithm propos howev algorithm simpli adopt tradit graph laplacian encod data local discrimin abil limit embed result alway suitabl subsequ classif instead paper deploy sign graph laplacian propos sign laplacian embed sle supervis dimens reduct explor label inform sle comprehens transfer discrimin carri origin data embed lowdimension space without perturb discrimin structur sle also retain local theoret prove immers properti comput rank project relat sle exist algorithm frame patch align thorough empir studi synthet real dataset demonstr effect sle dimens reduct manifold learn sign graph laplacian,6,5.9621844,-10.002152
Envy-Free Division of Sellable Goods,"Jeremy Karp, Aleksandr Kazachkov and Ariel Procaccia",Game Theory and Economic Paradigms (GTEP),"Computational social choice
Fair division
Envy-free allocation","GTEP: Auctions and Market-Based Systems
GTEP: Social Choice / Voting","We study the envy-free allocation of indivisible goods between two players. Our novel setting includes an option to sell each good for a fraction of the minimum value any player has for the good. To rigorously quantify the efficiency gain from selling, we reason about the price of envy-freeness of allocations of sellable goods — the ratio between the maximum social welfare and the social welfare of the best envy-free allocation. We show that envy-free allocations of sellable goods are significantly more efficient than their unsellable counterparts.","Envy-Free Division of Sellable Goods We study the envy-free allocation of indivisible goods between two players. Our novel setting includes an option to sell each good for a fraction of the minimum value any player has for the good. To rigorously quantify the efficiency gain from selling, we reason about the price of envy-freeness of allocations of sellable goods — the ratio between the maximum social welfare and the social welfare of the best envy-free allocation. We show that envy-free allocations of sellable goods are significantly more efficient than their unsellable counterparts. Computational social choice
Fair division
Envy-free allocation",envyfre divis sellabl good studi envyfre alloc indivis good two player novel set includ option sell good fraction minimum valu player good rigor quantifi effici gain sell reason price envyfre alloc sellabl good — ratio maximum social welfar social welfar best envyfre alloc show envyfre alloc sellabl good signific effici unsel counterpart comput social choic fair divis envyfre alloc,9,-17.802813,-18.778376
Potential-Aware Imperfect-Recall Abstraction with Earth Mover's Distance in Imperfect-Information Games,Sam Ganzfried and Tuomas Sandholm,Game Theory and Economic Paradigms (GTEP),"Game Theory
Multiagent Systems
Game Solving
Game Abstraction
Imperfect Information
Poker","GTEP: Game Theory
GTEP: Imperfect Information","There is often a large disparity between the size of a game we wish to solve and the size of the largest instances solvable by the best algorithms; for example, a popular variant of poker has about $10^{165}$ nodes in its game tree, while the currently best approximate equilibrium-finding algorithms scale to games with around $10^{12}$ nodes. In order to approximate equilibrium strategies in these games, the leading approach is to create a sufficiently small strategic approximation of the full game, called an abstraction, and to solve that smaller game instead. The leading abstraction algorithm for imperfect-information games generates abstractions that have imperfect recall and are distribution aware, using $k$-means with the earth mover's distance metric to cluster similar states together. A distribution-aware abstraction groups states together at a given round if their full distributions over future strength are similar (as opposed to, for example, just the expectation of their strength). The leading algorithm considers distributions over future strength at the final round of the game.  However, one might benefit by considering the distribution over strength in all future rounds, not just the final round. An abstraction algorithm that takes all future rounds into account is called potential aware. We present the first algorithm for computing potential-aware imperfect-recall abstractions using earth mover's distance. Experiments on no-limit Texas Hold'em show that our algorithm improves performance over the previously best approach.","Potential-Aware Imperfect-Recall Abstraction with Earth Mover's Distance in Imperfect-Information Games There is often a large disparity between the size of a game we wish to solve and the size of the largest instances solvable by the best algorithms; for example, a popular variant of poker has about $10^{165}$ nodes in its game tree, while the currently best approximate equilibrium-finding algorithms scale to games with around $10^{12}$ nodes. In order to approximate equilibrium strategies in these games, the leading approach is to create a sufficiently small strategic approximation of the full game, called an abstraction, and to solve that smaller game instead. The leading abstraction algorithm for imperfect-information games generates abstractions that have imperfect recall and are distribution aware, using $k$-means with the earth mover's distance metric to cluster similar states together. A distribution-aware abstraction groups states together at a given round if their full distributions over future strength are similar (as opposed to, for example, just the expectation of their strength). The leading algorithm considers distributions over future strength at the final round of the game.  However, one might benefit by considering the distribution over strength in all future rounds, not just the final round. An abstraction algorithm that takes all future rounds into account is called potential aware. We present the first algorithm for computing potential-aware imperfect-recall abstractions using earth mover's distance. Experiments on no-limit Texas Hold'em show that our algorithm improves performance over the previously best approach. Game Theory
Multiagent Systems
Game Solving
Game Abstraction
Imperfect Information
Poker",potentialawar imperfectrecal abstract earth mover distanc imperfectinform game often larg dispar size game wish solv size largest instanc solvabl best algorithm exampl popular variant poker 10165 node game tree current best approxim equilibriumfind algorithm scale game around 1012 node order approxim equilibrium strategi game lead approach creat suffici small strateg approxim full game call abstract solv smaller game instead lead abstract algorithm imperfectinform game generat abstract imperfect recal distribut awar use kmean earth mover distanc metric cluster similar state togeth distributionawar abstract group state togeth given round full distribut futur strength similar oppos exampl expect strength lead algorithm consid distribut futur strength final round game howev one might benefit consid distribut strength futur round final round abstract algorithm take futur round account call potenti awar present first algorithm comput potentialawar imperfectrecal abstract use earth mover distanc experi nolimit texa holdem show algorithm improv perform previous best approach game theori multiag system game solv game abstract imperfect inform poker,2,6.5681167,20.265057
Partial Multi-View Clustering,"Shao-Yuan Li, Yuan Jiang and Zhi-Hua Zhou","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","machine learning
unsupervised learning
multi-view clustering
partial view","MLA: Applications of Unsupervised Learning
MLA: Machine Learning Applications (General/other)
NMLA: Clustering
NMLA: Unsupervised Learning (Other)
NMLA: Machine Learning (General/other)","Real data are often with multiple modalities or coming from multiple channels, while multi-view clustering provides a natural formulation for generating clusters from such data. Previous studies assumed that each example appears in all views, or at least there is one view containing all examples. In real tasks, however, it is often the case that every view suffers from the missing of some data and therefore results in many partial examples, i.e., examples with some views missing. In this paper, we present possibly the first study on partial multi-view clustering. Our proposed approach, PVC, works by establishing a latent subspace where the instances corresponding to the same example in different views are close to each other, and the instances (belonging to different examples) in the same view are gathering smoothly. Experiments demonstrate the advantages of our proposed approach.","Partial Multi-View Clustering Real data are often with multiple modalities or coming from multiple channels, while multi-view clustering provides a natural formulation for generating clusters from such data. Previous studies assumed that each example appears in all views, or at least there is one view containing all examples. In real tasks, however, it is often the case that every view suffers from the missing of some data and therefore results in many partial examples, i.e., examples with some views missing. In this paper, we present possibly the first study on partial multi-view clustering. Our proposed approach, PVC, works by establishing a latent subspace where the instances corresponding to the same example in different views are close to each other, and the instances (belonging to different examples) in the same view are gathering smoothly. Experiments demonstrate the advantages of our proposed approach. machine learning
unsupervised learning
multi-view clustering
partial view",partial multiview cluster real data often multipl modal come multipl channel multiview cluster provid natur formul generat cluster data previous studi assum exampl appear view least one view contain exampl real task howev often case everi view suffer miss data therefor result mani partial exampl ie exampl view miss paper present possibl first studi partial multiview cluster propos approach pvc work establish latent subspac instanc correspond exampl differ view close instanc belong differ exampl view gather smooth experi demonstr advantag propos approach machin learn unsupervis learn multiview cluster partial view,7,-5.0264144,-15.847469
User Intent Identification from Online Discussions using a Joint Aspect-Action Topic Model,Ghasem Heyrani Nobari and Chua Tat Seng,"AI and the Web (AIW)
NLP and Machine Learning (NLPML)
NLP and Text Mining (NLPTM)
Novel Machine Learning Algorithms (NMLA)","Web
Online Discussions
Joint Modeling
Topic Modeling
Information Extraction
AI","AIW: Enhancing web search and information retrieval
AIW: Knowledge acquisition from the web
AIW: Machine learning and the web
NLPTM: Information Extraction
NMLA: Classification
NMLA: Clustering
NMLA: Data Mining and Knowledge Discovery
NMLA: Unsupervised Learning (Other)","Online discussions are growing as a popular, effective and reliable source of information for users because of their liveliness, flexibility and up-to-date information. Online discussions are usually developed and advanced by groups of users with various backgrounds and intents. However because of the diversities in topics and issues discussed by the users, supervised methods are not able to accurately model such dynamic conditions. In this paper, we propose a novel unsupervised generative model to derive aspect-action pairs from online discussions. The proposed method simultaneously captures and models these two features with their relationships that exist in each thread. We assume that each user post is generated by a mixture of aspect and action topics. Therefore, we design a model that captures the latent factors that incorporates the aspect types and intended actions, which describe how users develop a topic in a discussion. In order to demonstrate the effectiveness of our approach, we empirically compare our model against the state of the art methods on large-scale discussion dataset, crawled from apple discussions with over 3.3 million user posts from 340k discussion threads.","User Intent Identification from Online Discussions using a Joint Aspect-Action Topic Model Online discussions are growing as a popular, effective and reliable source of information for users because of their liveliness, flexibility and up-to-date information. Online discussions are usually developed and advanced by groups of users with various backgrounds and intents. However because of the diversities in topics and issues discussed by the users, supervised methods are not able to accurately model such dynamic conditions. In this paper, we propose a novel unsupervised generative model to derive aspect-action pairs from online discussions. The proposed method simultaneously captures and models these two features with their relationships that exist in each thread. We assume that each user post is generated by a mixture of aspect and action topics. Therefore, we design a model that captures the latent factors that incorporates the aspect types and intended actions, which describe how users develop a topic in a discussion. In order to demonstrate the effectiveness of our approach, we empirically compare our model against the state of the art methods on large-scale discussion dataset, crawled from apple discussions with over 3.3 million user posts from 340k discussion threads. Web
Online Discussions
Joint Modeling
Topic Modeling
Information Extraction
AI",user intent identif onlin discuss use joint aspectact topic model onlin discuss grow popular effect reliabl sourc inform user liveli flexibl uptod inform onlin discuss usual develop advanc group user various background intent howev divers topic issu discuss user supervis method abl accur model dynam condit paper propos novel unsupervis generat model deriv aspectact pair onlin discuss propos method simultan captur model two featur relationship exist thread assum user post generat mixtur aspect action topic therefor design model captur latent factor incorpor aspect type intend action describ user develop topic discuss order demonstr effect approach empir compar model state art method largescal discuss dataset crawl appl discuss 33 million user post 340k discuss thread web onlin discuss joint model topic model inform extract ai,0,9.05486,-0.6126309
Voting with Rank Dependent Scoring Rules,"Judy Goldsmith, Jerome Lang, Nicholas Mattei and Patrice Perny",Game Theory and Economic Paradigms (GTEP),"Computational Social Choice
Voting
Order Weighted Averages
Multi-agent Systems","GTEP: Auctions and Market-Based Systems
GTEP: Social Choice / Voting","Positional scoring rules in voting compute the score of an alternative by summing the
scores for the alternative induced by every vote.  This summation principle ensures that all
votes contribute equally to the score of an alternative.
We relax this assumption and, instead, aggregate scores by taking into account
the rank of a score in the ordered list of scores obtained from the votes.
This defines a new family of voting rules, rank-dependent scoring
rules (RDSRs), based on ordered weighted average (OWA) operators, which include
scoring rules, plurality, k-approval, and Olympic
averages. We study some properties of these rules, and show,
empirically, that certain RDSRs are less manipulable than Borda voting,
across a variety of statistical cultures.","Voting with Rank Dependent Scoring Rules Positional scoring rules in voting compute the score of an alternative by summing the
scores for the alternative induced by every vote.  This summation principle ensures that all
votes contribute equally to the score of an alternative.
We relax this assumption and, instead, aggregate scores by taking into account
the rank of a score in the ordered list of scores obtained from the votes.
This defines a new family of voting rules, rank-dependent scoring
rules (RDSRs), based on ordered weighted average (OWA) operators, which include
scoring rules, plurality, k-approval, and Olympic
averages. We study some properties of these rules, and show,
empirically, that certain RDSRs are less manipulable than Borda voting,
across a variety of statistical cultures. Computational Social Choice
Voting
Order Weighted Averages
Multi-agent Systems",vote rank depend score rule posit score rule vote comput score altern sum score altern induc everi vote summat principl ensur vote contribut equal score altern relax assumpt instead aggreg score take account rank score order list score obtain vote defin new famili vote rule rankdepend score rule rdsrs base order weight averag owa oper includ score rule plural kapprov olymp averag studi properti rule show empir certain rdsrs less manipul borda vote across varieti statist cultur comput social choic vote order weight averag multiag system,9,16.016397,-17.043316
Implementing GOLOG in Answer Set Programming,Malcolm Ryan,"Knowledge Representation and Reasoning (KRR)
Planning and Scheduling (PS)","GOLOG
Answer Set Programming
Planning
Search Control","KRR: Action, Change, and Causality
KRR: Logic Programming
PS: Planning (General/Other)","In this paper we investigate four different approaches to encoding domain-dependent control knowledge for Answer-Set Planning. Starting with a standard implementation of the answer-set planning language B, we add control knowledge expressed in the GOLOG logic programming language.  A naive encoding, following the original definitions of Reiter et al., is shown to scale poorly. We investigate three alternative codings based on the finite-state machine semantics of ConGOLOG. These perform better, although there is no clear winner. We discuss the pros and cons of each approach.","Implementing GOLOG in Answer Set Programming In this paper we investigate four different approaches to encoding domain-dependent control knowledge for Answer-Set Planning. Starting with a standard implementation of the answer-set planning language B, we add control knowledge expressed in the GOLOG logic programming language.  A naive encoding, following the original definitions of Reiter et al., is shown to scale poorly. We investigate three alternative codings based on the finite-state machine semantics of ConGOLOG. These perform better, although there is no clear winner. We discuss the pros and cons of each approach. GOLOG
Answer Set Programming
Planning
Search Control",implement golog answer set program paper investig four differ approach encod domaindepend control knowledg answerset plan start standard implement answerset plan languag b add control knowledg express golog logic program languag naiv encod follow origin definit reiter et al shown scale poor investig three altern code base finitest machin semant congolog perform better although clear winner discuss pros con approach golog answer set program plan search control,3,-14.665665,2.6346576
Agent Behavior Prediction and Its Generalization Analysis,"Fei Tian, Haifang Li, Wei Chen, Tao Qin, Enhong Chen and Tie-Yan Liu","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","generalization analysis
agent behavior model prediction
Markov Chain in Random Environments
empirical risk minimization algorithm","MLA: Environmental
MLA: Humanities
MLA: Applications of Supervised Learning
NMLA: Learning Theory","Machine learning algorithms have been applied to predict agent behaviors in real-world dynamic systems, such as advertiser behaviors in sponsored search and worker behaviors in crowdsourcing, and the prediction models have been used for the optimization of these systems. Note that the behavior data in these systems are generated by \emph{live} agents: once the systems change due to the adoption of the prediction models learnt from the behavior data, agents will observe (directly or indirectly) and respond to these changes by changing their own behaviors accordingly. As a result, the behavior data will evolve and will not be identically and independently distributed, which poses great challenges to the theoretical analysis on the machine learning algorithms for behavior prediction. To tackle this challenge, in this paper, we propose to use \emph{Markov Chain in Random Environments} (MCRE) to describe the behavior data, and perform generalization analysis of the machine learning algorithms on its basis. However, since the one-step transition probability matrix of MCRE depends on both previous states and the random environment, conventional techniques for generalization analysis cannot be directly applied. To address this issue, we propose a novel technique that transforms the original MCRE into a higher-dimensional time-homogeneous Markov chain. The new Markov chain involves more variables but is more regular, and thus easier to deal with. We prove the convergence of the new Markov chain when time approaches infinity. Then we prove a generalization bound for the machine learning algorithms on the behavior data generated by the new Markov chain, which depends on both the Markovian parameters and the covering number of the function class compounded by the loss function for behavior prediction and the behavior prediction model. To the best of our knowledge, this is the first work that performs the generalization analysis on data generated by complex processes in real-world dynamic systems.","Agent Behavior Prediction and Its Generalization Analysis Machine learning algorithms have been applied to predict agent behaviors in real-world dynamic systems, such as advertiser behaviors in sponsored search and worker behaviors in crowdsourcing, and the prediction models have been used for the optimization of these systems. Note that the behavior data in these systems are generated by \emph{live} agents: once the systems change due to the adoption of the prediction models learnt from the behavior data, agents will observe (directly or indirectly) and respond to these changes by changing their own behaviors accordingly. As a result, the behavior data will evolve and will not be identically and independently distributed, which poses great challenges to the theoretical analysis on the machine learning algorithms for behavior prediction. To tackle this challenge, in this paper, we propose to use \emph{Markov Chain in Random Environments} (MCRE) to describe the behavior data, and perform generalization analysis of the machine learning algorithms on its basis. However, since the one-step transition probability matrix of MCRE depends on both previous states and the random environment, conventional techniques for generalization analysis cannot be directly applied. To address this issue, we propose a novel technique that transforms the original MCRE into a higher-dimensional time-homogeneous Markov chain. The new Markov chain involves more variables but is more regular, and thus easier to deal with. We prove the convergence of the new Markov chain when time approaches infinity. Then we prove a generalization bound for the machine learning algorithms on the behavior data generated by the new Markov chain, which depends on both the Markovian parameters and the covering number of the function class compounded by the loss function for behavior prediction and the behavior prediction model. To the best of our knowledge, this is the first work that performs the generalization analysis on data generated by complex processes in real-world dynamic systems. generalization analysis
agent behavior model prediction
Markov Chain in Random Environments
empirical risk minimization algorithm",agent behavior predict general analysi machin learn algorithm appli predict agent behavior realworld dynam system advertis behavior sponsor search worker behavior crowdsourc predict model use optim system note behavior data system generat emphliv agent system chang due adopt predict model learnt behavior data agent observ direct indirect respond chang chang behavior accord result behavior data evolv ident independ distribut pose great challeng theoret analysi machin learn algorithm behavior predict tackl challeng paper propos use emphmarkov chain random environ mcre describ behavior data perform general analysi machin learn algorithm basi howev sinc onestep transit probabl matrix mcre depend previous state random environ convent techniqu general analysi cannot direct appli address issu propos novel techniqu transform origin mcre higherdimension timehomogen markov chain new markov chain involv variabl regular thus easier deal prove converg new markov chain time approach infin prove general bound machin learn algorithm behavior data generat new markov chain depend markovian paramet cover number function class compound loss function behavior predict behavior predict model best knowledg first work perform general analysi data generat complex process realworld dynam system general analysi agent behavior model predict markov chain random environ empir risk minim algorithm,7,-10.987237,-15.545715
Sparse Learning for Stochastic Composite Optimization,"Weizhong Zhang, Lijun Zhang, Yao Hu, Rong Jin, Deng Cai and Xiaofei He",Heuristic Search and Optimization (HSO),"Stochastic Optimization
Online Learning
Composite Gradient Mapping
Stochastic Gradient Descent",HSO: Optimization,"In this paper, we focus on the Sparse Learning for Stochastic Composite Optimization (SCO). Many algorithms have been proposed for SCO, and they have reached the optimal convergence rate $\mathcal{O}(1/T)$ recently. However, the sparsity of the solutions obtained by the existing methods is unsatisfactory due to mainly two reasons: (1) taking the average of the intermediate solutions as the final solution, (2) the reducing of the magnitude of the sparse regularizer in the iterations. In order to improve the sparse pattern of the solutions, we propose a simple but effective stochastic optimization scheme by adding a novel sparse online-to-batch conversion to the traditional algorithms for SCO. Two specific approaches are discussed in this paper to reveal the power of our scheme. The theoretical analysis shows that our scheme can find a solution with better sparse pattern without affecting the current optimal convergence rate. Experiment results on both synthetic and real-world data sets show that our proposed methods have obviously superior sparse recovery ability and have comparable convergence rate as the state-of-the-art algorithms for SCO.","Sparse Learning for Stochastic Composite Optimization In this paper, we focus on the Sparse Learning for Stochastic Composite Optimization (SCO). Many algorithms have been proposed for SCO, and they have reached the optimal convergence rate $\mathcal{O}(1/T)$ recently. However, the sparsity of the solutions obtained by the existing methods is unsatisfactory due to mainly two reasons: (1) taking the average of the intermediate solutions as the final solution, (2) the reducing of the magnitude of the sparse regularizer in the iterations. In order to improve the sparse pattern of the solutions, we propose a simple but effective stochastic optimization scheme by adding a novel sparse online-to-batch conversion to the traditional algorithms for SCO. Two specific approaches are discussed in this paper to reveal the power of our scheme. The theoretical analysis shows that our scheme can find a solution with better sparse pattern without affecting the current optimal convergence rate. Experiment results on both synthetic and real-world data sets show that our proposed methods have obviously superior sparse recovery ability and have comparable convergence rate as the state-of-the-art algorithms for SCO. Stochastic Optimization
Online Learning
Composite Gradient Mapping
Stochastic Gradient Descent",spars learn stochast composit optim paper focus spars learn stochast composit optim sco mani algorithm propos sco reach optim converg rate mathcalo1t recent howev sparsiti solut obtain exist method unsatisfactori due main two reason 1 take averag intermedi solut final solut 2 reduc magnitud spars regular iter order improv spars pattern solut propos simpl effect stochast optim scheme ad novel spars onlinetobatch convers tradit algorithm sco two specif approach discuss paper reveal power scheme theoret analysi show scheme find solut better spars pattern without affect current optim converg rate experi result synthet realworld data set show propos method obvious superior spars recoveri abil compar converg rate stateoftheart algorithm sco stochast optim onlin learn composit gradient map stochast gradient descent,4,3.0939066,-18.159092
Cross-lingual Knowledge Validation Based Taxonomy Derivation from Heterogeneous Online Wikis,"Zhigang Wang, Juanzi Li, Shuangjie Li, Mingyang Li and Jie Tang","AI and the Web (AIW)
Knowledge Representation and Reasoning (KRR)
NLP and Knowledge Representation (NLPKR)","Taxonomy Derivation
Knowledge Validation
Cross Lingual
Online Wikis","AIW: Knowledge acquisition from the web
AIW: Languages, tools, and methodologies for representing, managing, and visualizing semantic web data
AIW: Ontologies and the web: creation, extraction, evolution, mapping, merging, and alignment; tags and folksonomies
KRR: Knowledge Acquisition
KRR: Ontologies
NLPKR: Ontology Induction","Creating knowledge bases based on the crowd-sourced wikis, like Wikipedia, has attracted significant research interest in the field of intelligent Web. However, the derived taxonomies usually contain many mistakenly imported taxonomic relations due to the difference between the user-generated subsumption relations and the semantic taxonomic relations. Current approaches to solving the problem still suffer the following issues: (i) the heuristic-based methods strongly rely on specific language dependent rules. (ii) the corpus-based methods depend on a large-scale high-quality corpus, which is often unavailable.

In this paper, we formulate the cross-lingual taxonomy derivation problem as the problem of cross-lingual taxonomic relation prediction. We investigate different linguistic heuristics and language independent features, and propose a cross-lingual knowledge validation based dynamic adaptive boosting model to iteratively reinforce the performance of taxonomic relation prediction. The proposed approach successfully overcome the above issues, and experiments show that our approach significantly outperforms the designed state-of-the-art comparison methods.","Cross-lingual Knowledge Validation Based Taxonomy Derivation from Heterogeneous Online Wikis Creating knowledge bases based on the crowd-sourced wikis, like Wikipedia, has attracted significant research interest in the field of intelligent Web. However, the derived taxonomies usually contain many mistakenly imported taxonomic relations due to the difference between the user-generated subsumption relations and the semantic taxonomic relations. Current approaches to solving the problem still suffer the following issues: (i) the heuristic-based methods strongly rely on specific language dependent rules. (ii) the corpus-based methods depend on a large-scale high-quality corpus, which is often unavailable.

In this paper, we formulate the cross-lingual taxonomy derivation problem as the problem of cross-lingual taxonomic relation prediction. We investigate different linguistic heuristics and language independent features, and propose a cross-lingual knowledge validation based dynamic adaptive boosting model to iteratively reinforce the performance of taxonomic relation prediction. The proposed approach successfully overcome the above issues, and experiments show that our approach significantly outperforms the designed state-of-the-art comparison methods. Taxonomy Derivation
Knowledge Validation
Cross Lingual
Online Wikis",crosslingu knowledg valid base taxonomi deriv heterogen onlin wiki creat knowledg base base crowdsourc wiki like wikipedia attract signific research interest field intellig web howev deriv taxonomi usual contain mani mistaken import taxonom relat due differ usergener subsumpt relat semant taxonom relat current approach solv problem still suffer follow issu heuristicbas method strong reli specif languag depend rule ii corpusbas method depend largescal highqual corpus often unavail paper formul crosslingu taxonomi deriv problem problem crosslingu taxonom relat predict investig differ linguist heurist languag independ featur propos crosslingu knowledg valid base dynam adapt boost model iter reinforc perform taxonom relat predict propos approach success overcom issu experi show approach signific outperform design stateoftheart comparison method taxonomi deriv knowledg valid cross lingual onlin wiki,5,-3.5980668,-0.027571866
A Relevance-Based Compilation Method for Conformant Probabilistic Planning,Ran Taig and Ronen I. Brafman,Planning and Scheduling (PS),"Planning under uncertainty.
Conformant probabilistic planning.
Relevance based method.
compilation based approach.","PS: Probabilistic Planning
PS: Planning (General/Other)","Conformant probabilistic planning (CPP) differs from conformant planning (CP) by two key elements: the initial belief state is probabilistic,
and the conformant plan must achieve the goal with probability $\geq\theta$, for some $0<\theta\leq 1$. 
Taig and Brafman observed that one can reduce CPP to CP by finding a set of initial states whose probability $\geq\theta$, for which
a conformant plan exists. Previous solvers based on this idea used the underlying planner to select this set of states and to plan for them simultaneously.
Here, we suggest an alternative approach: Our planner starts with a separate preprocessing relevance analysis phase that determines a promising set of initial states on which to focus, and then calls an off-the-shelf conformant planner to solve the resulting problem. 
This approach has three major advantages. First, we can introduce specific and efficient relevance reasoning techniques for introducing the set of initial states, rather than depend on
the heuristic function used by the planner. Second, we can benefit from various optimizations used by existing conformant planners which are unsound if applied to the original 
CPP. Finally, we have the freedom to select among different existing CP solvers. Consequently, the new planner dominates previous solvers on almost all domains and scales to instances that were not solved before.","A Relevance-Based Compilation Method for Conformant Probabilistic Planning Conformant probabilistic planning (CPP) differs from conformant planning (CP) by two key elements: the initial belief state is probabilistic,
and the conformant plan must achieve the goal with probability $\geq\theta$, for some $0<\theta\leq 1$. 
Taig and Brafman observed that one can reduce CPP to CP by finding a set of initial states whose probability $\geq\theta$, for which
a conformant plan exists. Previous solvers based on this idea used the underlying planner to select this set of states and to plan for them simultaneously.
Here, we suggest an alternative approach: Our planner starts with a separate preprocessing relevance analysis phase that determines a promising set of initial states on which to focus, and then calls an off-the-shelf conformant planner to solve the resulting problem. 
This approach has three major advantages. First, we can introduce specific and efficient relevance reasoning techniques for introducing the set of initial states, rather than depend on
the heuristic function used by the planner. Second, we can benefit from various optimizations used by existing conformant planners which are unsound if applied to the original 
CPP. Finally, we have the freedom to select among different existing CP solvers. Consequently, the new planner dominates previous solvers on almost all domains and scales to instances that were not solved before. Planning under uncertainty.
Conformant probabilistic planning.
Relevance based method.
compilation based approach.",relevancebas compil method conform probabilist plan conform probabilist plan cpp differ conform plan cp two key element initi belief state probabilist conform plan must achiev goal probabl geqtheta 0thetaleq 1 taig brafman observ one reduc cpp cp find set initi state whose probabl geqtheta conform plan exist previous solver base idea use under planner select set state plan simultan suggest altern approach planner start separ preprocess relev analysi phase determin promis set initi state focus call offtheshelf conform planner solv result problem approach three major advantag first introduc specif effici relev reason techniqu introduc set initi state rather depend heurist function use planner second benefit various optim use exist conform planner unsound appli origin cpp final freedom select among differ exist cp solver consequ new planner domin previous solver almost domain scale instanc solv plan uncertainti conform probabilist plan relev base method compil base approach,3,-3.702355,17.78044
Incentivizing High-quality Content from Heterogeneous Users: On the Existence of Nash Equilibrium,"Yingce Xia, Tao Qin and Tie-Yan Liu","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","User generated content
Heterogeneous users
Equilibrium analysis","GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information
MAS: Mechanism Design","In this paper, we study the existence of pure Nash equilibrium (PNE) for the mechanisms used in Internet services (e.g., online reviews and question-answer websites) to incentivize users to generate high-quality content. Most existing work assumes that users are homogeneous and have the same ability. However, real-world users are heterogeneous and their abilities can be very different from each other due to their diverse background, culture, and profession. In this work, we consider heterogeneous users with the following framework: (1) the users are heterogeneous and each of them has a private type indicating the best quality of the content she can generate; (2) there is a fixed amount of reward to allocate to the participated users.  Under this framework, we study the existence of pure Nash equilibrium of several mechanisms composed by different allocation rules, action spaces, and information settings. We prove the existence of PNE for some mechanisms and the non-existence of PNE for some mechanisms. We also discuss how to find a PNE for those mechanisms with PNE either through a constructive way or a searching algorithm.","Incentivizing High-quality Content from Heterogeneous Users: On the Existence of Nash Equilibrium In this paper, we study the existence of pure Nash equilibrium (PNE) for the mechanisms used in Internet services (e.g., online reviews and question-answer websites) to incentivize users to generate high-quality content. Most existing work assumes that users are homogeneous and have the same ability. However, real-world users are heterogeneous and their abilities can be very different from each other due to their diverse background, culture, and profession. In this work, we consider heterogeneous users with the following framework: (1) the users are heterogeneous and each of them has a private type indicating the best quality of the content she can generate; (2) there is a fixed amount of reward to allocate to the participated users.  Under this framework, we study the existence of pure Nash equilibrium of several mechanisms composed by different allocation rules, action spaces, and information settings. We prove the existence of PNE for some mechanisms and the non-existence of PNE for some mechanisms. We also discuss how to find a PNE for those mechanisms with PNE either through a constructive way or a searching algorithm. User generated content
Heterogeneous users
Equilibrium analysis",incentiv highqual content heterogen user exist nash equilibrium paper studi exist pure nash equilibrium pne mechan use internet servic eg onlin review questionansw websit incentiv user generat highqual content exist work assum user homogen abil howev realworld user heterogen abil differ due divers background cultur profess work consid heterogen user follow framework 1 user heterogen privat type indic best qualiti content generat 2 fix amount reward alloc particip user framework studi exist pure nash equilibrium sever mechan compos differ alloc rule action space inform set prove exist pne mechan nonexist pne mechan also discuss find pne mechan pne either construct way search algorithm user generat content heterogen user equilibrium analysi,0,12.425217,11.342373
DJAO: A Communication-Constrained DCOP algorithm that combines features of ADOPT and Action-GDL,Yoonheui Kim and Victor Lesser,"Heuristic Search and Optimization (HSO)
Search and Constraint Satisfaction (SCS)","Distributed Problem Solving
DCOP
AND/OR search","HSO: Heuristic Search
HSO: Optimization
HSO: Distributed Search
MAS: Coordination and Collaboration
MAS: Distributed Problem Solving
SCS: Constraint Optimization
SCS: Distributed CSP/Optimization","In this paper we propose a novel DCOP algorithm, called DJAO, that is able to
efficiently find a solution with low communication overhead; this algorithm can be used for optimal and bounded approximate solutions by appropriately setting the error bounds. Our approach builds
 on distributed junction trees used in Action-GDL to represent independence relations
among variables. We construct an AND/OR search space based on these junction trees.
This new type of search space results in higher degrees for each OR node, consequently yielding a more efficient search graph in the distributed settings. DJAO uses a branch-and-bound search algorithm to distributedly find solutions within this search graph. We introduce a heuristics to compute the upper and lower bound estimates that the search starts with, which is integral to our approach for reducing communication overhead. We empirically evaluate our approach in various settings.","DJAO: A Communication-Constrained DCOP algorithm that combines features of ADOPT and Action-GDL In this paper we propose a novel DCOP algorithm, called DJAO, that is able to
efficiently find a solution with low communication overhead; this algorithm can be used for optimal and bounded approximate solutions by appropriately setting the error bounds. Our approach builds
 on distributed junction trees used in Action-GDL to represent independence relations
among variables. We construct an AND/OR search space based on these junction trees.
This new type of search space results in higher degrees for each OR node, consequently yielding a more efficient search graph in the distributed settings. DJAO uses a branch-and-bound search algorithm to distributedly find solutions within this search graph. We introduce a heuristics to compute the upper and lower bound estimates that the search starts with, which is integral to our approach for reducing communication overhead. We empirically evaluate our approach in various settings. Distributed Problem Solving
DCOP
AND/OR search",djao communicationconstrain dcop algorithm combin featur adopt actiongdl paper propos novel dcop algorithm call djao abl effici find solut low communic overhead algorithm use optim bound approxim solut appropri set error bound approach build distribut junction tree use actiongdl repres independ relat among variabl construct andor search space base junction tree new type search space result higher degre node consequ yield effici search graph distribut set djao use branchandbound search algorithm distribut find solut within search graph introduc heurist comput upper lower bound estim search start integr approach reduc communic overhead empir evalu approach various set distribut problem solv dcop andor search,4,-13.321258,8.649961
Oversubscription Planning: Complexity and Compilability,Meysam Aghighi and Peter Jonsson,Planning and Scheduling (PS),"oversubscription planning
computational complexity
compilability","PS: Deterministic Planning
PS: Planning (General/Other)","Many real-world planning problems are oversubscription problems where all
goals are not simultaneously achievable and the planner needs to find a
feasible subset. We present complexity results for the so-called partial satisfaction
and net benefit problems under various restrictions; this extends previous work
by van den Briel et al. Our results reveal strong connections between these
problems and with classical planning. We also present a method for efficiently
compiling oversubscription problems into the ordinary plan existence problem;
this generalizes previous work by Keyder & Geffner.","Oversubscription Planning: Complexity and Compilability Many real-world planning problems are oversubscription problems where all
goals are not simultaneously achievable and the planner needs to find a
feasible subset. We present complexity results for the so-called partial satisfaction
and net benefit problems under various restrictions; this extends previous work
by van den Briel et al. Our results reveal strong connections between these
problems and with classical planning. We also present a method for efficiently
compiling oversubscription problems into the ordinary plan existence problem;
this generalizes previous work by Keyder & Geffner. oversubscription planning
computational complexity
compilability",oversubscript plan complex compil mani realworld plan problem oversubscript problem goal simultan achiev planner need find feasibl subset present complex result socal partial satisfact net benefit problem various restrict extend previous work van den briel et al result reveal strong connect problem classic plan also present method effici compil oversubscript problem ordinari plan exist problem general previous work keyder geffner oversubscript plan comput complex compil,5,-3.2681496,17.738335
Monte Carlo Filtering using Kernel Embedding of Distributions,"Motonobu Kanagawa, Yu Nishiyama, Arthur Gretton and Kenji Fukumizu",Novel Machine Learning Algorithms (NMLA),"kernel method
kernel embedding of distributions
Monte Carlo filtering
state-space model",NMLA: Kernel Methods,"Recent advances of kernel methods have yielded the framework for representing probabilities using a reproducing kernel Hilbert space, called kernel embedding of distributions. In this paper, we propose a Monte Carlo filtering algorithm based on kernel embeddings. The proposed method is applied to state-space models where sampling from the transition model is possible, while the observation model is to be learned from training samples without assuming a parametric model. We derive convergence rates for the sampling method introduced to the kernel embedding approach. Experimental results on synthetic models and a real vision-based robot localization problem confirm the effectiveness of the proposed approach.","Monte Carlo Filtering using Kernel Embedding of Distributions Recent advances of kernel methods have yielded the framework for representing probabilities using a reproducing kernel Hilbert space, called kernel embedding of distributions. In this paper, we propose a Monte Carlo filtering algorithm based on kernel embeddings. The proposed method is applied to state-space models where sampling from the transition model is possible, while the observation model is to be learned from training samples without assuming a parametric model. We derive convergence rates for the sampling method introduced to the kernel embedding approach. Experimental results on synthetic models and a real vision-based robot localization problem confirm the effectiveness of the proposed approach. kernel method
kernel embedding of distributions
Monte Carlo filtering
state-space model",mont carlo filter use kernel embed distribut recent advanc kernel method yield framework repres probabl use reproduc kernel hilbert space call kernel embed distribut paper propos mont carlo filter algorithm base kernel embed propos method appli statespac model sampl transit model possibl observ model learn train sampl without assum parametr model deriv converg rate sampl method introduc kernel embed approach experiment result synthet model real visionbas robot local problem confirm effect propos approach kernel method kernel embed distribut mont carlo filter statespac model,4,-1.3550214,-23.676079
New Models for Competitive Contagion,"Moez Draief, Hoda Heidari and Michael Kearns",Game Theory and Economic Paradigms (GTEP),"Competitive Contagion
Networks
Connectivity
Endogenous budgets","GTEP: Game Theory
GTEP: Equilibrium","In this paper, we introduce and examine two new and natural models for competitive contagion in networks, a game-theoretic generalization of the viral marketing problem. In our setting, firms compete to maximize their market share in a network of consumers whose adoption decisions are stochastically determined by the choices of their neighbors. 

Building on the switching-selecting framework introduced by Goyal and Kearns (2012), we first introduce a new model in which the payoff to firms comprises not only the number of vertices who adopt their (competing) technologies, but also the network connectivity among those nodes. For a general class of stochastic dynamics driving the local adoption process, we derive upper bounds for (1) the (pure strategy) Price of Anarchy (PoA), which measures the inefficiency of resource use at equilibrium, and (2) the Budget Multiplier, which captures the extent to which the network amplifies the imbalances in the firms' initial budgets. These bounds depend on the firm budgets and the maximum degree of the network, but no other structural properties. In addition, we give general conditions under which the PoA and the Budget Multiplier can be unbounded.

We also introduce a model in which budgeting decisions are endogenous, rather than externally given as is typical in the viral marketing problem. In this setting, the firms are allowed to choose the number of seeds to initially infect (at a fixed cost per seed), as well as which nodes to select as seeds. In sharp contrast to the results of Goyal and Kearns (2012), we show that for almost any local adoption dynamics, there exists a family of graphs for which the PoA and Budget Multiplier are unbounded.","New Models for Competitive Contagion In this paper, we introduce and examine two new and natural models for competitive contagion in networks, a game-theoretic generalization of the viral marketing problem. In our setting, firms compete to maximize their market share in a network of consumers whose adoption decisions are stochastically determined by the choices of their neighbors. 

Building on the switching-selecting framework introduced by Goyal and Kearns (2012), we first introduce a new model in which the payoff to firms comprises not only the number of vertices who adopt their (competing) technologies, but also the network connectivity among those nodes. For a general class of stochastic dynamics driving the local adoption process, we derive upper bounds for (1) the (pure strategy) Price of Anarchy (PoA), which measures the inefficiency of resource use at equilibrium, and (2) the Budget Multiplier, which captures the extent to which the network amplifies the imbalances in the firms' initial budgets. These bounds depend on the firm budgets and the maximum degree of the network, but no other structural properties. In addition, we give general conditions under which the PoA and the Budget Multiplier can be unbounded.

We also introduce a model in which budgeting decisions are endogenous, rather than externally given as is typical in the viral marketing problem. In this setting, the firms are allowed to choose the number of seeds to initially infect (at a fixed cost per seed), as well as which nodes to select as seeds. In sharp contrast to the results of Goyal and Kearns (2012), we show that for almost any local adoption dynamics, there exists a family of graphs for which the PoA and Budget Multiplier are unbounded. Competitive Contagion
Networks
Connectivity
Endogenous budgets",new model competit contagion paper introduc examin two new natur model competit contagion network gametheoret general viral market problem set firm compet maxim market share network consum whose adopt decis stochast determin choic neighbor build switchingselect framework introduc goyal kearn 2012 first introduc new model payoff firm compris number vertic adopt compet technolog also network connect among node general class stochast dynam drive local adopt process deriv upper bound 1 pure strategi price anarchi poa measur ineffici resourc use equilibrium 2 budget multipli captur extent network amplifi imbal firm initi budget bound depend firm budget maximum degre network structur properti addit give general condit poa budget multipli unbound also introduc model budget decis endogen rather extern given typic viral market problem set firm allow choos number seed initi infect fix cost per seed well node select seed sharp contrast result goyal kearn 2012 show almost local adopt dynam exist famili graph poa budget multipli unbound competit contagion network connect endogen budget,5,9.299325,3.6063607
Hybrid Heterogeneous Transfer Learning through Deep Learning,"Joey Tianyi Zhou, Sinno Jialin Pan, Ivor W. Tsang and Yan Yan",Novel Machine Learning Algorithms (NMLA),"transfer learning
domain adaptation
deep learning","NMLA: Transfer, Adaptation, Multitask Learning","Most previous  heterogeneous transfer learning methods learn a cross-domain feature mapping between heterogeneous feature spaces based on a few cross-domain instance-correspondences, and these corresponding instances are assumed to be representatives in the source and target domains respectively. However, in many real-world scenarios, this assumption may not hold. As a result, the feature mapping may not be constructed precisely or the mapped data from the source (or target) domain may suffer from a data or feature shift issue in the target (or source) domain. In this case, a classifier trained on the labeled transformed-source-domain  data may not be useful for the target domain. In this paper, we present a new transfer learning framework called {\em Hybrid Heterogeneous Transfer Learning} (HHTL), which allows choosing the corresponding instances to be biased in either the source or target domain. Moreover,  we propose a deep learning approach to learn better feature representations of each domain data  to reduce the data shift issue, and a better feature mapping between cross-domain heterogeneous features  for the accurate transfer simultaneously. Extensive experiments on several multilingual sentiment classification tasks verify the effectiveness of our proposed approach compared with the baseline methods.","Hybrid Heterogeneous Transfer Learning through Deep Learning Most previous  heterogeneous transfer learning methods learn a cross-domain feature mapping between heterogeneous feature spaces based on a few cross-domain instance-correspondences, and these corresponding instances are assumed to be representatives in the source and target domains respectively. However, in many real-world scenarios, this assumption may not hold. As a result, the feature mapping may not be constructed precisely or the mapped data from the source (or target) domain may suffer from a data or feature shift issue in the target (or source) domain. In this case, a classifier trained on the labeled transformed-source-domain  data may not be useful for the target domain. In this paper, we present a new transfer learning framework called {\em Hybrid Heterogeneous Transfer Learning} (HHTL), which allows choosing the corresponding instances to be biased in either the source or target domain. Moreover,  we propose a deep learning approach to learn better feature representations of each domain data  to reduce the data shift issue, and a better feature mapping between cross-domain heterogeneous features  for the accurate transfer simultaneously. Extensive experiments on several multilingual sentiment classification tasks verify the effectiveness of our proposed approach compared with the baseline methods. transfer learning
domain adaptation
deep learning",hybrid heterogen transfer learn deep learn previous heterogen transfer learn method learn crossdomain featur map heterogen featur space base crossdomain instancecorrespond correspond instanc assum repres sourc target domain respect howev mani realworld scenario assumpt may hold result featur map may construct precis map data sourc target domain may suffer data featur shift issu target sourc domain case classifi train label transformedsourcedomain data may use target domain paper present new transfer learn framework call em hybrid heterogen transfer learn hhtl allow choos correspond instanc bias either sourc target domain moreov propos deep learn approach learn better featur represent domain data reduc data shift issu better featur map crossdomain heterogen featur accur transfer simultan extens experi sever multilingu sentiment classif task verifi effect propos approach compar baselin method transfer learn domain adapt deep learn,6,-15.049387,-10.822456
Decentralized Multi-Agent Reinforcement Learning in Average-Reward Dynamic DCOPs,"Nguyen Duc Thien, William Yeoh, Hoong Chuin Lau, Shlomo Zilberstein and Chongjie Zhang","Multiagent Systems (MAS)
Search and Constraint Satisfaction (SCS)","Distributed Constraint Optimization Problems
DCOPs
Reinforcement Learning","MAS: Coordination and Collaboration
MAS: Distributed Problem Solving
MAS: Multiagent Learning
SCS: Distributed CSP/Optimization","Researchers have introduced the Dynamic Distributed Constraint Optimization Problem (Dynamic DCOP) formulation to model dynamically changing multi-agent coordination problems, where a dynamic DCOP is a sequence of (static canonical) DCOPs, each partially different from the DCOP preceding it. Existing work typically assumes that the problem in each time step is decoupled from the problems in other time steps, which might not hold in some applications. Therefore, in this paper, we make the following contributions: (i) We introduce a new model, called Markovian Dynamic DCOPs (MD-DCOPs), where the DCOP in the next time step is a function of the value assignments in the current time step; (ii) We introduce two distributed reinforcement learning algorithms, the Distributed RVI Q-learning algorithm and the Distributed R-learning algorithm, that balance exploration and exploitation to solve MD-DCOPs in an online manner; and (iii) We empirically evaluate them against an existing multi-arm bandit DCOP algorithm on dynamic DCOPs.","Decentralized Multi-Agent Reinforcement Learning in Average-Reward Dynamic DCOPs Researchers have introduced the Dynamic Distributed Constraint Optimization Problem (Dynamic DCOP) formulation to model dynamically changing multi-agent coordination problems, where a dynamic DCOP is a sequence of (static canonical) DCOPs, each partially different from the DCOP preceding it. Existing work typically assumes that the problem in each time step is decoupled from the problems in other time steps, which might not hold in some applications. Therefore, in this paper, we make the following contributions: (i) We introduce a new model, called Markovian Dynamic DCOPs (MD-DCOPs), where the DCOP in the next time step is a function of the value assignments in the current time step; (ii) We introduce two distributed reinforcement learning algorithms, the Distributed RVI Q-learning algorithm and the Distributed R-learning algorithm, that balance exploration and exploitation to solve MD-DCOPs in an online manner; and (iii) We empirically evaluate them against an existing multi-arm bandit DCOP algorithm on dynamic DCOPs. Distributed Constraint Optimization Problems
DCOPs
Reinforcement Learning",decentr multiag reinforc learn averagereward dynam dcop research introduc dynam distribut constraint optim problem dynam dcop formul model dynam chang multiag coordin problem dynam dcop sequenc static canon dcop partial differ dcop preced exist work typic assum problem time step decoupl problem time step might hold applic therefor paper make follow contribut introduc new model call markovian dynam dcop mddcop dcop next time step function valu assign current time step ii introduc two distribut reinforc learn algorithm distribut rvi qlearn algorithm distribut rlearn algorithm balanc explor exploit solv mddcop onlin manner iii empir evalu exist multiarm bandit dcop algorithm dynam dcop distribut constraint optim problem dcop reinforc learn,3,-18.584364,9.78181
Approximate Equilibrium and Incentivizing Social Coordination,Elliot Anshelevich and Shreyas Sekar,Game Theory and Economic Paradigms (GTEP),"Approximate Nash Equilibrium
Coordination Games
Price of Stability
Price of Anarchy
Network Games","GTEP: Game Theory
GTEP: Coordination and Collaboration
GTEP: Equilibrium","We study techniques to incentivize self-interested agents to form socially desirable solutions in scenarios where they benefit from mutual coordination. Towards this end, we consider coordination games where agents have different intrinsic preferences but they stand to gain if others choose the same strategy as them. For non-trivial versions of our game, stable solutions like Nash Equilibrium may not exist, or may be socially inefficient even when they do exist. This motivates us to focus on designing efficient algorithms to compute (almost) stable solutions like Approximate Equilibrium that can be be realized if agents are provided some additional incentives. Alternatively, approximate stability corresponds to the addition of a switching cost that agents have to pay in order to deviate. Our results apply in many settings like adoption of new products, project selection, and group formation, where a central authority can direct agents towards a strategy but agents may defect if they have better alternatives. We show that for any given instance, we can either compute a high quality approximate equilibrium or a near-optimal solution that can be stabilized by providing a small fraction of the social welfare to all players. Our results imply that little influence is necessary in order to ensure that selfish players coordinate and form socially efficient solutions.","Approximate Equilibrium and Incentivizing Social Coordination We study techniques to incentivize self-interested agents to form socially desirable solutions in scenarios where they benefit from mutual coordination. Towards this end, we consider coordination games where agents have different intrinsic preferences but they stand to gain if others choose the same strategy as them. For non-trivial versions of our game, stable solutions like Nash Equilibrium may not exist, or may be socially inefficient even when they do exist. This motivates us to focus on designing efficient algorithms to compute (almost) stable solutions like Approximate Equilibrium that can be be realized if agents are provided some additional incentives. Alternatively, approximate stability corresponds to the addition of a switching cost that agents have to pay in order to deviate. Our results apply in many settings like adoption of new products, project selection, and group formation, where a central authority can direct agents towards a strategy but agents may defect if they have better alternatives. We show that for any given instance, we can either compute a high quality approximate equilibrium or a near-optimal solution that can be stabilized by providing a small fraction of the social welfare to all players. Our results imply that little influence is necessary in order to ensure that selfish players coordinate and form socially efficient solutions. Approximate Nash Equilibrium
Coordination Games
Price of Stability
Price of Anarchy
Network Games",approxim equilibrium incentiv social coordin studi techniqu incentiv selfinterest agent form social desir solut scenario benefit mutual coordin toward end consid coordin game agent differ intrins prefer stand gain other choos strategi nontrivi version game stabl solut like nash equilibrium may exist may social ineffici even exist motiv us focus design effici algorithm comput almost stabl solut like approxim equilibrium realiz agent provid addit incent altern approxim stabil correspond addit switch cost agent pay order deviat result appli mani set like adopt new product project select group format central author direct agent toward strategi agent may defect better altern show given instanc either comput high qualiti approxim equilibrium nearoptim solut stabil provid small fraction social welfar player result impli littl influenc necessari order ensur selfish player coordin form social effici solut approxim nash equilibrium coordin game price stabil price anarchi network game,2,7.375207,14.858864
Qualitative Reasoning with Modelica Models,"Matthew Klenk, Daniel Bobrow, Johan De Kleer and Bill Janssen","Applications (APP)
Knowledge Representation and Reasoning (KRR)","qualitative reasoning
design
Modelica
model-based reasoning","APP: Other Applications
KRR: Knowledge Representation Languages
KRR: Qualitative Reasoning","Applications of qualitative reasoning to engineering design face a knowledge acquisition challenge.  Designers are not fluent in qualitative modeling languages and techniques.  To overcome this barrier, we perform qualitative simulation using models solely written in Modelica, a popular language among designers for modeling hybrid systems.  This paper has two contributions: (1) a formalization of the relationship between the results of the Modelica and qualitative simulations for the same model along with a novel algorithm for computing the consequences of events in qualitative simulation, and (2) three classes of additional constraints that reduce the number of unrealizable trajectories when performing qualitative simulation with Modelica models.  We support these contributions with  examples and a case study that shows a reduction by a factor of six the size of the qualitative simulation.","Qualitative Reasoning with Modelica Models Applications of qualitative reasoning to engineering design face a knowledge acquisition challenge.  Designers are not fluent in qualitative modeling languages and techniques.  To overcome this barrier, we perform qualitative simulation using models solely written in Modelica, a popular language among designers for modeling hybrid systems.  This paper has two contributions: (1) a formalization of the relationship between the results of the Modelica and qualitative simulations for the same model along with a novel algorithm for computing the consequences of events in qualitative simulation, and (2) three classes of additional constraints that reduce the number of unrealizable trajectories when performing qualitative simulation with Modelica models.  We support these contributions with  examples and a case study that shows a reduction by a factor of six the size of the qualitative simulation. qualitative reasoning
design
Modelica
model-based reasoning",qualit reason modelica model applic qualit reason engin design face knowledg acquisit challeng design fluent qualit model languag techniqu overcom barrier perform qualit simul use model sole written modelica popular languag among design model hybrid system paper two contribut 1 formal relationship result modelica qualit simul model along novel algorithm comput consequ event qualit simul 2 three class addit constraint reduc number unrealiz trajectori perform qualit simul modelica model support contribut exampl case studi show reduct factor six size qualit simul qualit reason design modelica modelbas reason,0,-4.818712,11.657135
Evaluating Trauma Patients: Addressing Missing Covariates Using Joint Optimization,"Alex Van Esbroeck, Satinder Singh, Ilan Rubinfeld and Zeeshan Syed",Machine Learning Applications (MLA),"missing values
classification
trauma","MLA: Bio/Medicine
MLA: Applications of Supervised Learning","Missing values are a common problem when applying classification algorithms to real-world medical data. This is especially true for trauma patients, where clinical variables frequently go uncollected due to the severity and urgency of their condition. Standard approaches to handling missingness first learn a model to estimate missing data values, and subsequently train and evaluate a classifier using data imputed with this model. Recently, several works have demonstrated the benefits of jointly estimating the imputation model and classifier parameters. However existing approaches make assumptions that limit their utility with many real-world medical datasets, particularly that data elements are missing at random, which is often invalid. We present a novel approach to jointly learning the imputation model and classifier. Unlike existing methods, the proposed approach makes no assumptions about the missingness of the data, can be used with arbitrary probabilistic data models and classification loss functions, and can be used when both training and testing data have missing values. We investigate the approach's utility in the prediction of several patient outcomes in a large national registry of trauma patients, and find that it significantly outperforms standard sequential methods.","Evaluating Trauma Patients: Addressing Missing Covariates Using Joint Optimization Missing values are a common problem when applying classification algorithms to real-world medical data. This is especially true for trauma patients, where clinical variables frequently go uncollected due to the severity and urgency of their condition. Standard approaches to handling missingness first learn a model to estimate missing data values, and subsequently train and evaluate a classifier using data imputed with this model. Recently, several works have demonstrated the benefits of jointly estimating the imputation model and classifier parameters. However existing approaches make assumptions that limit their utility with many real-world medical datasets, particularly that data elements are missing at random, which is often invalid. We present a novel approach to jointly learning the imputation model and classifier. Unlike existing methods, the proposed approach makes no assumptions about the missingness of the data, can be used with arbitrary probabilistic data models and classification loss functions, and can be used when both training and testing data have missing values. We investigate the approach's utility in the prediction of several patient outcomes in a large national registry of trauma patients, and find that it significantly outperforms standard sequential methods. missing values
classification
trauma",evalu trauma patient address miss covari use joint optim miss valu common problem appli classif algorithm realworld medic data especi true trauma patient clinic variabl frequent go uncollect due sever urgenc condit standard approach handl missing first learn model estim miss data valu subsequ train evalu classifi use data imput model recent sever work demonstr benefit joint estim imput model classifi paramet howev exist approach make assumpt limit util mani realworld medic dataset particular data element miss random often invalid present novel approach joint learn imput model classifi unlik exist method propos approach make assumpt missing data use arbitrari probabilist data model classif loss function use train test data miss valu investig approach util predict sever patient outcom larg nation registri trauma patient find signific outperform standard sequenti method miss valu classif trauma,6,-7.192364,-6.9778266
Tightening Bounds for Bayesian Network Structure Learning,"Xiannian Fan, Changhe Yuan and Brandon Malone",Reasoning under Uncertainty (RU),"Bayesian network
structure learning
lower and upper bounds","NMLA: Graphical Model Learning
RU: Bayesian Networks","A recent breadth-first branch and bound algorithm (BFBnB) for learning Bayesian network structures (Malone et al. 2011) uses two bounds to prune the search space for better efficiency; one is a lower bound calculated from pattern database heuristics, and the other is an upper bound obtained by a hill climbing search. Whenever the lower bound of a search path exceeds its upper bound, the path is guaranteed to lead to suboptimal solutions and is discarded immediately. This paper introduces methods for tightening the bounds. The lower bound is tightened by using more informed variable groupings in creating the pattern databases, and the upper bound is tightened using an anytime learning algorithm. Empirical results show that these bounds improve the efficiency of Bayesian network learning by two to three orders of magnitude.","Tightening Bounds for Bayesian Network Structure Learning A recent breadth-first branch and bound algorithm (BFBnB) for learning Bayesian network structures (Malone et al. 2011) uses two bounds to prune the search space for better efficiency; one is a lower bound calculated from pattern database heuristics, and the other is an upper bound obtained by a hill climbing search. Whenever the lower bound of a search path exceeds its upper bound, the path is guaranteed to lead to suboptimal solutions and is discarded immediately. This paper introduces methods for tightening the bounds. The lower bound is tightened by using more informed variable groupings in creating the pattern databases, and the upper bound is tightened using an anytime learning algorithm. Empirical results show that these bounds improve the efficiency of Bayesian network learning by two to three orders of magnitude. Bayesian network
structure learning
lower and upper bounds",tighten bound bayesian network structur learn recent breadthfirst branch bound algorithm bfbnb learn bayesian network structur malon et al 2011 use two bound prune search space better effici one lower bound calcul pattern databas heurist upper bound obtain hill climb search whenev lower bound search path exceed upper bound path guarante lead suboptim solut discard immedi paper introduc method tighten bound lower bound tighten use inform variabl group creat pattern databas upper bound tighten use anytim learn algorithm empir result show bound improv effici bayesian network learn two three order magnitud bayesian network structur learn lower upper bound,5,-13.797748,9.942374
Pathway Specification and Comparative Queries: A High Level Language with Petri Net Semantics,Saadat Anwar and Chitta Baral,Knowledge Representation and Reasoning (KRR),"Biological pathways
Comparative queries
Question answering","APP: Biomedical / Bioinformatics
KRR: Knowledge Representation Languages","Understanding biological pathways is an important activity in the biological domain for drug development. Due to the parallelism and complexity inherent in pathways, computer models that can answer queries about pathways are needed. A researcher may ask ``what-if'' questions comparing alternate scenarios, that require deeper understanding of the underlying model. In this paper, we present overview of such a system we developed and an English-like high level language to expressed pathways and queries. Our language is inspired by high level action and query languages and it uses Petri Net execution semantics.","Pathway Specification and Comparative Queries: A High Level Language with Petri Net Semantics Understanding biological pathways is an important activity in the biological domain for drug development. Due to the parallelism and complexity inherent in pathways, computer models that can answer queries about pathways are needed. A researcher may ask ``what-if'' questions comparing alternate scenarios, that require deeper understanding of the underlying model. In this paper, we present overview of such a system we developed and an English-like high level language to expressed pathways and queries. Our language is inspired by high level action and query languages and it uses Petri Net execution semantics. Biological pathways
Comparative queries
Question answering",pathway specif compar queri high level languag petri net semant understand biolog pathway import activ biolog domain drug develop due parallel complex inher pathway comput model answer queri pathway need research may ask whatif question compar altern scenario requir deeper understand under model paper present overview system develop englishlik high level languag express pathway queri languag inspir high level action queri languag use petri net execut semant biolog pathway compar queri question answer,8,22.540834,3.2026763
Solving the Inferential Frame Problem in the General Game Description Language,"Javier Romero, Abdallah Saffidine and Michael Thielscher","Game Playing and Interactive Entertainment (GPIE)
Knowledge Representation and Reasoning (KRR)","General game playing
Inferential frame problem
Game description language","GPIE: General Game Playing
KRR: Action, Change, and Causality","The Game Description Language GDL is the standard input language for general game-playing systems. While players can gain a lot of traction by an efficient inference algorithm for GDL, state-of-the-art reasoners suffer from a variant of a classical KR problem, the inferential frame problem. We present a method by which general game players can transform any given game description into a representation that solves this problem. Our experimental results demonstrate that with the help of automatically generated domain knowledge, a significant speedup can thus be obtained for the majority of the game descriptions from the AAAI competition.","Solving the Inferential Frame Problem in the General Game Description Language The Game Description Language GDL is the standard input language for general game-playing systems. While players can gain a lot of traction by an efficient inference algorithm for GDL, state-of-the-art reasoners suffer from a variant of a classical KR problem, the inferential frame problem. We present a method by which general game players can transform any given game description into a representation that solves this problem. Our experimental results demonstrate that with the help of automatically generated domain knowledge, a significant speedup can thus be obtained for the majority of the game descriptions from the AAAI competition. General game playing
Inferential frame problem
Game description language",solv inferenti frame problem general game descript languag game descript languag gdl standard input languag general gameplay system player gain lot traction effici infer algorithm gdl stateoftheart reason suffer variant classic kr problem inferenti frame problem present method general game player transform given game descript represent solv problem experiment result demonstr help automat generat domain knowledg signific speedup thus obtain major game descript aaai competit general game play inferenti frame problem game descript languag,2,7.497527,21.332954
Testable Implications of Linear Structural Equation Models,"Bryant Chen, Judea Pearl and Jin Tian","Knowledge Representation and Reasoning (KRR)
Reasoning under Uncertainty (RU)","structural equation models
causality
causal models
linear
testable implications
constraints
verma constraints
overidentifying constraints
overidentifying restrictions
graphical models","KRR: Action, Change, and Causality
RU: Graphical Models (Other)","In causal inference, all methods of model learning rely on testable implications, namely, properties of the joint distribution that are dictated by the model structure.  These constraints, if not satisfied in the data, allow us to reject or modify the model. Most common methods of testing a linear structural equation model (SEM) rely on the likelihood ratio or chi-square test which simultaneously tests all of the restrictions implied by the model.  Local constraints, on the other hand, offer increased power (Bollen and Pearl, 2013; McDonald, 2002) and, in the case of failure, provide the modeler with insight for revising the model specification. One strategy of uncovering local constraints in linear SEMs is to search for overidentified path coefficients. While these overidentifying constraints are well known, no method has been given for systematically discovering them. In this paper, we extend the half-trek criterion of (Foygel et al., 2012) to identify a larger set of structural coefficients and use it to systematically discover overidentifying constraints. Still open is the question of whether our algorithm is complete.","Testable Implications of Linear Structural Equation Models In causal inference, all methods of model learning rely on testable implications, namely, properties of the joint distribution that are dictated by the model structure.  These constraints, if not satisfied in the data, allow us to reject or modify the model. Most common methods of testing a linear structural equation model (SEM) rely on the likelihood ratio or chi-square test which simultaneously tests all of the restrictions implied by the model.  Local constraints, on the other hand, offer increased power (Bollen and Pearl, 2013; McDonald, 2002) and, in the case of failure, provide the modeler with insight for revising the model specification. One strategy of uncovering local constraints in linear SEMs is to search for overidentified path coefficients. While these overidentifying constraints are well known, no method has been given for systematically discovering them. In this paper, we extend the half-trek criterion of (Foygel et al., 2012) to identify a larger set of structural coefficients and use it to systematically discover overidentifying constraints. Still open is the question of whether our algorithm is complete. structural equation models
causality
causal models
linear
testable implications
constraints
verma constraints
overidentifying constraints
overidentifying restrictions
graphical models",testabl implic linear structur equat model causal infer method model learn reli testabl implic name properti joint distribut dictat model structur constraint satisfi data allow us reject modifi model common method test linear structur equat model sem reli likelihood ratio chisquar test simultan test restrict impli model local constraint hand offer increas power bollen pearl 2013 mcdonald 2002 case failur provid model insight revis model specif one strategi uncov local constraint linear sem search overidentifi path coeffici overidentifi constraint well known method given systemat discov paper extend halftrek criterion foygel et al 2012 identifi larger set structur coeffici use systemat discov overidentifi constraint still open question whether algorithm complet structur equat model causal causal model linear testabl implic constraint verma constraint overidentifi constraint overidentifi restrict graphic model,1,-5.395889,4.6889496
Manifold Spanning Graphs,Cj Carey and Sridhar Mahadevan,Novel Machine Learning Algorithms (NMLA),"graph construction
manifold learning
topology","NMLA: Dimension Reduction/Feature Selection
NMLA: Unsupervised Learning (Other)","Graph construction is the essential first step for nearly all manifold learning algorithms. While many applications assume that a simple k-nearest or epsilon-close neighbors graph will accurately model the topology of the underlying manifold, these methods often require expert tuning and may not produce high quality graphs. In this paper, the hyperparameter sensitivity of existing graph construction methods is demonstrated. We then present a new algorithm for unsupervised graph construction, based on minimal assumptions about the input data and its manifold structure. Notably, this method requires no hyperparameter tuning.","Manifold Spanning Graphs Graph construction is the essential first step for nearly all manifold learning algorithms. While many applications assume that a simple k-nearest or epsilon-close neighbors graph will accurately model the topology of the underlying manifold, these methods often require expert tuning and may not produce high quality graphs. In this paper, the hyperparameter sensitivity of existing graph construction methods is demonstrated. We then present a new algorithm for unsupervised graph construction, based on minimal assumptions about the input data and its manifold structure. Notably, this method requires no hyperparameter tuning. graph construction
manifold learning
topology",manifold span graph graph construct essenti first step near manifold learn algorithm mani applic assum simpl knearest epsilonclos neighbor graph accur model topolog under manifold method often requir expert tune may produc high qualiti graph paper hyperparamet sensit exist graph construct method demonstr present new algorithm unsupervis graph construct base minim assumpt input data manifold structur notabl method requir hyperparamet tune graph construct manifold learn topolog,5,-7.688337,8.187658
Learning Relative Similarity by Stochastic Dual Coordinate Ascent,"Pengcheng Wu, Yi Ding, Peilin Zhao, Steven C.H. Hoi and Chunyan Miao","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","distance metric learning
similarity learning
online learning
retrieval","MLA: Applications of Supervised Learning
MLA: Machine Learning Applications (General/other)
NMLA: Online Learning
NMLA: Supervised Learning (Other)
VIS: Image and Video Retrieval
VIS: Statistical Methods and Learning","Learning relative similarity from pairwise instances is an important problem in machine learning and potentially very useful for many applications, such as image and text retrieval. Despite being studied for years, some existing methods solved by Stochastic Gradient Descent (SGD) techniques generally suffer from slow convergence. In this paper, we investigate the application of Stochastic Dual Coordinate Ascent (SDCA) technique to tackle the optimization task of relative similarity learning by extending from vector to matrix parameters. Theoretically, we prove the optimal linear convergence rate for the proposed SDCA algorithm, beating the well-known sublinear convergence rate by the previous best metric learning algorithms. Empirically, we conduct extensive experiments on both standard and large-scale data sets to validate the effectiveness of the proposed algorithm for retrieval tasks.","Learning Relative Similarity by Stochastic Dual Coordinate Ascent Learning relative similarity from pairwise instances is an important problem in machine learning and potentially very useful for many applications, such as image and text retrieval. Despite being studied for years, some existing methods solved by Stochastic Gradient Descent (SGD) techniques generally suffer from slow convergence. In this paper, we investigate the application of Stochastic Dual Coordinate Ascent (SDCA) technique to tackle the optimization task of relative similarity learning by extending from vector to matrix parameters. Theoretically, we prove the optimal linear convergence rate for the proposed SDCA algorithm, beating the well-known sublinear convergence rate by the previous best metric learning algorithms. Empirically, we conduct extensive experiments on both standard and large-scale data sets to validate the effectiveness of the proposed algorithm for retrieval tasks. distance metric learning
similarity learning
online learning
retrieval",learn relat similar stochast dual coordin ascent learn relat similar pairwis instanc import problem machin learn potenti use mani applic imag text retriev despit studi year exist method solv stochast gradient descent sgd techniqu general suffer slow converg paper investig applic stochast dual coordin ascent sdca techniqu tackl optim task relat similar learn extend vector matrix paramet theoret prove optim linear converg rate propos sdca algorithm beat wellknown sublinear converg rate previous best metric learn algorithm empir conduct extens experi standard largescal data set valid effect propos algorithm retriev task distanc metric learn similar learn onlin learn retriev,4,1.2244116,-14.818267
locality preserving projection via multi-objective learning for domain adaptation,"Le Shu, Tianyang Ma and Longin Latecki","Knowledge Representation and Reasoning (KRR)
Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","locality preserving projection
multi-objective learning
domain adaptation","KRR: Knowledge Representation (General/Other)
NMLA: Transfer, Adaptation, Multitask Learning","In many practical cases, we need to generalize a model trained in a source domain to a new target domain.However, the distribution of these two domains may differ very significantly, especially sometimes some crucial target features may not have support in the source domain. This paper propose a novel locality preserving projection methods for domain adaptation task,which can find a linear mapping preserving the 'intrinsic structure' for both the source and the target domain. In this work, we first construct two graphs encoding the neighborhood information for the source domain and target domain separately. We then try to find linear projection coefficients which have the property of locality preserving for each graph.Instead of combing the two objective function under compatibility assumption and requiring the user to decide the importance of each objective function. We propose a multi-objective formulation for this problem and solve it simultaneously using pareto optimization.The pareto frontier captures all possible good linear projection vectors that are prefered by one or more objectives.The effectiveness of our approach is justified by both theoretical analysis and empirical results on real world data sets. The new feature representation  shows better prediction accuracy as our experiment clearly demonstrated.","locality preserving projection via multi-objective learning for domain adaptation In many practical cases, we need to generalize a model trained in a source domain to a new target domain.However, the distribution of these two domains may differ very significantly, especially sometimes some crucial target features may not have support in the source domain. This paper propose a novel locality preserving projection methods for domain adaptation task,which can find a linear mapping preserving the 'intrinsic structure' for both the source and the target domain. In this work, we first construct two graphs encoding the neighborhood information for the source domain and target domain separately. We then try to find linear projection coefficients which have the property of locality preserving for each graph.Instead of combing the two objective function under compatibility assumption and requiring the user to decide the importance of each objective function. We propose a multi-objective formulation for this problem and solve it simultaneously using pareto optimization.The pareto frontier captures all possible good linear projection vectors that are prefered by one or more objectives.The effectiveness of our approach is justified by both theoretical analysis and empirical results on real world data sets. The new feature representation  shows better prediction accuracy as our experiment clearly demonstrated. locality preserving projection
multi-objective learning
domain adaptation",local preserv project via multiobject learn domain adapt mani practic case need general model train sourc domain new target domainhowev distribut two domain may differ signific especi sometim crucial target featur may support sourc domain paper propos novel local preserv project method domain adapt taskwhich find linear map preserv intrins structur sourc target domain work first construct two graph encod neighborhood inform sourc domain target domain separ tri find linear project coeffici properti local preserv graphinstead comb two object function compat assumpt requir user decid import object function propos multiobject formul problem solv simultan use pareto optimizationth pareto frontier captur possibl good linear project vector prefer one objectivesth effect approach justifi theoret analysi empir result real world data set new featur represent show better predict accuraci experi clear demonstr local preserv project multiobject learn domain adapt,6,-15.959176,-13.084754
Internally Stable Kidney Exchange,"Yicheng Liu, Pingzhong Tang and Wenyi Fang",Multiagent Systems (MAS),"Kidney exchange
Stable matching
Maximum weighted matching","GTEP: Auctions and Market-Based Systems
MAS: Mechanism Design","Stability is a central concept in matching-based mechanism design. It imposes a fundamental requirement that no subset of agents could beneficially deviate from the prescribed outcome. However, deployment of stability in the current design of kidney exchange mechanisms presents at least two challenges. First, it reduces social welfare of the mechanism and sometimes prevent the mechanism from producing any outcome at all. Second, it sometimes incurs computation cost to clear the mechanism.

In this paper, we propose an alternative notion of stability. Our theoretical and experimental studies demonstrate that the new notion of stability addresses both challenges above and could be deployed in the current kidney exchange design.","Internally Stable Kidney Exchange Stability is a central concept in matching-based mechanism design. It imposes a fundamental requirement that no subset of agents could beneficially deviate from the prescribed outcome. However, deployment of stability in the current design of kidney exchange mechanisms presents at least two challenges. First, it reduces social welfare of the mechanism and sometimes prevent the mechanism from producing any outcome at all. Second, it sometimes incurs computation cost to clear the mechanism.

In this paper, we propose an alternative notion of stability. Our theoretical and experimental studies demonstrate that the new notion of stability addresses both challenges above and could be deployed in the current kidney exchange design. Kidney exchange
Stable matching
Maximum weighted matching",intern stabl kidney exchang stabil central concept matchingbas mechan design impos fundament requir subset agent could benefici deviat prescrib outcom howev deploy stabil current design kidney exchang mechan present least two challeng first reduc social welfar mechan sometim prevent mechan produc outcom second sometim incur comput cost clear mechan paper propos altern notion stabil theoret experiment studi demonstr new notion stabil address challeng could deploy current kidney exchang design kidney exchang stabl match maximum weight match,9,16.197054,16.320526
Intra-view and Inter-view Supervised Correlation Analysis for Multi-view Feature Learning,"Xiao-Yuan Jing, Rui-Min Hu, Yang-Ping Zhu, Shan-Shan Wu, Chao Liang and Jing-Yu Yang",Novel Machine Learning Algorithms (NMLA),"Canonical correlation analysis (CCA)
Multi-view supervised feature learning
Inter-view and intra-view supervised correlation analysis (I2SCA)
Analytical solution
Kernelized extension","NMLA: Dimension Reduction/Feature Selection
NMLA: Supervised Learning (Other)
VIS: Statistical Methods and Learning","The object always can be observed at multiple views, and multi-view feature learning is an attractive research topic with great practical success. Canonical correlation analysis (CCA) has become an important technique in multi-view learning, since it can fully utilize the inter-view correlation. In this paper, we mainly study the CCA based multi-view supervised feature learning technique where the labels of training samples are known. Several supervised CCA based multi-view methods have been presented, which focus on investigating the supervised correlation across different views. However, they take no account of the intra-view correlation between samples. Researchers have also introduced the discriminant analysis technique into multi-view feature learning, such as multi-view discriminant analysis (MvDA). But they ignore the canonical correlation within each view and between all views. In this paper, we propose a novel multi-view feature learning approach based on intra-view and inter-view supervised correlation analysis (I2SCA), which can explore the useful correlation information of samples within each view and between all views. The objective function of I2SCA is designed to simultaneously extract the discriminatingly correlated features from both inter-view and intra-view. It can obtain an analytical solution without iterative calculation. And we provide a kernelized extension of I2SCA to tackle the linearly inseparable problem in the original feature space. Three widely-used datasets are employed as test data. Experimental results demonstrate that our proposed approaches outperform several representative multi-view supervised feature learning methods.","Intra-view and Inter-view Supervised Correlation Analysis for Multi-view Feature Learning The object always can be observed at multiple views, and multi-view feature learning is an attractive research topic with great practical success. Canonical correlation analysis (CCA) has become an important technique in multi-view learning, since it can fully utilize the inter-view correlation. In this paper, we mainly study the CCA based multi-view supervised feature learning technique where the labels of training samples are known. Several supervised CCA based multi-view methods have been presented, which focus on investigating the supervised correlation across different views. However, they take no account of the intra-view correlation between samples. Researchers have also introduced the discriminant analysis technique into multi-view feature learning, such as multi-view discriminant analysis (MvDA). But they ignore the canonical correlation within each view and between all views. In this paper, we propose a novel multi-view feature learning approach based on intra-view and inter-view supervised correlation analysis (I2SCA), which can explore the useful correlation information of samples within each view and between all views. The objective function of I2SCA is designed to simultaneously extract the discriminatingly correlated features from both inter-view and intra-view. It can obtain an analytical solution without iterative calculation. And we provide a kernelized extension of I2SCA to tackle the linearly inseparable problem in the original feature space. Three widely-used datasets are employed as test data. Experimental results demonstrate that our proposed approaches outperform several representative multi-view supervised feature learning methods. Canonical correlation analysis (CCA)
Multi-view supervised feature learning
Inter-view and intra-view supervised correlation analysis (I2SCA)
Analytical solution
Kernelized extension",intraview interview supervis correl analysi multiview featur learn object alway observ multipl view multiview featur learn attract research topic great practic success canon correl analysi cca becom import techniqu multiview learn sinc fulli util interview correl paper main studi cca base multiview supervis featur learn techniqu label train sampl known sever supervis cca base multiview method present focus investig supervis correl across differ view howev take account intraview correl sampl research also introduc discrimin analysi techniqu multiview featur learn multiview discrimin analysi mvda ignor canon correl within view view paper propos novel multiview featur learn approach base intraview interview supervis correl analysi i2sca explor use correl inform sampl within view view object function i2sca design simultan extract discrimin correl featur interview intraview obtain analyt solut without iter calcul provid kernel extens i2sca tackl linear insepar problem origin featur space three widelyus dataset employ test data experiment result demonstr propos approach outperform sever repres multiview supervis featur learn method canon correl analysi cca multiview supervis featur learn interview intraview supervis correl analysi i2sca analyt solut kernel extens,7,-2.8538067,-15.718017
Finding Median Point-Set Using Earth Mover's Distance,Hu Ding and Jinhui Xu,"Heuristic Search and Optimization (HSO)
Machine Learning Applications (MLA)","Prototype learning
Earth mover's distance
Optimization","HSO: Heuristic Search
HSO: Optimization
HSO: Evaluation and Analysis (Search and Optimization)
MLA: Applications of Unsupervised Learning
MLA: Machine Learning Applications (General/other)
NMLA: Evaluation and Analysis (Machine Learning)
NMLA: Unsupervised Learning (Other)
NMLA: Machine Learning (General/other)","In this paper, we study a prototype learning problem, called {\em Median Point-Set}, whose objective is to construct a prototype for a set of given point-sets so as to minimize the total earth mover's distances (EMD) between the prototype and the point-sets, where EMD between two point-sets is measured under affine transformation. For this problem, we present the first purely geometric approach. Comparing to existing graph-based approaches ({\em e.g.,} median graph, shock graph), our approach has several unique advantages: (1) No encoding and decoding procedures are needed to map between objects and graphs, and therefore avoid errors caused by information losing during the mappings; (2) Staying only in the geometric domain makes our approach computationally more efficient and robust to noise. As a key ingredient of our approach, we present  the first quality guaranteed algorithm for minimizing EMD between two point-sets under affine transformation. We evaluate the performance of our technique for prototype reconstruction on a random dataset and a benchmark dataset, handwriting Chinese characters.  Experiments suggest  that our technique considerably outperforms  the existing graph-based methods.","Finding Median Point-Set Using Earth Mover's Distance In this paper, we study a prototype learning problem, called {\em Median Point-Set}, whose objective is to construct a prototype for a set of given point-sets so as to minimize the total earth mover's distances (EMD) between the prototype and the point-sets, where EMD between two point-sets is measured under affine transformation. For this problem, we present the first purely geometric approach. Comparing to existing graph-based approaches ({\em e.g.,} median graph, shock graph), our approach has several unique advantages: (1) No encoding and decoding procedures are needed to map between objects and graphs, and therefore avoid errors caused by information losing during the mappings; (2) Staying only in the geometric domain makes our approach computationally more efficient and robust to noise. As a key ingredient of our approach, we present  the first quality guaranteed algorithm for minimizing EMD between two point-sets under affine transformation. We evaluate the performance of our technique for prototype reconstruction on a random dataset and a benchmark dataset, handwriting Chinese characters.  Experiments suggest  that our technique considerably outperforms  the existing graph-based methods. Prototype learning
Earth mover's distance
Optimization",find median pointset use earth mover distanc paper studi prototyp learn problem call em median pointset whose object construct prototyp set given pointset minim total earth mover distanc emd prototyp pointset emd two pointset measur affin transform problem present first pure geometr approach compar exist graphbas approach em eg median graph shock graph approach sever uniqu advantag 1 encod decod procedur need map object graph therefor avoid error caus inform lose map 2 stay geometr domain make approach comput effici robust nois key ingredi approach present first qualiti guarante algorithm minim emd two pointset affin transform evalu perform techniqu prototyp reconstruct random dataset benchmark dataset handwrit chines charact experi suggest techniqu consider outperform exist graphbas method prototyp learn earth mover distanc optim,8,-0.14319937,-1.0004473
Preprocessing for Propositional Model Counting,Jean Marie Lagniez and Pierre Marquis,Search and Constraint Satisfaction (SCS),"model counting
preprocessing
propositional
compilation
direct model counting","SCS: SAT and CSP: Solvers and Tools
SCS: Constraint Satisfaction (General/other)","This paper is concerned with preprocessing techniques for propositional model counting. We have implemented a preprocessor which includes many elementary preprocessing techniques, including occurrence reduction, vivification, backbone identification, as well as equivalence, AND and XOR gate identification and replacement. We performed intensive experiments, using a huge number of benchmarks coming from a large number of families. Two approaches to model counting have been considered downstream: ”direct” model counting using Cachet and compilation-based model counting, based on the C2D compiler. The experimental results we have obtained show that our preprocessor is both efficient and robust.","Preprocessing for Propositional Model Counting This paper is concerned with preprocessing techniques for propositional model counting. We have implemented a preprocessor which includes many elementary preprocessing techniques, including occurrence reduction, vivification, backbone identification, as well as equivalence, AND and XOR gate identification and replacement. We performed intensive experiments, using a huge number of benchmarks coming from a large number of families. Two approaches to model counting have been considered downstream: ”direct” model counting using Cachet and compilation-based model counting, based on the C2D compiler. The experimental results we have obtained show that our preprocessor is both efficient and robust. model counting
preprocessing
propositional
compilation
direct model counting",preprocess proposit model count paper concern preprocess techniqu proposit model count implement preprocessor includ mani elementari preprocess techniqu includ occurr reduct vivif backbon identif well equival xor gate identif replac perform intens experi use huge number benchmark come larg number famili two approach model count consid downstream ”direct” model count use cachet compilationbas model count base c2d compil experiment result obtain show preprocessor effici robust model count preprocess proposit compil direct model count,1,10.898351,-13.977343
Propagating Regular Counting Constraints,"Nicolas Beldiceanu, Pierre Flener, Justin Pearson and Pascal Van Hentenryck",Search and Constraint Satisfaction (SCS),"constraints over a sequence
finite automata
counting
propagators
domain consistency","SCS: Constraint Satisfaction
SCS: Global Constraints","Constraints over finite sequences of variables are ubiquitous in sequencing and timetabling. This led to general modeling techniques and generic propagators, often based on deterministic finite automata (DFA) and their extensions. We consider counter-DFAs (cDFA), which provide concise models for regular counting constraints, that is constraints over the number of times a regular-language pattern occurs in a sequence. We show how to enforce domain consistency in polynomial time for atmost and atleast regular counting constraints based on the frequent case of a cDFA with only accepting states and a single counter that can be increased by transitions. We also show that the satisfaction of exact regular counting constraints is NP-hard and that an incomplete propagator for exact regular counting constraints is faster and provides more pruning than the existing propagator from (Beldiceanu, Carlsson, and Petit 2004). Finally, by avoiding the unrolling of the cDFA used by COSTREGULAR, the space complexity reduces from O(n|\Sigma||Q|) to O(n(|\Sigma|+|Q|)), where \Sigma is the alphabet and Q the state set of the cDFA.","Propagating Regular Counting Constraints Constraints over finite sequences of variables are ubiquitous in sequencing and timetabling. This led to general modeling techniques and generic propagators, often based on deterministic finite automata (DFA) and their extensions. We consider counter-DFAs (cDFA), which provide concise models for regular counting constraints, that is constraints over the number of times a regular-language pattern occurs in a sequence. We show how to enforce domain consistency in polynomial time for atmost and atleast regular counting constraints based on the frequent case of a cDFA with only accepting states and a single counter that can be increased by transitions. We also show that the satisfaction of exact regular counting constraints is NP-hard and that an incomplete propagator for exact regular counting constraints is faster and provides more pruning than the existing propagator from (Beldiceanu, Carlsson, and Petit 2004). Finally, by avoiding the unrolling of the cDFA used by COSTREGULAR, the space complexity reduces from O(n|\Sigma||Q|) to O(n(|\Sigma|+|Q|)), where \Sigma is the alphabet and Q the state set of the cDFA. constraints over a sequence
finite automata
counting
propagators
domain consistency",propag regular count constraint constraint finit sequenc variabl ubiquit sequenc timet led general model techniqu generic propag often base determinist finit automata dfa extens consid counterdfa cdfa provid concis model regular count constraint constraint number time regularlanguag pattern occur sequenc show enforc domain consist polynomi time atmost atleast regular count constraint base frequent case cdfa accept state singl counter increas transit also show satisfact exact regular count constraint nphard incomplet propag exact regular count constraint faster provid prune exist propag beldiceanu carlsson petit 2004 final avoid unrol cdfa use costregular space complex reduc onsigmaq onsigmaq sigma alphabet q state set cdfa constraint sequenc finit automata count propag domain consist,3,-2.849155,5.7845397
SOML: Sparse Online Metric Learning with Application to Image Retrieval,"Xingyu Gao, Steven C.H. Hoi, Yongdong Zhang, Ji Wan and Jintao Li",Machine Learning Applications (MLA),"Sparse Online Learning
Distance Metric Learning
Image Retrieval",MLA: Applications of Supervised Learning,"Image similarity search plays a key role in many multimedia
applications, where multimedia data (such as images and videos) are
usually represented in high-dimensional feature space. In this
paper, we propose a novel Sparse Online Metric Learning (SOML)
scheme for learning sparse distance functions from large-scale
high-dimensional data and explore its application to image
retrieval. In contrast to many existing distance metric learning
algorithms that are often designed for low-dimensional data, the
proposed algorithms are able to learn sparse distance metrics from
high-dimensional data in an efficient and scalable manner. Our
experimental results show that the proposed method achieves better
or at least comparable accuracy performance than the
state-of-the-art non-sparse distance metric learning approaches, but
enjoys a significant advantage in computational efficiency and
sparsity, making it more practical for real-world applications.","SOML: Sparse Online Metric Learning with Application to Image Retrieval Image similarity search plays a key role in many multimedia
applications, where multimedia data (such as images and videos) are
usually represented in high-dimensional feature space. In this
paper, we propose a novel Sparse Online Metric Learning (SOML)
scheme for learning sparse distance functions from large-scale
high-dimensional data and explore its application to image
retrieval. In contrast to many existing distance metric learning
algorithms that are often designed for low-dimensional data, the
proposed algorithms are able to learn sparse distance metrics from
high-dimensional data in an efficient and scalable manner. Our
experimental results show that the proposed method achieves better
or at least comparable accuracy performance than the
state-of-the-art non-sparse distance metric learning approaches, but
enjoys a significant advantage in computational efficiency and
sparsity, making it more practical for real-world applications. Sparse Online Learning
Distance Metric Learning
Image Retrieval",soml spars onlin metric learn applic imag retriev imag similar search play key role mani multimedia applic multimedia data imag video usual repres highdimension featur space paper propos novel spars onlin metric learn soml scheme learn spars distanc function largescal highdimension data explor applic imag retriev contrast mani exist distanc metric learn algorithm often design lowdimension data propos algorithm abl learn spars distanc metric highdimension data effici scalabl manner experiment result show propos method achiev better least compar accuraci perform stateoftheart nonspars distanc metric learn approach enjoy signific advantag comput effici sparsiti make practic realworld applic spars onlin learn distanc metric learn imag retriev,4,1.4313319,-15.77501
Strategyproof exchange with multiple private endowments,"Taiki Todo, Haixin Sun and Makoto Yokoo",Game Theory and Economic Paradigms (GTEP),"Mechanism design
Exchange
Manipulation
Strategyproofness","GTEP: Auctions and Market-Based Systems
GTEP: Game Theory
MAS: E-Commerce
MAS: Mechanism Design","We study a mechanism design problem for exchange economies where each agent is initially endowed with a set of indivisible goods and side payments are not allowed. We assume each agent can withhold some endowments, as well as misreport her preference. Under this assumption, strategyproofness requires that for each agent, reporting her true preference with revealing all her endowments is a dominant strategy, and thus implies individual rationality. 
Our objective in this paper is to analyze the effect of such private ownership in exchange economies with multiple endowments.  As fundamental results, we first show that the revelation principle holds under a natural assumption and that strategyproofness and Pareto efficiency are incompatible even under the lexicographic preference domain. We then propose a class of exchange rules, each of which has a corresponding directed graph to prescribe possible trades, and provide a necessary and sufficient condition on the graph structure so that the rule satisfies strategy-proofness.","Strategyproof exchange with multiple private endowments We study a mechanism design problem for exchange economies where each agent is initially endowed with a set of indivisible goods and side payments are not allowed. We assume each agent can withhold some endowments, as well as misreport her preference. Under this assumption, strategyproofness requires that for each agent, reporting her true preference with revealing all her endowments is a dominant strategy, and thus implies individual rationality. 
Our objective in this paper is to analyze the effect of such private ownership in exchange economies with multiple endowments.  As fundamental results, we first show that the revelation principle holds under a natural assumption and that strategyproofness and Pareto efficiency are incompatible even under the lexicographic preference domain. We then propose a class of exchange rules, each of which has a corresponding directed graph to prescribe possible trades, and provide a necessary and sufficient condition on the graph structure so that the rule satisfies strategy-proofness. Mechanism design
Exchange
Manipulation
Strategyproofness",strategyproof exchang multipl privat endow studi mechan design problem exchang economi agent initi endow set indivis good side payment allow assum agent withhold endow well misreport prefer assumpt strategyproof requir agent report true prefer reveal endow domin strategi thus impli individu ration object paper analyz effect privat ownership exchang economi multipl endow fundament result first show revel principl hold natur assumpt strategyproof pareto effici incompat even lexicograph prefer domain propos class exchang rule correspond direct graph prescrib possibl trade provid necessari suffici condit graph structur rule satisfi strategyproof mechan design exchang manipul strategyproof,9,15.937313,16.103432
Mechanism design for mobile geo-location advertising,"Nicola Gatti, Marco Rocco, Sofia Ceppi and Enrico H. Gerding","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Mechanism Design
Auctions
Computational Advertising
Mobile Advertising
Game Theory (cooperative and non–cooperative)","GTEP: Auctions and Market-Based Systems
GTEP: Game Theory
MAS: Mechanism Design","Mobile geo-location advertising, where mobile ads are targeted based on a user's location, has been identified as a key growth factor for the mobile market. As with online advertising, a crucial ingredient for their success is the development of effective economic mechanisms. An important difference is that mobile ads are shown sequentially over time and information about the user can be learned based on their movements. Furthermore, ads need to be shown selectively to prevent ad fatigue. To this end, we introduce, for the first time, a user model and suitable economic mechanisms which take these factors into account. Specifically, we design two truthful mechanisms which produce an advertisement plan based on the user's movements. One mechanism is allocatively efficient, but requires exponential compute time in the worst case. The other requires polynomial time, but  is not allocatively efficient. Finally, we experimentally evaluate the trade-off between compute time and efficiency of our mechanisms.","Mechanism design for mobile geo-location advertising Mobile geo-location advertising, where mobile ads are targeted based on a user's location, has been identified as a key growth factor for the mobile market. As with online advertising, a crucial ingredient for their success is the development of effective economic mechanisms. An important difference is that mobile ads are shown sequentially over time and information about the user can be learned based on their movements. Furthermore, ads need to be shown selectively to prevent ad fatigue. To this end, we introduce, for the first time, a user model and suitable economic mechanisms which take these factors into account. Specifically, we design two truthful mechanisms which produce an advertisement plan based on the user's movements. One mechanism is allocatively efficient, but requires exponential compute time in the worst case. The other requires polynomial time, but  is not allocatively efficient. Finally, we experimentally evaluate the trade-off between compute time and efficiency of our mechanisms. Mechanism Design
Auctions
Computational Advertising
Mobile Advertising
Game Theory (cooperative and non–cooperative)",mechan design mobil geoloc advertis mobil geoloc advertis mobil ad target base user locat identifi key growth factor mobil market onlin advertis crucial ingredi success develop effect econom mechan import differ mobil ad shown sequenti time inform user learn base movement furthermor ad need shown select prevent ad fatigu end introduc first time user model suitabl econom mechan take factor account specif design two truth mechan produc advertis plan base user movement one mechan alloc effici requir exponenti comput time worst case requir polynomi time alloc effici final experiment evalu tradeoff comput time effici mechan mechan design auction comput advertis mobil advertis game theori cooper non–coop,9,12.194897,11.696052
A Propagator Design Framework for Constraints over Sequences,"Jean-Noël Monette, Pierre Flener and Justin Pearson",Search and Constraint Satisfaction (SCS),"Constraint Programming
Global Constraints
Stepwise Refinement
Tuple Variables","SCS: Constraint Satisfaction
SCS: Global Constraints",Constraints over variable sequences are ubiquitous and many of their propagators have been inspired by dynamic programming (DP).  We propose a conceptual framework for designing such propagators: pruning rules are refined upon the application of transformation operators to a DP-style formulation of a constraint; a representation of the variable domains is picked; and a coordination of the pruning rules is picked.,"A Propagator Design Framework for Constraints over Sequences Constraints over variable sequences are ubiquitous and many of their propagators have been inspired by dynamic programming (DP).  We propose a conceptual framework for designing such propagators: pruning rules are refined upon the application of transformation operators to a DP-style formulation of a constraint; a representation of the variable domains is picked; and a coordination of the pruning rules is picked. Constraint Programming
Global Constraints
Stepwise Refinement
Tuple Variables",propag design framework constraint sequenc constraint variabl sequenc ubiquit mani propag inspir dynam program dp propos conceptu framework design propag prune rule refin upon applic transform oper dpstyle formul constraint represent variabl domain pick coordin prune rule pick constraint program global constraint stepwis refin tupl variabl,3,-3.4715173,5.8578215
Regression Model and Privacy Preserved Learning by Matrix Completion,"Jinfeng Yi, Jun Wang and Rong Jin","Applications (APP)
Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","Regression analysis
Privacy
Matrix completion","APP: Biomedical / Bioinformatics
APP: Computational Social Science
APP: Security and Privacy
MLA: Bio/Medicine
MLA: Applications of Supervised Learning
MLA: Machine Learning Applications (General/other)
MAS: Mechanism Design
NMLA: Data Mining and Knowledge Discovery
NMLA: Feature Construction/Reformulation
NMLA: Machine Learning (General/other)
RU: Uncertainty in AI (General/Other)","Sensitive data such as medical records and business reports usually contains valuable information that can be used to build prediction models. However, designing learning models by directly using sensitive data might result in severe privacy and copyright issues. In this paper, we propose a novel matrix completion based framework that is able to handle two challenging issues simultaneously: i) recovering missing and noisy sensitive data, and ii) preserving the privacy of the sensitive data during the learning process. In particular, the proposed framework is able to mask the sensitive data while ensuring that the transformed data are still usable for training regression models. We show that two key properties, namely \emph{model preserving} and \emph{privacy preserving}, are satisfied by the transformed data obtained from the proposed framework. In \emph{model preserving}, we guarantee that the linear regression model built from the masked data approximates the regression model learned from the original data in a perfect way. In \emph{privacy preserving}, we ensure that the original sensitive data cannot be recovered since the transformation procedure is irreversible. Given these two characteristics, the transformed data can be safely released to any learners for designing prediction models without revealing any private content. Our empirical studies with a synthesized dataset and multiple sensitive benchmark datasets verify our theoretical claim as well as the effectiveness of the proposed framework.","Regression Model and Privacy Preserved Learning by Matrix Completion Sensitive data such as medical records and business reports usually contains valuable information that can be used to build prediction models. However, designing learning models by directly using sensitive data might result in severe privacy and copyright issues. In this paper, we propose a novel matrix completion based framework that is able to handle two challenging issues simultaneously: i) recovering missing and noisy sensitive data, and ii) preserving the privacy of the sensitive data during the learning process. In particular, the proposed framework is able to mask the sensitive data while ensuring that the transformed data are still usable for training regression models. We show that two key properties, namely \emph{model preserving} and \emph{privacy preserving}, are satisfied by the transformed data obtained from the proposed framework. In \emph{model preserving}, we guarantee that the linear regression model built from the masked data approximates the regression model learned from the original data in a perfect way. In \emph{privacy preserving}, we ensure that the original sensitive data cannot be recovered since the transformation procedure is irreversible. Given these two characteristics, the transformed data can be safely released to any learners for designing prediction models without revealing any private content. Our empirical studies with a synthesized dataset and multiple sensitive benchmark datasets verify our theoretical claim as well as the effectiveness of the proposed framework. Regression analysis
Privacy
Matrix completion",regress model privaci preserv learn matrix complet sensit data medic record busi report usual contain valuabl inform use build predict model howev design learn model direct use sensit data might result sever privaci copyright issu paper propos novel matrix complet base framework abl handl two challeng issu simultan recov miss noisi sensit data ii preserv privaci sensit data learn process particular propos framework abl mask sensit data ensur transform data still usabl train regress model show two key properti name emphmodel preserv emphprivaci preserv satisfi transform data obtain propos framework emphmodel preserv guarante linear regress model built mask data approxim regress model learn origin data perfect way emphprivaci preserv ensur origin sensit data cannot recov sinc transform procedur irrevers given two characterist transform data safe releas learner design predict model without reveal privat content empir studi synthes dataset multipl sensit benchmark dataset verifi theoret claim well effect propos framework regress analysi privaci matrix complet,6,-11.201022,-5.068153
A Constructive Argumentation Framework,Souhila Kaci and Yakoub Salhi,,"Logic-based Argumentation
Constructive Arguments
Intuitionistic Logic",KRR: Argumentation,"Dung’s argumentation framework is an abstract framework based on a set of arguments and a binary attack relation defined over the set. One instantiation, among many others, of Dung’s framework consists in constructing the arguments from a set of propositional logic formulas. Thus an argument is seen as a reason for or against the truth of a particular statement. Despite its advantages, the argumentation approach for inconsistency handling also has important shortcomings. More precisely, in some applications what one is interested in are not so much only the conclusions supported by the arguments but also to the precise explications of such conclusions. We show that argumentation framework applied to classical logic formulas is not suitable to deal with this problem. On the other hand, intuitionistic logic appears to be a natural alternative candidate logic (instead of classical logic) to instantiate Dung’s framework. We develop constructive argumentation framework. We show that intuitionistic logic offers nice and desirable properties of the arguments. We also provide a characterization of the arguments in this setting in terms of minimal inconsistent subsets when intuitionistic logic is embedded in the modal logic S4.","A Constructive Argumentation Framework Dung’s argumentation framework is an abstract framework based on a set of arguments and a binary attack relation defined over the set. One instantiation, among many others, of Dung’s framework consists in constructing the arguments from a set of propositional logic formulas. Thus an argument is seen as a reason for or against the truth of a particular statement. Despite its advantages, the argumentation approach for inconsistency handling also has important shortcomings. More precisely, in some applications what one is interested in are not so much only the conclusions supported by the arguments but also to the precise explications of such conclusions. We show that argumentation framework applied to classical logic formulas is not suitable to deal with this problem. On the other hand, intuitionistic logic appears to be a natural alternative candidate logic (instead of classical logic) to instantiate Dung’s framework. We develop constructive argumentation framework. We show that intuitionistic logic offers nice and desirable properties of the arguments. We also provide a characterization of the arguments in this setting in terms of minimal inconsistent subsets when intuitionistic logic is embedded in the modal logic S4. Logic-based Argumentation
Constructive Arguments
Intuitionistic Logic",construct argument framework dung argument framework abstract framework base set argument binari attack relat defin set one instanti among mani other dung framework consist construct argument set proposit logic formula thus argument seen reason truth particular statement despit advantag argument approach inconsist handl also import shortcom precis applic one interest much conclus support argument also precis explic conclus show argument framework appli classic logic formula suitabl deal problem hand intuitionist logic appear natur altern candid logic instead classic logic instanti dung framework develop construct argument framework show intuitionist logic offer nice desir properti argument also provid character argument set term minim inconsist subset intuitionist logic embed modal logic s4 logicbas argument construct argument intuitionist logic,6,0.25549033,1.2424253
Point-based POMDP solving with factored value function approximation,"Tiago Veiga, Matthijs Spaan and Pedro Lima","Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","POMDP
Value function approximation
Point-based methods","PS: Probabilistic Planning
PS: Planning (General/Other)
RU: Sequential Decision Making
RU: Uncertainty in AI (General/Other)","Partially observable Markov decision processes (POMDPs) provide a principled mathematical framework for modeling autonomous decision-making problems. POMDP solutions are often represented by a value function comprised of a set of vectors. In the case of factored models, the size of these vectors grows exponentially with the number of state factors, leading to scalability issues. We consider an approximate value function representation based on a linear combination of basis functions. In particular, we present a backup operator that can be used in any point-based POMDP solvers. Furthermore, we show how independence between observation factors can be exploited for large computational gains. We experimentally verify our contributions and show that they can improve point-based methods in policy quality and solution size.","Point-based POMDP solving with factored value function approximation Partially observable Markov decision processes (POMDPs) provide a principled mathematical framework for modeling autonomous decision-making problems. POMDP solutions are often represented by a value function comprised of a set of vectors. In the case of factored models, the size of these vectors grows exponentially with the number of state factors, leading to scalability issues. We consider an approximate value function representation based on a linear combination of basis functions. In particular, we present a backup operator that can be used in any point-based POMDP solvers. Furthermore, we show how independence between observation factors can be exploited for large computational gains. We experimentally verify our contributions and show that they can improve point-based methods in policy quality and solution size. POMDP
Value function approximation
Point-based methods",pointbas pomdp solv factor valu function approxim partial observ markov decis process pomdp provid principl mathemat framework model autonom decisionmak problem pomdp solut often repres valu function compris set vector case factor model size vector grow exponenti number state factor lead scalabl issu consid approxim valu function represent base linear combin basi function particular present backup oper use pointbas pomdp solver furthermor show independ observ factor exploit larg comput gain experiment verifi contribut show improv pointbas method polici qualiti solut size pomdp valu function approxim pointbas method,5,-0.68423873,-6.4319572
A Multiarmed Bandit Incentive Mechanism for Crowdsourcing Demand Response in Smart Grids,"Shweta Jain, Balakrishnan Narayanaswamy and Yadati Narahari","Computational Sustainability and AI (CSAI)
Game Theory and Economic Paradigms (GTEP)
Human-Computation and Crowd Sourcing (HCC)","CSAI: Modeling the interactions of agents with different and often conflicting interests
GTEP: Auctions and Market-Based Systems
HCC: Game-theoretic mechanism design of incentives for motivation and honest reporting","CSAI: Modeling the interactions of agents with different and often conflicting interests
GTEP: Auctions and Market-Based Systems
HCC: Game-theoretic mechanism design of incentives for motivation and honest reporting","Demand response is a critical part of renewable integration and energy cost reduction goals across the world. Motivated by the need to reduce costs arising from electricity shortage and renewable energy fluctuations, we propose a novel  multiarmed bandit mechanism for demand response (MAB-MDR) which makes monetary offers  to strategic consumers who have unknown response characteristics. Our work is inspired by  connection to and intuition from crowdsourcing mechanisms.  The proposed mechanism incorporates realistic features such as a time varying and quadratic cost function. The mechanism marries auctions, that allow users to report their preferences, with online algorithms, that allow distribution companies to learn user-specific parameters. We show that MAB-MDR is dominant strategy incentive compatible, individually rational, and achieves sublinear regret. 
Such mechanisms can be effectively deployed in smart grids using new information and control architecture innovations 
and lead to welcome savings in energy costs.","A Multiarmed Bandit Incentive Mechanism for Crowdsourcing Demand Response in Smart Grids Demand response is a critical part of renewable integration and energy cost reduction goals across the world. Motivated by the need to reduce costs arising from electricity shortage and renewable energy fluctuations, we propose a novel  multiarmed bandit mechanism for demand response (MAB-MDR) which makes monetary offers  to strategic consumers who have unknown response characteristics. Our work is inspired by  connection to and intuition from crowdsourcing mechanisms.  The proposed mechanism incorporates realistic features such as a time varying and quadratic cost function. The mechanism marries auctions, that allow users to report their preferences, with online algorithms, that allow distribution companies to learn user-specific parameters. We show that MAB-MDR is dominant strategy incentive compatible, individually rational, and achieves sublinear regret. 
Such mechanisms can be effectively deployed in smart grids using new information and control architecture innovations 
and lead to welcome savings in energy costs. CSAI: Modeling the interactions of agents with different and often conflicting interests
GTEP: Auctions and Market-Based Systems
HCC: Game-theoretic mechanism design of incentives for motivation and honest reporting",multiarm bandit incent mechan crowdsourc demand respons smart grid demand respons critic part renew integr energi cost reduct goal across world motiv need reduc cost aris electr shortag renew energi fluctuat propos novel multiarm bandit mechan demand respons mabmdr make monetari offer strateg consum unknown respons characterist work inspir connect intuit crowdsourc mechan propos mechan incorpor realist featur time vari quadrat cost function mechan marri auction allow user report prefer onlin algorithm allow distribut compani learn userspecif paramet show mabmdr domin strategi incent compat individu ration achiev sublinear regret mechan effect deploy smart grid use new inform control architectur innov lead welcom save energi cost csai model interact agent differ often conflict interest gtep auction marketbas system hcc gametheoret mechan design incent motiv honest report,9,12.831386,14.4399605
Binary Aggregation by Selection of the Most Representative Voter,Ulle Endriss and Umberto Grandi,"Game Theory and Economic Paradigms (GTEP)
Knowledge Representation and Reasoning (KRR)
Multiagent Systems (MAS)","Computational Social Choice
Approximation
Judgment Aggregation","GTEP: Social Choice / Voting
KRR: Preferences
MAS: Multiagent Systems (General/other)","In binary aggregation, a group of voters each express yes/no choices
regarding a number of possibly correlated issues and we are asked to
decide on a collective choice that accurately reflects the views of this
group. A good collective choice will minimise the distance to each of
the individual choices, but using such a distance-based aggregation rule
is computationally intractable. Instead, we explore a class of
low-complexity aggregation rules that select the most representative
voter in any given situation and return that voter's choice as the
collective outcome.","Binary Aggregation by Selection of the Most Representative Voter In binary aggregation, a group of voters each express yes/no choices
regarding a number of possibly correlated issues and we are asked to
decide on a collective choice that accurately reflects the views of this
group. A good collective choice will minimise the distance to each of
the individual choices, but using such a distance-based aggregation rule
is computationally intractable. Instead, we explore a class of
low-complexity aggregation rules that select the most representative
voter in any given situation and return that voter's choice as the
collective outcome. Computational Social Choice
Approximation
Judgment Aggregation",binari aggreg select repres voter binari aggreg group voter express yesno choic regard number possibl correl issu ask decid collect choic accur reflect view group good collect choic minimis distanc individu choic use distancebas aggreg rule comput intract instead explor class lowcomplex aggreg rule select repres voter given situat return voter choic collect outcom comput social choic approxim judgment aggreg,9,16.709486,-1.1500615
Pairwise-Covariance Linear Discriminant Analysis,"Deguang Kong, Chris Ding and Qihe Pan","Applications (APP)
Machine Learning Applications (MLA)","linear discriminant analysis
gradient
pairwise
covariance",,"In machine learning, linear discriminant analysis (LDA) is a
popular dimension reduction method. In this paper, we first pro-
vide a new perspective of LDA from information theory per-
spective. From this new perspective, we propose a new formula-
tion of LDA, which uses the pairwise averaged class covariance
instead of the globally averaged class covariance used in stan-
dard LDA. This pairwise (averaged) covariance describes data
distribution more accurately. The new perspective also provides
a natural way to properly weight different pairwise distances,
which emphasizes the pairs of class with small distances, and
this leads to the proposed pairwise covariance properly weight-
ed LDA (pcLDA). The kernel version of pcLDA is presented to
handle nonlinear projections. Efficient algorithms are presented
to efficiently compute the proposed models.","Pairwise-Covariance Linear Discriminant Analysis In machine learning, linear discriminant analysis (LDA) is a
popular dimension reduction method. In this paper, we first pro-
vide a new perspective of LDA from information theory per-
spective. From this new perspective, we propose a new formula-
tion of LDA, which uses the pairwise averaged class covariance
instead of the globally averaged class covariance used in stan-
dard LDA. This pairwise (averaged) covariance describes data
distribution more accurately. The new perspective also provides
a natural way to properly weight different pairwise distances,
which emphasizes the pairs of class with small distances, and
this leads to the proposed pairwise covariance properly weight-
ed LDA (pcLDA). The kernel version of pcLDA is presented to
handle nonlinear projections. Efficient algorithms are presented
to efficiently compute the proposed models. linear discriminant analysis
gradient
pairwise
covariance",pairwisecovari linear discrimin analysi machin learn linear discrimin analysi lda popular dimens reduct method paper first pro vide new perspect lda inform theori per spectiv new perspect propos new formula tion lda use pairwis averag class covari instead global averag class covari use stan dard lda pairwis averag covari describ data distribut accur new perspect also provid natur way proper weight differ pairwis distanc emphas pair class small distanc lead propos pairwis covari proper weight ed lda pclda kernel version pclda present handl nonlinear project effici algorithm present effici comput propos model linear discrimin analysi gradient pairwis covari,1,-1.3541852,-4.0571637
State Aggregation in Monte Carlo Tree Search,"Jesse Hostetler, Alan Fern and Tom Dietterich","Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","markov decision process
monte carlo tree search
state abstraction","PS: Probabilistic Planning
RU: Sequential Decision Making","Monte Carlo tree search (MCTS) is a popular class of algorithms for online decision making in large Markov decision processes (MDPs). The effectiveness of these algorithms, however, often deteriorates for MDPs with high stochastic branching factors. In this paper, we study state aggregation as a way of reducing stochastic branching in tree search. Prior work has studied formal properties of MDP state aggregation in the context of dynamic programming and reinforcement learning, but little attention has been paid to state aggregation in MCTS. Our main contribution is to establish basic results about the optimality-preserving properties of state aggregation for search trees. We then apply these results to show that popular MCTS algorithms such as UCT and sparse sampling can employ fairly coarse state aggregation schemes while retaining their theoretical properties. As a proof of concept, we experimentally confirm that state aggregation in MCTS improves finite-sample performance.","State Aggregation in Monte Carlo Tree Search Monte Carlo tree search (MCTS) is a popular class of algorithms for online decision making in large Markov decision processes (MDPs). The effectiveness of these algorithms, however, often deteriorates for MDPs with high stochastic branching factors. In this paper, we study state aggregation as a way of reducing stochastic branching in tree search. Prior work has studied formal properties of MDP state aggregation in the context of dynamic programming and reinforcement learning, but little attention has been paid to state aggregation in MCTS. Our main contribution is to establish basic results about the optimality-preserving properties of state aggregation for search trees. We then apply these results to show that popular MCTS algorithms such as UCT and sparse sampling can employ fairly coarse state aggregation schemes while retaining their theoretical properties. As a proof of concept, we experimentally confirm that state aggregation in MCTS improves finite-sample performance. markov decision process
monte carlo tree search
state abstraction",state aggreg mont carlo tree search mont carlo tree search mcts popular class algorithm onlin decis make larg markov decis process mdps effect algorithm howev often deterior mdps high stochast branch factor paper studi state aggreg way reduc stochast branch tree search prior work studi formal properti mdp state aggreg context dynam program reinforc learn littl attent paid state aggreg mcts main contribut establish basic result optimalitypreserv properti state aggreg search tree appli result show popular mcts algorithm uct spars sampl employ fair coars state aggreg scheme retain theoret properti proof concept experiment confirm state aggreg mcts improv finitesampl perform markov decis process mont carlo tree search state abstract,4,-9.645199,0.6856961
Saturated Path-Constrained MDP: Planning under Uncertainty and Deterministic Model-Checking Constraints,"Jonathan Sprauel, Andrey Kolobov and Florent Teichteil-Königsbuch","Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","Safe and Optimal Controller Synthesis
Uncertainty and Stochasticity
Planning under Uncertainty
Model-Checking PCTL Constraints
Path-Constrained Markov Decision Processes","PS: Markov Models of Environments
PS: Probabilistic Planning
PS: Planning (General/Other)
RU: Sequential Decision Making
SCS: Constraint Satisfaction (General/other)","In many probabilistic planning scenarios, a system's behavior needs to not only maximize the expected utility but also obey certain restrictions. This paper presents Saturated Path-Constrained Markov Decision Processes (SPC MDPs), a new MDP type for planning under uncertainty with deterministic model-checking constraints, e.g., ""state s must be visited before s'"", ""the system must end up in s"", or ""the system must never enter s"". We present a mathematical analysis of SPC MDPs, showing that although SPC MDPs generally have no optimal policies, every instance of this class has an epsilon-optimal randomized policy for any epsilon > 0. We propose a dynamic programming-based algorithm for finding such policies, and empirically demonstrate this algorithm to be orders of magnitude faster than its next-best alternative.","Saturated Path-Constrained MDP: Planning under Uncertainty and Deterministic Model-Checking Constraints In many probabilistic planning scenarios, a system's behavior needs to not only maximize the expected utility but also obey certain restrictions. This paper presents Saturated Path-Constrained Markov Decision Processes (SPC MDPs), a new MDP type for planning under uncertainty with deterministic model-checking constraints, e.g., ""state s must be visited before s'"", ""the system must end up in s"", or ""the system must never enter s"". We present a mathematical analysis of SPC MDPs, showing that although SPC MDPs generally have no optimal policies, every instance of this class has an epsilon-optimal randomized policy for any epsilon > 0. We propose a dynamic programming-based algorithm for finding such policies, and empirically demonstrate this algorithm to be orders of magnitude faster than its next-best alternative. Safe and Optimal Controller Synthesis
Uncertainty and Stochasticity
Planning under Uncertainty
Model-Checking PCTL Constraints
Path-Constrained Markov Decision Processes",satur pathconstrain mdp plan uncertainti determinist modelcheck constraint mani probabilist plan scenario system behavior need maxim expect util also obey certain restrict paper present satur pathconstrain markov decis process spc mdps new mdp type plan uncertainti determinist modelcheck constraint eg state must visit system must end system must never enter present mathemat analysi spc mdps show although spc mdps general optim polici everi instanc class epsilonoptim random polici epsilon 0 propos dynam programmingbas algorithm find polici empir demonstr algorithm order magnitud faster nextbest altern safe optim control synthesi uncertainti stochast plan uncertainti modelcheck pctl constraint pathconstrain markov decis process,3,-2.775021,-7.6290526
Bounding the Support Size in Extensive Form Games with Imperfect Information,"Martin Schmid, Matej Moravcik and Milan Hladik",Game Theory and Economic Paradigms (GTEP),"Game theory
Nash equilibrium
Support
Extensive form games
Bayesian extensive games
Poker
Equilibrium preserving transformation","GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information","It's a well known fact that in extensive form games with perfect information, there is a Nash equilibrium with support of size one. This doesn't hold for games with imperfect information, where the size of minimal support can be larger. We present a dependency between the level of uncertainty and the minimum support size. For many games, there is a big disproportion between the game uncertainty and the number of actions available. In Bayesian extensive games with perfect information, the only uncertainty is about the type of players. In card games, the uncertainty comes from dealing the deck. In these games, we can significantly reduce the support size. Our result applies to general-sum extensive form games with any finite number of players.","Bounding the Support Size in Extensive Form Games with Imperfect Information It's a well known fact that in extensive form games with perfect information, there is a Nash equilibrium with support of size one. This doesn't hold for games with imperfect information, where the size of minimal support can be larger. We present a dependency between the level of uncertainty and the minimum support size. For many games, there is a big disproportion between the game uncertainty and the number of actions available. In Bayesian extensive games with perfect information, the only uncertainty is about the type of players. In card games, the uncertainty comes from dealing the deck. In these games, we can significantly reduce the support size. Our result applies to general-sum extensive form games with any finite number of players. Game theory
Nash equilibrium
Support
Extensive form games
Bayesian extensive games
Poker
Equilibrium preserving transformation",bound support size extens form game imperfect inform well known fact extens form game perfect inform nash equilibrium support size one doesnt hold game imperfect inform size minim support larger present depend level uncertainti minimum support size mani game big disproport game uncertainti number action avail bayesian extens game perfect inform uncertainti type player card game uncertainti come deal deck game signific reduc support size result appli generalsum extens form game finit number player game theori nash equilibrium support extens form game bayesian extens game poker equilibrium preserv transform,2,7.7077823,20.26673
Exploiting Support Sets for Answer Set Programs with External Evaluations,"Thomas Eiter, Michael Fink, Christoph Redl and Daria Stepanova",Knowledge Representation and Reasoning (KRR),"Answer Set Programming
External Sources
Description Logic Programs","KRR: Ontologies
KRR: Description Logics
KRR: Knowledge Representation Languages
KRR: Logic Programming
KRR: Nonmonotonic Reasoning
KRR: Knowledge Representation (General/Other)","Answer set programs (ASP) with external evaluations are a declarative means to capture advanced applications. However, their evaluation can be expensive due to external source accesses. In this paper we consider hex-programs that provide external atoms as a bidirectional interface to external sources and present a novel evaluation method based on support sets, which informally are portions of the input to an external atom that will determine its output for any completion of the partial input. Support sets allow one to shortcut the external source access, which can be completely eliminated. This is particularly attractive if a compact representation of suitable support sets is efficiently constructible. We discuss some applications with this property, among them description logic programs over DL-Lite ontologies, and present experimental results showing that support sets can significantly improve efficiency.","Exploiting Support Sets for Answer Set Programs with External Evaluations Answer set programs (ASP) with external evaluations are a declarative means to capture advanced applications. However, their evaluation can be expensive due to external source accesses. In this paper we consider hex-programs that provide external atoms as a bidirectional interface to external sources and present a novel evaluation method based on support sets, which informally are portions of the input to an external atom that will determine its output for any completion of the partial input. Support sets allow one to shortcut the external source access, which can be completely eliminated. This is particularly attractive if a compact representation of suitable support sets is efficiently constructible. We discuss some applications with this property, among them description logic programs over DL-Lite ontologies, and present experimental results showing that support sets can significantly improve efficiency. Answer Set Programming
External Sources
Description Logic Programs",exploit support set answer set program extern evalu answer set program asp extern evalu declar mean captur advanc applic howev evalu expens due extern sourc access paper consid hexprogram provid extern atom bidirect interfac extern sourc present novel evalu method base support set inform portion input extern atom determin output complet partial input support set allow one shortcut extern sourc access complet elimin particular attract compact represent suitabl support set effici construct discuss applic properti among descript logic program dllite ontolog present experiment result show support set signific improv effici answer set program extern sourc descript logic program,8,-13.370514,2.143842
Towards Topological-transformation Robust Shape Comparison:  A Sparse Representation Based Manifold Embedding Approach,Longwen Gao and Shuigeng Zhou,Vision (VIS),"Shape Comparison
Manifold Embedding
Sparse Representation",VIS: Object Recognition,"Non-rigid shape comparison based on manifold embedding using Generalized Multidimensional Scaling (GMDS) has attracted a lot of attention for its high accuracy. However, this method requires that shape surface is not elastic. In other words, it is sensitive to topological transformations such as stretching and compressing. To tackle this problem, we propose a new approach that constructs a high-dimensional space to embed the manifolds of shapes, which could completely withstand rigid transformations and considerably tolerate topological transformations. Experiments on TOSCA shapes validate the proposed approach.","Towards Topological-transformation Robust Shape Comparison:  A Sparse Representation Based Manifold Embedding Approach Non-rigid shape comparison based on manifold embedding using Generalized Multidimensional Scaling (GMDS) has attracted a lot of attention for its high accuracy. However, this method requires that shape surface is not elastic. In other words, it is sensitive to topological transformations such as stretching and compressing. To tackle this problem, we propose a new approach that constructs a high-dimensional space to embed the manifolds of shapes, which could completely withstand rigid transformations and considerably tolerate topological transformations. Experiments on TOSCA shapes validate the proposed approach. Shape Comparison
Manifold Embedding
Sparse Representation",toward topologicaltransform robust shape comparison spars represent base manifold embed approach nonrigid shape comparison base manifold embed use general multidimension scale gmds attract lot attent high accuraci howev method requir shape surfac elast word sensit topolog transform stretch compress tackl problem propos new approach construct highdimension space emb manifold shape could complet withstand rigid transform consider toler topolog transform experi tosca shape valid propos approach shape comparison manifold embed spars represent,4,5.5826297,-11.0792675
Uncorrelated Multi-view Fisher Discrimination Dictionary Learning For Recognition,"Xiao-Yuan Jing, Rui-Min Hu, Fei Wu, Xi-Lin Chen, Qian Liu and Yong-Fang Yao",Vision (VIS),"Fisher discrimination Dictionary learning (FDDL)
Multi-view FDDL (MFDDL)
Uncorrelated MFDDL (UMFDDL)
Uncorrelated constraint","NMLA: Supervised Learning (Other)
VIS: Categorization
VIS: Object Recognition","Dictionary learning (DL) has now become an important feature learning technique that owns state-of-the-art recog-nition performance. Due to sparse characteristic of data in real-world applications, dictionary learning uses a set of learned dictionary bases to represent the linear decomposi-tion of a data point. Fisher discrimination DL (FDDL) is a representative supervised DL method, which constructs a structured dictionary whose atoms correspond to the class labels. Recent years have witnessed a growing interest in multi-view (more than two views) feature learning tech-niques. Although some multi-view (or multi-modal) DL methods have been presented, there still exists much room for improvement. How to enhance the total discriminability of dictionaries and reduce their redundancy is a crucial re-search topic. To boost the performance of multi-view dic-tionary learning technique, we propose an uncorrelated multi-view fisher discrimination DL (UMFDDL) approach for recognition. By making dictionary atoms correspond to the class labels such that the obtained reconstruction error is discriminative, UMFDDL aims to jointly learn multiple dic-tionaries with totally favorable discriminative power. Fur-thermore, we design the uncorrelated constraint for multi-view DL, so as to reduce the redundancy among dictionaries learned from different views. Experiments on several public datasets demonstrate the effectiveness of the proposed approach.","Uncorrelated Multi-view Fisher Discrimination Dictionary Learning For Recognition Dictionary learning (DL) has now become an important feature learning technique that owns state-of-the-art recog-nition performance. Due to sparse characteristic of data in real-world applications, dictionary learning uses a set of learned dictionary bases to represent the linear decomposi-tion of a data point. Fisher discrimination DL (FDDL) is a representative supervised DL method, which constructs a structured dictionary whose atoms correspond to the class labels. Recent years have witnessed a growing interest in multi-view (more than two views) feature learning tech-niques. Although some multi-view (or multi-modal) DL methods have been presented, there still exists much room for improvement. How to enhance the total discriminability of dictionaries and reduce their redundancy is a crucial re-search topic. To boost the performance of multi-view dic-tionary learning technique, we propose an uncorrelated multi-view fisher discrimination DL (UMFDDL) approach for recognition. By making dictionary atoms correspond to the class labels such that the obtained reconstruction error is discriminative, UMFDDL aims to jointly learn multiple dic-tionaries with totally favorable discriminative power. Fur-thermore, we design the uncorrelated constraint for multi-view DL, so as to reduce the redundancy among dictionaries learned from different views. Experiments on several public datasets demonstrate the effectiveness of the proposed approach. Fisher discrimination Dictionary learning (FDDL)
Multi-view FDDL (MFDDL)
Uncorrelated MFDDL (UMFDDL)
Uncorrelated constraint",uncorrel multiview fisher discrimin dictionari learn recognit dictionari learn dl becom import featur learn techniqu own stateoftheart recognit perform due spars characterist data realworld applic dictionari learn use set learn dictionari base repres linear decomposit data point fisher discrimin dl fddl repres supervis dl method construct structur dictionari whose atom correspond class label recent year wit grow interest multiview two view featur learn techniqu although multiview multimod dl method present still exist much room improv enhanc total discrimin dictionari reduc redund crucial research topic boost perform multiview dictionari learn techniqu propos uncorrel multiview fisher discrimin dl umfddl approach recognit make dictionari atom correspond class label obtain reconstruct error discrimin umfddl aim joint learn multipl dictionari total favor discrimin power furthermor design uncorrel constraint multiview dl reduc redund among dictionari learn differ view experi sever public dataset demonstr effect propos approach fisher discrimin dictionari learn fddl multiview fddl mfddl uncorrel mfddl umfddl uncorrel constraint,7,-1.844299,-15.748574
Semantic Data Representation for Improving Tensor Factorization,"Makoto Nakatsuji, Yasuhiro Fujiwara, Hiroyuki Toda, Hiroshi Sawada, Jin Zheng and James Hendler","AI and the Web (AIW)
Novel Machine Learning Algorithms (NMLA)","Collaborative Filtering
Recommender System
Tensor Factorization
Rating prediction
Bayesian probabilistic tensor factorization
Linked Open Data
Taxonomy
Semantics on the web","AIW: Exploiting Linked Open Data
AIW: Ontologies and the web: creation, extraction, evolution, mapping, merging, and alignment; tags and folksonomies
AIW: Web-based recommendation systems
NMLA: Data Mining and Knowledge Discovery
NMLA: Recommender Systems","Predicting human activities is important for improving recommendation
 systems or analyzing social relationships among users. Those human
 activities are usually represented as multi-object relationships
 (e.g. user's tagging activity for items or user's tweeting activity at
 some location).  Since multi-object relationships are naturally
 represented as a tensor, tensor factorization is becoming more
 important for predicting users' possible activities.  However, the
 prediction accuracy of tensor factorization is weak for ambiguous
 and/or sparsely observed objects. Our solution, Semantic data
 Representation for Tensor Factorization (SRTF), tackles these problems
 by incorporating semantics into tensor factorization based on the
 following ideas: (1) it links objects to vocabularies/taxonomies and
 resolves the ambiguity caused by objects that can be used for multiple
 purposes. (2) it links objects to composite classes that merge classes
 in different kinds of vocabularies/taxonomies (e.g. classes in
 vocabularies for movie genres and those for directors) to avoid low
 prediction accuracy caused by rough-grained semantic space.  (3) it
 lifts sparsely observed objects into their classes to solve the
 sparsity problem for rarely observed objects.
 Experiments show that SRTF achieves 10\% higher accuracy than current
 best methods.","Semantic Data Representation for Improving Tensor Factorization Predicting human activities is important for improving recommendation
 systems or analyzing social relationships among users. Those human
 activities are usually represented as multi-object relationships
 (e.g. user's tagging activity for items or user's tweeting activity at
 some location).  Since multi-object relationships are naturally
 represented as a tensor, tensor factorization is becoming more
 important for predicting users' possible activities.  However, the
 prediction accuracy of tensor factorization is weak for ambiguous
 and/or sparsely observed objects. Our solution, Semantic data
 Representation for Tensor Factorization (SRTF), tackles these problems
 by incorporating semantics into tensor factorization based on the
 following ideas: (1) it links objects to vocabularies/taxonomies and
 resolves the ambiguity caused by objects that can be used for multiple
 purposes. (2) it links objects to composite classes that merge classes
 in different kinds of vocabularies/taxonomies (e.g. classes in
 vocabularies for movie genres and those for directors) to avoid low
 prediction accuracy caused by rough-grained semantic space.  (3) it
 lifts sparsely observed objects into their classes to solve the
 sparsity problem for rarely observed objects.
 Experiments show that SRTF achieves 10\% higher accuracy than current
 best methods. Collaborative Filtering
Recommender System
Tensor Factorization
Rating prediction
Bayesian probabilistic tensor factorization
Linked Open Data
Taxonomy
Semantics on the web",semant data represent improv tensor factor predict human activ import improv recommend system analyz social relationship among user human activ usual repres multiobject relationship eg user tag activ item user tweet activ locat sinc multiobject relationship natur repres tensor tensor factor becom import predict user possibl activ howev predict accuraci tensor factor weak ambigu andor spars observ object solut semant data represent tensor factor srtf tackl problem incorpor semant tensor factor base follow idea 1 link object vocabulariestaxonomi resolv ambigu caus object use multipl purpos 2 link object composit class merg class differ kind vocabulariestaxonomi eg class vocabulari movi genr director avoid low predict accuraci caus roughgrain semant space 3 lift spars observ object class solv sparsiti problem rare observ object experi show srtf achiev 10 higher accuraci current best method collabor filter recommend system tensor factor rate predict bayesian probabilist tensor factor link open data taxonomi semant web,0,-23.488955,3.0387309
Local-To-Global Consistency Implies Tractability of Abduction,Michał Wrona,Knowledge Representation and Reasoning (KRR),"Diagnosis and Abductive Reasoning
Spatial and Temporal Reasoning
Local Consistency
Computational Complexity","KRR: Computational Complexity of Reasoning
KRR: Diagnosis and Abductive Reasoning
KRR: Geometric, Spatial, and Temporal Reasoning
KRR: Nonmonotonic Reasoning
KRR: Qualitative Reasoning","Abduction is a form of nonmonotonic reasoning that looks for an explanation, 
built from a  given set of hypotheses, 
for an observed manifestation according to some knowledge base.
Following the concept behind the Schaefer's parametrization
CSP(Gamma) of the Constraint Satisfaction Problem (CSP),
we study here the complexity of the abduction problem
Abduction(Gamma, Hyp, M) parametrized by certain (omega-categorical) infinite 
relational structures Gamma, Hyp, and M
from which a knowledge base, hypotheses and a manifestation are built, respectively.

We say that Gamma  has local-to-global consistency if
there is k such that establishing strong k-consistency on an instance of CSP(Gamma) yields a globally consistent 
(whose every solution may be obtained straightforwardly from partial solutions) set of constraints.
In this case CSP(Gamma) is solvable in polynomial time.
Our main contribution is an algorithm that under some natural conditions
decides  Abduction(Gamma, Hyp, M) in P when Gamma
has local-to-global consistency. 

As we show in the number of examples, our approach offers
an opportunity to consider abduction
in the context of spatial and temporal reasoning (qualitative calculi such as Allen's
interval algebra or RCC-5) and that our procedure solves some related abduction problems in polynomial time.","Local-To-Global Consistency Implies Tractability of Abduction Abduction is a form of nonmonotonic reasoning that looks for an explanation, 
built from a  given set of hypotheses, 
for an observed manifestation according to some knowledge base.
Following the concept behind the Schaefer's parametrization
CSP(Gamma) of the Constraint Satisfaction Problem (CSP),
we study here the complexity of the abduction problem
Abduction(Gamma, Hyp, M) parametrized by certain (omega-categorical) infinite 
relational structures Gamma, Hyp, and M
from which a knowledge base, hypotheses and a manifestation are built, respectively.

We say that Gamma  has local-to-global consistency if
there is k such that establishing strong k-consistency on an instance of CSP(Gamma) yields a globally consistent 
(whose every solution may be obtained straightforwardly from partial solutions) set of constraints.
In this case CSP(Gamma) is solvable in polynomial time.
Our main contribution is an algorithm that under some natural conditions
decides  Abduction(Gamma, Hyp, M) in P when Gamma
has local-to-global consistency. 

As we show in the number of examples, our approach offers
an opportunity to consider abduction
in the context of spatial and temporal reasoning (qualitative calculi such as Allen's
interval algebra or RCC-5) and that our procedure solves some related abduction problems in polynomial time. Diagnosis and Abductive Reasoning
Spatial and Temporal Reasoning
Local Consistency
Computational Complexity",localtoglob consist impli tractabl abduct abduct form nonmonoton reason look explan built given set hypothes observ manifest accord knowledg base follow concept behind schaefer parametr cspgamma constraint satisfact problem csp studi complex abduct problem abductiongamma hyp parametr certain omegacategor infinit relat structur gamma hyp knowledg base hypothes manifest built respect say gamma localtoglob consist k establish strong kconsist instanc cspgamma yield global consist whose everi solut may obtain straightforward partial solut set constraint case cspgamma solvabl polynomi time main contribut algorithm natur condit decid abductiongamma hyp p gamma localtoglob consist show number exampl approach offer opportun consid abduct context spatial tempor reason qualit calculi allen interv algebra rcc5 procedur solv relat abduct problem polynomi time diagnosi abduct reason spatial tempor reason local consist comput complex,3,-4.2178755,8.39136
Generalized Higher-Order Tensor Decomposition via Parallel ADMM,"Fanhua Shang, Yuanyuan Liu and James Cheng",Novel Machine Learning Algorithms (NMLA),"Tensor decomposition
Higher-order orthogonal iteration
Parallel Optimization","NMLA: Dimension Reduction/Feature Selection
NMLA: Feature Construction/Reformulation
NMLA: Unsupervised Learning (Other)","Higher-order tensors are becoming prevalent in many scientific areas such as computer vision, social network analysis, data mining and neuroscience. Traditional tensor decomposition approaches face three major challenges: model selecting, gross corruptions, and computational efficiency. To address these problems, we first propose a parallel trace norm regularized tensor decomposition method, and formulate it as a convex optimization problem. This method does not require the rank of each model to be specified beforehand, and can automatically determine the number of factors in each mode through our optimization scheme. By considering the low-rank structure of the observed tensor, we analyze the equivalent relationship of the trace norm between a low-rank tensor and its core tensor. Then, we cast a non-convex tensor decomposition model into a weighted combination of multiple much smaller-scale matrix trace norm minimization. Finally, we develop two parallel alternating direction methods of multipliers (ADMM) to solve the proposed problems. Experimental results verify that our regularized formulation is reasonable, and our methods are very robust to noise or outliers.","Generalized Higher-Order Tensor Decomposition via Parallel ADMM Higher-order tensors are becoming prevalent in many scientific areas such as computer vision, social network analysis, data mining and neuroscience. Traditional tensor decomposition approaches face three major challenges: model selecting, gross corruptions, and computational efficiency. To address these problems, we first propose a parallel trace norm regularized tensor decomposition method, and formulate it as a convex optimization problem. This method does not require the rank of each model to be specified beforehand, and can automatically determine the number of factors in each mode through our optimization scheme. By considering the low-rank structure of the observed tensor, we analyze the equivalent relationship of the trace norm between a low-rank tensor and its core tensor. Then, we cast a non-convex tensor decomposition model into a weighted combination of multiple much smaller-scale matrix trace norm minimization. Finally, we develop two parallel alternating direction methods of multipliers (ADMM) to solve the proposed problems. Experimental results verify that our regularized formulation is reasonable, and our methods are very robust to noise or outliers. Tensor decomposition
Higher-order orthogonal iteration
Parallel Optimization",general higherord tensor decomposit via parallel admm higherord tensor becom preval mani scientif area comput vision social network analysi data mine neurosci tradit tensor decomposit approach face three major challeng model select gross corrupt comput effici address problem first propos parallel trace norm regular tensor decomposit method formul convex optim problem method requir rank model specifi beforehand automat determin number factor mode optim scheme consid lowrank structur observ tensor analyz equival relationship trace norm lowrank tensor core tensor cast nonconvex tensor decomposit model weight combin multipl much smallerscal matrix trace norm minim final develop two parallel altern direct method multipli admm solv propos problem experiment result verifi regular formul reason method robust nois outlier tensor decomposit higherord orthogon iter parallel optim,7,-22.95869,2.4850879
Abduction Framework for Repairing Incomplete  EL Ontologies:  Complexity Results and Algorithms,"Fang Wei-Kleiner, Zlatan Dragisic and Patrick Lambrix","AI and the Web (AIW)
Knowledge Representation and Reasoning (KRR)","ontology debugging
ontology engineering
description logics","AIW: Ontologies and the web: creation, extraction, evolution, mapping, merging, and alignment; tags and folksonomies
KRR: Ontologies
KRR: Description Logics
KRR: Diagnosis and Abductive Reasoning","In this paper we consider the problem of repairing missing is-a relations in ontologies. We formalize the problem as a generalized TBox abduction problem (GTAP).
Based on this abduction framework, we present complexity results for the existence, relevance and necessity decision problems for the GTAP with and without some
specific preference relations for ontologies that can be represented using a member of the EL family of description logics. Further, we present an algorithm for finding solutions, a system as well as experiments.","Abduction Framework for Repairing Incomplete  EL Ontologies:  Complexity Results and Algorithms In this paper we consider the problem of repairing missing is-a relations in ontologies. We formalize the problem as a generalized TBox abduction problem (GTAP).
Based on this abduction framework, we present complexity results for the existence, relevance and necessity decision problems for the GTAP with and without some
specific preference relations for ontologies that can be represented using a member of the EL family of description logics. Further, we present an algorithm for finding solutions, a system as well as experiments. ontology debugging
ontology engineering
description logics",abduct framework repair incomplet el ontolog complex result algorithm paper consid problem repair miss isa relat ontolog formal problem general tbox abduct problem gtap base abduct framework present complex result exist relev necess decis problem gtap without specif prefer relat ontolog repres use member el famili descript logic present algorithm find solut system well experi ontolog debug ontolog engin descript logic,8,24.121265,6.989335
Mixing-time Regularized Policy Gradient,"Tetsuro Morimura, Takayuki Osogami and Tomoyuki Shirai",Novel Machine Learning Algorithms (NMLA),"Reinforcement learning
Policy gradient
Markov chain mixing time","CS: Problem solving and decision making
NMLA: Reinforcement Learning
RU: Sequential Decision Making","Policy gradient reinforcement learning (PGRL) methods have received substantial attention as a mean for seeking stochastic policies that maximize a cumulative reward. However, PGRL methods can often take a huge number of learning steps before it finds a reasonable stochastic policy. This learning speed depends on the mixing time of the Markov chains that are given by the policies that PGRL explores. In this paper, we give a new PGRL approach that regularizes the rule of updating the policy with the hitting time that bounds the mixing time. In particular, hitting-time regressions based on temporal-difference learning are proposed. The proposed approach will keep the Markov chain compact and can improve the learning efficiency. Numerical experiments show the proposed method outperforms the conventional policy gradient methods.","Mixing-time Regularized Policy Gradient Policy gradient reinforcement learning (PGRL) methods have received substantial attention as a mean for seeking stochastic policies that maximize a cumulative reward. However, PGRL methods can often take a huge number of learning steps before it finds a reasonable stochastic policy. This learning speed depends on the mixing time of the Markov chains that are given by the policies that PGRL explores. In this paper, we give a new PGRL approach that regularizes the rule of updating the policy with the hitting time that bounds the mixing time. In particular, hitting-time regressions based on temporal-difference learning are proposed. The proposed approach will keep the Markov chain compact and can improve the learning efficiency. Numerical experiments show the proposed method outperforms the conventional policy gradient methods. Reinforcement learning
Policy gradient
Markov chain mixing time",mixingtim regular polici gradient polici gradient reinforc learn pgrl method receiv substanti attent mean seek stochast polici maxim cumul reward howev pgrl method often take huge number learn step find reason stochast polici learn speed depend mix time markov chain given polici pgrl explor paper give new pgrl approach regular rule updat polici hit time bound mix time particular hittingtim regress base temporaldiffer learn propos propos approach keep markov chain compact improv learn effici numer experi show propos method outperform convent polici gradient method reinforc learn polici gradient markov chain mix time,4,-0.8459042,-8.679623
Exploring the Boundaries of Decidable Verification of Non-Terminating Golog Programs,"Jens Classen, Martin Liebenberg, Gerhard Lakemeyer and Benjamin Zarrieß",Knowledge Representation and Reasoning (KRR),"Situation Calculus
Golog
Verification","KRR: Action, Change, and Causality
KRR: Geometric, Spatial, and Temporal Reasoning","The action programming language \Golog\ has been found useful for the control of autonomous agents such as mobile robots. In scenarios like these, tasks are often open-ended so that the respective control programs are non-terminating. Before deploying such programs on a robot, it is often desirable to verify that they meet certain requirements. For this purpose, Claßen and Lakemeyer recently introduced algorithms for the verification of temporal properties of Golog programs. However, given the expressiveness of Golog, their verification procedures are not guaranteed to terminate. In this paper, we show how decidability can be obtained by suitably restricting the underlying base logic, the effect axioms for primitive actions, and the use of actions within Golog programs.  Moreover, we show that dropping any of these restrictions immediately leads to undecidability of the verification problem.","Exploring the Boundaries of Decidable Verification of Non-Terminating Golog Programs The action programming language \Golog\ has been found useful for the control of autonomous agents such as mobile robots. In scenarios like these, tasks are often open-ended so that the respective control programs are non-terminating. Before deploying such programs on a robot, it is often desirable to verify that they meet certain requirements. For this purpose, Claßen and Lakemeyer recently introduced algorithms for the verification of temporal properties of Golog programs. However, given the expressiveness of Golog, their verification procedures are not guaranteed to terminate. In this paper, we show how decidability can be obtained by suitably restricting the underlying base logic, the effect axioms for primitive actions, and the use of actions within Golog programs.  Moreover, we show that dropping any of these restrictions immediately leads to undecidability of the verification problem. Situation Calculus
Golog
Verification",explor boundari decid verif nontermin golog program action program languag golog found use control autonom agent mobil robot scenario like task often openend respect control program nontermin deploy program robot often desir verifi meet certain requir purpos claßen lakemey recent introduc algorithm verif tempor properti golog program howev given express golog verif procedur guarante termin paper show decid obtain suitabl restrict under base logic effect axiom primit action use action within golog program moreov show drop restrict immedi lead undecid verif problem situat calculus golog verif,3,-14.473475,2.339733
Using Timed Game Automata to Synthesize Execution Strategies for Simple Temporal Networks with Uncertainty,"Alessandro Cimatti, Luke Hunsberger, Andrea Micheli and Marco Roveri","Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","Dynamic Controllability
Strategy Synthesis
Simple Temporal Networks with Uncertainty
Timed Game Automata","PS: Plan Execution and Monitoring
PS: Scheduling
RU: Uncertainty in AI (General/Other)","A Simple Temporal Network with Uncertainty (STNU) is a structure for
representing and reasoning about temporal constraints in domains where
some temporal durations are not controlled by the executor.  The most
important property of an STNU is whether it is dynamically
controllable (DC); that is, whether there exists a strategy for
executing the controllable time-points that guarantees that all
constraints will be satisfied no matter how the uncontrollable
durations turn out.

This paper provides a novel mapping from STNUs to Timed Game Automata
(TGAs) that: (1) explicates the deep theoretical relationships between
STNUs and TGAs; and (2) enables the memoryless strategies generated
from the TGA to be transformed into equivalent STNU execution
strategies that reduce the real-time computational burden for the
executor.  The paper formally proves that the STNU-to-TGA encoding
properly captures the execution semantics of STNUs.  It also provides
experimental evidence of the proposed approaches, generating offline
execution strategies for dynamically controllable STNUs encoded as
TGAs.","Using Timed Game Automata to Synthesize Execution Strategies for Simple Temporal Networks with Uncertainty A Simple Temporal Network with Uncertainty (STNU) is a structure for
representing and reasoning about temporal constraints in domains where
some temporal durations are not controlled by the executor.  The most
important property of an STNU is whether it is dynamically
controllable (DC); that is, whether there exists a strategy for
executing the controllable time-points that guarantees that all
constraints will be satisfied no matter how the uncontrollable
durations turn out.

This paper provides a novel mapping from STNUs to Timed Game Automata
(TGAs) that: (1) explicates the deep theoretical relationships between
STNUs and TGAs; and (2) enables the memoryless strategies generated
from the TGA to be transformed into equivalent STNU execution
strategies that reduce the real-time computational burden for the
executor.  The paper formally proves that the STNU-to-TGA encoding
properly captures the execution semantics of STNUs.  It also provides
experimental evidence of the proposed approaches, generating offline
execution strategies for dynamically controllable STNUs encoded as
TGAs. Dynamic Controllability
Strategy Synthesis
Simple Temporal Networks with Uncertainty
Timed Game Automata",use time game automata synthes execut strategi simpl tempor network uncertainti simpl tempor network uncertainti stnu structur repres reason tempor constraint domain tempor durat control executor import properti stnu whether dynam control dc whether exist strategi execut control timepoint guarante constraint satisfi matter uncontrol durat turn paper provid novel map stnus time game automata tgas 1 explic deep theoret relationship stnus tgas 2 enabl memoryless strategi generat tga transform equival stnu execut strategi reduc realtim comput burden executor paper formal prove stnutotga encod proper captur execut semant stnus also provid experiment evid propos approach generat offlin execut strategi dynam control stnus encod tgas dynam control strategi synthesi simpl tempor network uncertainti time game automata,3,4.070106,6.938128
Adaptation Guided Case Base Maintenance,Vahid Jalali and David Leake,Novel Machine Learning Algorithms (NMLA),"Adaptation-Guided Case-Based Maintenance
Case-Based Maintenance
Case-Based Reasoning",NMLA: Case-Based Reasoning,"In case-based reasoning (CBR), problems are solved by retrieving prior cases and adapting their solutions to fit new problems. Controlling the growth of the case base in CBR is a fundamental problem. Much research on case-base maintenance has developed methods aimed at compacting case bases while maintaining system competence, by deleting cases whose absence is considered least likely to degrade the system's problem-solving, given static case adaptation knowledge. This paper proposes adaptation-guided case-base maintenance (AGCBM), a case-base maintenance approach exploiting the ability to dynamically generate new adaptation knowledge from cases. In AGCMB, case retention decisions are based both on their value as base cases for solving problems and on their value for generating new adaptation rules, in turn increasing the problem-solving value of other cases in the case base. The paper tests the method for numerical prediction tasks (case-based regression) in which rules are generated automatically using the case difference heuristic. Tests on four sample domains compare accuracy with a set of five candidate case-based maintenance methods, for varying case-base densities. AGCBM outperformed the alternatives all domains, with the benefit most substantial for the greatest amounts of compression.","Adaptation Guided Case Base Maintenance In case-based reasoning (CBR), problems are solved by retrieving prior cases and adapting their solutions to fit new problems. Controlling the growth of the case base in CBR is a fundamental problem. Much research on case-base maintenance has developed methods aimed at compacting case bases while maintaining system competence, by deleting cases whose absence is considered least likely to degrade the system's problem-solving, given static case adaptation knowledge. This paper proposes adaptation-guided case-base maintenance (AGCBM), a case-base maintenance approach exploiting the ability to dynamically generate new adaptation knowledge from cases. In AGCMB, case retention decisions are based both on their value as base cases for solving problems and on their value for generating new adaptation rules, in turn increasing the problem-solving value of other cases in the case base. The paper tests the method for numerical prediction tasks (case-based regression) in which rules are generated automatically using the case difference heuristic. Tests on four sample domains compare accuracy with a set of five candidate case-based maintenance methods, for varying case-base densities. AGCBM outperformed the alternatives all domains, with the benefit most substantial for the greatest amounts of compression. Adaptation-Guided Case-Based Maintenance
Case-Based Maintenance
Case-Based Reasoning",adapt guid case base mainten casebas reason cbr problem solv retriev prior case adapt solut fit new problem control growth case base cbr fundament problem much research casebas mainten develop method aim compact case base maintain system compet delet case whose absenc consid least like degrad system problemsolv given static case adapt knowledg paper propos adaptationguid casebas mainten agcbm casebas mainten approach exploit abil dynam generat new adapt knowledg case agcmb case retent decis base valu base case solv problem valu generat new adapt rule turn increas problemsolv valu case case base paper test method numer predict task casebas regress rule generat automat use case differ heurist test four sampl domain compar accuraci set five candid casebas mainten method vari casebas densiti agcbm outperform altern domain benefit substanti greatest amount compress adaptationguid casebas mainten casebas mainten casebas reason,5,-2.8490722,0.56238395
The Fisher Market Game: Equilibrium and Welfare,"Simina Brânzei, Yiling Chen, Xiaotie Deng, Aris Filos-Ratsikas, Søren Kristoffer Stiil Frederiksen and Jie Zhang",Game Theory and Economic Paradigms (GTEP),"Fisher markets
Fisher market game
Equilibrium analysis
Price of anarchy","GTEP: Auctions and Market-Based Systems
GTEP: Game Theory","The Fisher market model is one of the most fundamental resource allocation models in economics. In a Fisher market, the prices and allocations of goods are determined according to the preferences and budgets of buyers to clear the market.

In a Fisher market game, however, buyers are strategic and report their preferences over goods; the market-clearing prices and allocations are then determined based on their reported preferences rather than their real preferences.
We show that the Fisher market game always has a pure Nash equilibrium, for buyers with linear, Leontief, and Cobb-Douglas utility functions, which are three representative classes of utility functions in the important Constant Elasticity of Substitution (CES) family. Furthermore, to quantify the social efficiency, we prove Price of Anarchy bounds for the game when the utility functions of buyers fall into these three classes respectively.","The Fisher Market Game: Equilibrium and Welfare The Fisher market model is one of the most fundamental resource allocation models in economics. In a Fisher market, the prices and allocations of goods are determined according to the preferences and budgets of buyers to clear the market.

In a Fisher market game, however, buyers are strategic and report their preferences over goods; the market-clearing prices and allocations are then determined based on their reported preferences rather than their real preferences.
We show that the Fisher market game always has a pure Nash equilibrium, for buyers with linear, Leontief, and Cobb-Douglas utility functions, which are three representative classes of utility functions in the important Constant Elasticity of Substitution (CES) family. Furthermore, to quantify the social efficiency, we prove Price of Anarchy bounds for the game when the utility functions of buyers fall into these three classes respectively. Fisher markets
Fisher market game
Equilibrium analysis
Price of anarchy",fisher market game equilibrium welfar fisher market model one fundament resourc alloc model econom fisher market price alloc good determin accord prefer budget buyer clear market fisher market game howev buyer strateg report prefer good marketclear price alloc determin base report prefer rather real prefer show fisher market game alway pure nash equilibrium buyer linear leontief cobbdougla util function three repres class util function import constant elast substitut ces famili furthermor quantifi social effici prove price anarchi bound game util function buyer fall three class respect fisher market fisher market game equilibrium analysi price anarchi,9,6.8327923,6.0464206
Huffman Coding for Storing Non-uniformly Distributed Messages in Networks of Neural Cliques,"Bartosz Boguslawski, Vincent Gripon, Fabrice Seguin and Frédéric Heitzmann","Applications (APP)
Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","neural clique
sparsity
associative memory
non-uniform distribution
compression code","APP: Security and Privacy
APP: Other Applications
MLA: Machine Learning Applications (General/other)
NMLA: Neural Networks/Deep Learning","Associative memories are data structures that allow retrieval of previously stored messages given part of their content. They thus behave similarly to human brain's memory that is capable for instance of retrieving the end of a song given its beginning. Among different families of associative memories, sparse ones are known to provide the best efficiency (ratio of the number of bits stored to that of bits used). Nevertheless, it is well known that non-uniformity of the stored messages can lead to dramatic decrease in performance. Recently, a new family of sparse associative memories achieving almost-optimal efficiency has been proposed. Their structure induces a direct mapping between input messages and stored patterns. In this work, we show the impact of non-uniformity on the performance of this recent model and we exploit the structure of the model to introduce several strategies to allow for efficient storage of non-uniform messages. We show that a technique based on Huffman coding is the most efficient.","Huffman Coding for Storing Non-uniformly Distributed Messages in Networks of Neural Cliques Associative memories are data structures that allow retrieval of previously stored messages given part of their content. They thus behave similarly to human brain's memory that is capable for instance of retrieving the end of a song given its beginning. Among different families of associative memories, sparse ones are known to provide the best efficiency (ratio of the number of bits stored to that of bits used). Nevertheless, it is well known that non-uniformity of the stored messages can lead to dramatic decrease in performance. Recently, a new family of sparse associative memories achieving almost-optimal efficiency has been proposed. Their structure induces a direct mapping between input messages and stored patterns. In this work, we show the impact of non-uniformity on the performance of this recent model and we exploit the structure of the model to introduce several strategies to allow for efficient storage of non-uniform messages. We show that a technique based on Huffman coding is the most efficient. neural clique
sparsity
associative memory
non-uniform distribution
compression code",huffman code store nonuniform distribut messag network neural cliqu associ memori data structur allow retriev previous store messag given part content thus behav similar human brain memori capabl instanc retriev end song given begin among differ famili associ memori spars one known provid best effici ratio number bit store bit use nevertheless well known nonuniform store messag lead dramat decreas perform recent new famili spars associ memori achiev almostoptim effici propos structur induc direct map input messag store pattern work show impact nonuniform perform recent model exploit structur model introduc sever strategi allow effici storag nonuniform messag show techniqu base huffman code effici neural cliqu sparsiti associ memori nonuniform distribut compress code,8,2.9328315,0.975577
An Agent-Based Model Studying the Acquisition of a Language System of Logical Constructions,Josefina Sierra-Santibanez,Cognitive Modeling (CM),"Cognitive Modeling
Symbolic AI
Simulating Humans
Adaptive Behavior","CM: Adaptive Behavior
CM: Simulating Humans
CM: Symbolic AI","This paper presents an agent-based model that studies the emergence
and evolution of a language system of logical constructions, i.e. a
vocabulary and a set of grammatical constructions that allow
expressing logical combinations of categories. The model assumes
the agents have a common vocabulary for basic categories, the ability
to construct logical combinations of categories using Boolean
functions, and some general purpose cognitive capacities for
invention, adoption, induction and adaptation. But it does not assume the agents
have a vocabulary for Boolean functions nor grammatical constructions
for expressing such logical combinations of categories through
language.  The results of the experiments we have performed show that
a language system of logical constructions emerges as a result of a
process of self-organisation of the individual agents'
interactions when these agents adapt their preferences for vocabulary
and grammatical constructions to those they observe are used more
often by the rest of the population, and that such language system 
is transmitted from one generation to the next.","An Agent-Based Model Studying the Acquisition of a Language System of Logical Constructions This paper presents an agent-based model that studies the emergence
and evolution of a language system of logical constructions, i.e. a
vocabulary and a set of grammatical constructions that allow
expressing logical combinations of categories. The model assumes
the agents have a common vocabulary for basic categories, the ability
to construct logical combinations of categories using Boolean
functions, and some general purpose cognitive capacities for
invention, adoption, induction and adaptation. But it does not assume the agents
have a vocabulary for Boolean functions nor grammatical constructions
for expressing such logical combinations of categories through
language.  The results of the experiments we have performed show that
a language system of logical constructions emerges as a result of a
process of self-organisation of the individual agents'
interactions when these agents adapt their preferences for vocabulary
and grammatical constructions to those they observe are used more
often by the rest of the population, and that such language system 
is transmitted from one generation to the next. Cognitive Modeling
Symbolic AI
Simulating Humans
Adaptive Behavior",agentbas model studi acquisit languag system logic construct paper present agentbas model studi emerg evolut languag system logic construct ie vocabulari set grammat construct allow express logic combin categori model assum agent common vocabulari basic categori abil construct logic combin categori use boolean function general purpos cognit capac invent adopt induct adapt assum agent vocabulari boolean function grammat construct express logic combin categori languag result experi perform show languag system logic construct emerg result process selforganis individu agent interact agent adapt prefer vocabulari grammat construct observ use often rest popul languag system transmit one generat next cognit model symbol ai simul human adapt behavior,0,3.3198962,9.604595
Tree-Based On-line Reinforcement Learning,Andre Barreto,Reasoning under Uncertainty (RU),"Reinforcement Learning
Markov Decision Processes
Fitted Q-Iteration",RU: Sequential Decision Making,"Fitted Q-iteration (FQI) stands out among reinforcement-learning algorithms for its flexibility and easy of use. FQI can be combined with any regression method, and this choice determines the algorithm's theoretical and computational properties. The combination of FQI with an ensemble of regression trees gives rises to an algorithm, FQIT, that is computationally efficient, scalable to high dimensional spaces, and robust to irrelevant variables, outliers, and noise. Despite its nice properties and good performance in practice, FQIT also has some limitations: the fact that an ensemble of trees must be constructed (or updated) at each iteration confines the algorithm to the batch scenario. This paper aims to address this specific issue. Based on a strategy recently proposed in the literature, called the stochastic-factorization trick, we propose a modification of FQIT that makes it fully incremental, and thus suitable for on-line learning. We call the resulting method tree-based stochastic factorization (TBSF). We derive an upper bound for the difference between the value functions computed by FQIT and TBSF, and also show in which circumstances the approximations coincide. A series of computational experiments is presented to illustrate the properties of TBSF and to show its usefulness in practice.","Tree-Based On-line Reinforcement Learning Fitted Q-iteration (FQI) stands out among reinforcement-learning algorithms for its flexibility and easy of use. FQI can be combined with any regression method, and this choice determines the algorithm's theoretical and computational properties. The combination of FQI with an ensemble of regression trees gives rises to an algorithm, FQIT, that is computationally efficient, scalable to high dimensional spaces, and robust to irrelevant variables, outliers, and noise. Despite its nice properties and good performance in practice, FQIT also has some limitations: the fact that an ensemble of trees must be constructed (or updated) at each iteration confines the algorithm to the batch scenario. This paper aims to address this specific issue. Based on a strategy recently proposed in the literature, called the stochastic-factorization trick, we propose a modification of FQIT that makes it fully incremental, and thus suitable for on-line learning. We call the resulting method tree-based stochastic factorization (TBSF). We derive an upper bound for the difference between the value functions computed by FQIT and TBSF, and also show in which circumstances the approximations coincide. A series of computational experiments is presented to illustrate the properties of TBSF and to show its usefulness in practice. Reinforcement Learning
Markov Decision Processes
Fitted Q-Iteration",treebas onlin reinforc learn fit qiter fqi stand among reinforcementlearn algorithm flexibl easi use fqi combin regress method choic determin algorithm theoret comput properti combin fqi ensembl regress tree give rise algorithm fqit comput effici scalabl high dimension space robust irrelev variabl outlier nois despit nice properti good perform practic fqit also limit fact ensembl tree must construct updat iter confin algorithm batch scenario paper aim address specif issu base strategi recent propos literatur call stochasticfactor trick propos modif fqit make fulli increment thus suitabl onlin learn call result method treebas stochast factor tbsf deriv upper bound differ valu function comput fqit tbsf also show circumst approxim coincid seri comput experi present illustr properti tbsf show use practic reinforc learn markov decis process fit qiter,4,0.1144751,-7.6234903
Doubly Regularized Portfolio with Risk Minimization,"Weiwei Shen, Jun Wang and Shiqian Ma","Applications (APP)
Machine Learning Applications (MLA)","Portfolion Management
Risk Minimization
Doubly Regularized Portfolio","APP: Other Applications
MLA: Machine Learning Applications (General/other)","Due to recent empirical success, machine learning algorithms have drawn sufficient attention and are becoming important analysis tools in financial industry. In particular, as the core engine of many financial services such as private wealth and pension fund management, portfolio management calls for the application of those novel algorithms. Most of portfolio allocation strategies do not account for costs from market frictions such as transaction costs and capital gain taxes, as the complexity of sensible cost models often causes the induced problem intractable. In this paper, we propose a doubly regularized sparse and consistent portfolio that provides a modest but effective solution to the above difficulty. Specifically, as all kinds of trading costs primarily root in large transaction volumes, to reduce volumes we synergistically combine two penalty terms with classic risk minimization models to ensure: (1) only a small set of assets are selected to invest in each period; (2) portfolios in subsequent trading periods are similar. To assess the new portfolio, we apply standard evaluation criteria and conduct extensive experiments on well-known benchmarks and market datasets. Compared to various state-of-the-art portfolios, the proposed portfolio demonstrates a superior performance of having both higher risk-adjusted returns and dramatically decreased transaction volumes.","Doubly Regularized Portfolio with Risk Minimization Due to recent empirical success, machine learning algorithms have drawn sufficient attention and are becoming important analysis tools in financial industry. In particular, as the core engine of many financial services such as private wealth and pension fund management, portfolio management calls for the application of those novel algorithms. Most of portfolio allocation strategies do not account for costs from market frictions such as transaction costs and capital gain taxes, as the complexity of sensible cost models often causes the induced problem intractable. In this paper, we propose a doubly regularized sparse and consistent portfolio that provides a modest but effective solution to the above difficulty. Specifically, as all kinds of trading costs primarily root in large transaction volumes, to reduce volumes we synergistically combine two penalty terms with classic risk minimization models to ensure: (1) only a small set of assets are selected to invest in each period; (2) portfolios in subsequent trading periods are similar. To assess the new portfolio, we apply standard evaluation criteria and conduct extensive experiments on well-known benchmarks and market datasets. Compared to various state-of-the-art portfolios, the proposed portfolio demonstrates a superior performance of having both higher risk-adjusted returns and dramatically decreased transaction volumes. Portfolion Management
Risk Minimization
Doubly Regularized Portfolio",doubli regular portfolio risk minim due recent empir success machin learn algorithm drawn suffici attent becom import analysi tool financi industri particular core engin mani financi servic privat wealth pension fund manag portfolio manag call applic novel algorithm portfolio alloc strategi account cost market friction transact cost capit gain tax complex sensibl cost model often caus induc problem intract paper propos doubli regular spars consist portfolio provid modest effect solut difficulti specif kind trade cost primarili root larg transact volum reduc volum synergist combin two penalti term classic risk minim model ensur 1 small set asset select invest period 2 portfolio subsequ trade period similar assess new portfolio appli standard evalu criteria conduct extens experi wellknown benchmark market dataset compar various stateoftheart portfolio propos portfolio demonstr superior perform higher riskadjust return dramat decreas transact volum portfolion manag risk minim doubli regular portfolio,4,12.362543,-0.5827678
"Non-convex feature learning via $\ell_{p,\infty}$ operator","Deguang Kong, Chris Ding and Qihe Pan","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","feature
sparse
learning",,"We present a feature selection method for solving sparse regularization problem, which has a composite regularization of $\ell_p$ norm and $\ell_{\infty}$ norm.
We use proximal gradient method to solve this \L1inf operator problem, where a simple but efficient algorithm is designed to minimize a relative simple objective function, which contains a vector of $\ell_2$ norm and $\ell_\infty$ norm. Proposed method brings some insight for solving sparsity-favoring norm, and
extensive experiments are conducted to characterize the effect of varying $p$ and to compare with other approaches on real world multi-class and multi-label datasets.","Non-convex feature learning via $\ell_{p,\infty}$ operator We present a feature selection method for solving sparse regularization problem, which has a composite regularization of $\ell_p$ norm and $\ell_{\infty}$ norm.
We use proximal gradient method to solve this \L1inf operator problem, where a simple but efficient algorithm is designed to minimize a relative simple objective function, which contains a vector of $\ell_2$ norm and $\ell_\infty$ norm. Proposed method brings some insight for solving sparsity-favoring norm, and
extensive experiments are conducted to characterize the effect of varying $p$ and to compare with other approaches on real world multi-class and multi-label datasets. feature
sparse
learning",nonconvex featur learn via ellpinfti oper present featur select method solv spars regular problem composit regular ellp norm ellinfti norm use proxim gradient method solv l1inf oper problem simpl effici algorithm design minim relat simpl object function contain vector ell2 norm ellinfti norm propos method bring insight solv sparsityfavor norm extens experi conduct character effect vari p compar approach real world multiclass multilabel dataset featur spars learn,4,2.963435,-20.166603
Robust Non-negative Dictionary Learning,"Deguang Kong, Chris Ding and Qihe Pan","Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","dictionary learning
non-negative
clustering
multiplicative",,"Dictionary learning plays an important role in machine learning, where data vectors are modeled as a sparse linear combinations of basis factors (i.e., dictionary). However, how to conduct dictionary learning in noisy environment has not been well studied. Moreover, in practice, the dictionary (i.e., the lower rank approximation of the data matrix) and the sparse representations are required to be nonnegative, such as applications for image annotation, document summarization, microarray analysis. In this paper, we propose a new formulation for non-negative dictionary learning in noisy environment, where structure sparsity is enforced on sparse representation. The proposed new formulation is also robust for data with noises and outliers, due to a robust loss function used.  We derive an efficient multiplicative updating algorithm to solve the optimization problem,  where dictionary and sparse representation are updated iteratively. We prove the convergence and correctness of proposed algorithm rigorously.
We show the differences of dictionary at different level of sparsity constraint.
The proposed algorithm can be adapted for data clustering and semi-supervised learning purpose. Promising results in extensive experiments validate the effectiveness of proposed approach.","Robust Non-negative Dictionary Learning Dictionary learning plays an important role in machine learning, where data vectors are modeled as a sparse linear combinations of basis factors (i.e., dictionary). However, how to conduct dictionary learning in noisy environment has not been well studied. Moreover, in practice, the dictionary (i.e., the lower rank approximation of the data matrix) and the sparse representations are required to be nonnegative, such as applications for image annotation, document summarization, microarray analysis. In this paper, we propose a new formulation for non-negative dictionary learning in noisy environment, where structure sparsity is enforced on sparse representation. The proposed new formulation is also robust for data with noises and outliers, due to a robust loss function used.  We derive an efficient multiplicative updating algorithm to solve the optimization problem,  where dictionary and sparse representation are updated iteratively. We prove the convergence and correctness of proposed algorithm rigorously.
We show the differences of dictionary at different level of sparsity constraint.
The proposed algorithm can be adapted for data clustering and semi-supervised learning purpose. Promising results in extensive experiments validate the effectiveness of proposed approach. dictionary learning
non-negative
clustering
multiplicative",robust nonneg dictionari learn dictionari learn play import role machin learn data vector model spars linear combin basi factor ie dictionari howev conduct dictionari learn noisi environ well studi moreov practic dictionari ie lower rank approxim data matrix spars represent requir nonneg applic imag annot document summar microarray analysi paper propos new formul nonneg dictionari learn noisi environ structur sparsiti enforc spars represent propos new formul also robust data nois outlier due robust loss function use deriv effici multipl updat algorithm solv optim problem dictionari spars represent updat iter prove converg correct propos algorithm rigor show differ dictionari differ level sparsiti constraint propos algorithm adapt data cluster semisupervis learn purpos promis result extens experi valid effect propos approach dictionari learn nonneg cluster multipl,4,-1.065419,-15.666606
On the Axiomatic Characterization of Runoff Voting Rules,"Rupert Freeman, Markus Brill and Vincent Conitzer",Game Theory and Economic Paradigms (GTEP),"computational social choice
runoff scoring rules
independence of clones",GTEP: Social Choice / Voting,"Runoff voting rules such as single transferable vote (STV) and Baldwin's rule are of particular interest in computational social choice due to their recursive nature and hardness of manipulation, as well as in (human) practice because they are relatively easy to understand. However, they are not known for their compliance with desirable axiomatic properties, which we attempt to rectify here. We characterize runoff rules that are based on scoring rules using two axioms: a weakening of local independence of irrelevant alternatives and a variant of population-consistency. We then show, as our main technical result, that STV is the only runoff scoring rule satisfying an independence-of-clones property. Furthermore, we provide axiomatizations of Baldwin's rule and Coombs' rule.","On the Axiomatic Characterization of Runoff Voting Rules Runoff voting rules such as single transferable vote (STV) and Baldwin's rule are of particular interest in computational social choice due to their recursive nature and hardness of manipulation, as well as in (human) practice because they are relatively easy to understand. However, they are not known for their compliance with desirable axiomatic properties, which we attempt to rectify here. We characterize runoff rules that are based on scoring rules using two axioms: a weakening of local independence of irrelevant alternatives and a variant of population-consistency. We then show, as our main technical result, that STV is the only runoff scoring rule satisfying an independence-of-clones property. Furthermore, we provide axiomatizations of Baldwin's rule and Coombs' rule. computational social choice
runoff scoring rules
independence of clones",axiomat character runoff vote rule runoff vote rule singl transfer vote stv baldwin rule particular interest comput social choic due recurs natur hard manipul well human practic relat easi understand howev known complianc desir axiomat properti attempt rectifi character runoff rule base score rule use two axiom weaken local independ irrelev altern variant populationconsist show main technic result stv runoff score rule satisfi independenceofclon properti furthermor provid axiomat baldwin rule coomb rule comput social choic runoff score rule independ clone,9,16.44353,-16.811352
Supervised Scoring with Monotone Multidimensional Splines,Abraham Othman,Computational Sustainability and AI (CSAI),"Sustainability
Green Buildings
Interpolation
Scientific Computing","CSAI: Modeling and control of complex high-dimensional systems
CSAI: Support for public engagement and decision making by the public
HCC: Optimality in the context of human computation","Scoring involves the compression of a number of quantitative attributes into a single meaningful value. We consider the problem of how to generate scores in a setting where they should be weakly monotone (either non-increasing or non-decreasing) in their dimensions. Our approach allows an expert to score an arbitrary set of points to produce meaningful, continuous, monotone scores over the entire domain, while exactly interpolating through those inputs. In contrast, existing monotone interpolating methods only work in two dimensions and typically require exhaustive grid input. Our technique significantly lowers the bar to score creation, allowing domain experts to develop mathematically coherent scores. The method is used in practice to create the LEED Performance energy and water scores that gauge building sustainability.","Supervised Scoring with Monotone Multidimensional Splines Scoring involves the compression of a number of quantitative attributes into a single meaningful value. We consider the problem of how to generate scores in a setting where they should be weakly monotone (either non-increasing or non-decreasing) in their dimensions. Our approach allows an expert to score an arbitrary set of points to produce meaningful, continuous, monotone scores over the entire domain, while exactly interpolating through those inputs. In contrast, existing monotone interpolating methods only work in two dimensions and typically require exhaustive grid input. Our technique significantly lowers the bar to score creation, allowing domain experts to develop mathematically coherent scores. The method is used in practice to create the LEED Performance energy and water scores that gauge building sustainability. Sustainability
Green Buildings
Interpolation
Scientific Computing",supervis score monoton multidimension spline score involv compress number quantit attribut singl meaning valu consid problem generat score set weak monoton either nonincreas nondecreas dimens approach allow expert score arbitrari set point produc meaning continu monoton score entir domain exact interpol input contrast exist monoton interpol method work two dimens typic requir exhaust grid input techniqu signific lower bar score creation allow domain expert develop mathemat coher score method use practic creat leed perform energi water score gaug build sustain sustain green build interpol scientif comput,5,15.488946,-15.744202
CoreCluster: A Degeneracy Based Graph Clustering Framework,"Christos Giatsidis, Fragkiskos Malliaros, Dimitrios Thilikos and Michalis Vazirgiannis","AI and the Web (AIW)
Applications (APP)","Community detection
Graph clustering
Graph degeneracy
Graph mining","AIW: Social networking and community identification
APP: Social Networks","Graph clustering or community detection constitutes an important
task for investigating the internal structure of graphs, with a
plethora of applications in several diverse domains. Traditional
tools for graph clustering, such as spectral methods, typically suffer
from high time and space complexity. In this article, we present
CoreCluster, an efficient graph clustering framework based on
the concept of graph degeneracy, that can be used along with any
known graph clustering algorithm. Our approach capitalizes on
processing the graph in a hierarchical manner provided by its core
expansion sequence, an ordered partition of the graph into different
levels according to the k-core decomposition. Such a partition
provides a way to process the graph in an incremental manner that
preserves its clustering structure, while making the execution of the
chosen clustering algorithm much faster due to the smaller size of
the graph’s partitions onto which the algorithm operates.","CoreCluster: A Degeneracy Based Graph Clustering Framework Graph clustering or community detection constitutes an important
task for investigating the internal structure of graphs, with a
plethora of applications in several diverse domains. Traditional
tools for graph clustering, such as spectral methods, typically suffer
from high time and space complexity. In this article, we present
CoreCluster, an efficient graph clustering framework based on
the concept of graph degeneracy, that can be used along with any
known graph clustering algorithm. Our approach capitalizes on
processing the graph in a hierarchical manner provided by its core
expansion sequence, an ordered partition of the graph into different
levels according to the k-core decomposition. Such a partition
provides a way to process the graph in an incremental manner that
preserves its clustering structure, while making the execution of the
chosen clustering algorithm much faster due to the smaller size of
the graph’s partitions onto which the algorithm operates. Community detection
Graph clustering
Graph degeneracy
Graph mining",coreclust degeneraci base graph cluster framework graph cluster communiti detect constitut import task investig intern structur graph plethora applic sever divers domain tradit tool graph cluster spectral method typic suffer high time space complex articl present coreclust effici graph cluster framework base concept graph degeneraci use along known graph cluster algorithm approach capit process graph hierarch manner provid core expans sequenc order partit graph differ level accord kcore decomposit partit provid way process graph increment manner preserv cluster structur make execut chosen cluster algorithm much faster due smaller size graph partit onto algorithm oper communiti detect graph cluster graph degeneraci graph mine,7,-8.8174095,7.766248
A Generalized Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles of Complex Types,"Dror Sholomon, Omid E. David and Nathan S. Netanyahu","Novel Machine Learning Algorithms (NMLA)
Vision (VIS)","genetic algorithms
jigsaw puzzle
computer vision
recombination operators","NMLA: Evolutionary Computation
VIS: Perception","In this paper we introduce new types of square-piece jigsaw puzzles, where in addition to the unknown location and orientation of each piece, a piece might also need to be flipped. These puzzles, which are associated with a number of real world problems, are considerably harder, from a computational standpoint. Specifically, we present a novel generalized genetic algorithm (GA)-based solver that can handle puzzle pieces of unknown location and orientation (Type 2 puzzles), and puzzle pieces of unknown location, orientation and attitude (Type 4 puzzles). To the best of our knowledge, our solver provides a new state-of-the-art, solving previously attempted puzzles faster and far more accurately, solving puzzle sizes that have never been attempted before, and solving the newly introduced double-sided puzzle types automatically and effectively. This paper also presents, among other results, the most extensive set of experimental results compiled as of yet, on Type 2 puzzles.","A Generalized Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles of Complex Types In this paper we introduce new types of square-piece jigsaw puzzles, where in addition to the unknown location and orientation of each piece, a piece might also need to be flipped. These puzzles, which are associated with a number of real world problems, are considerably harder, from a computational standpoint. Specifically, we present a novel generalized genetic algorithm (GA)-based solver that can handle puzzle pieces of unknown location and orientation (Type 2 puzzles), and puzzle pieces of unknown location, orientation and attitude (Type 4 puzzles). To the best of our knowledge, our solver provides a new state-of-the-art, solving previously attempted puzzles faster and far more accurately, solving puzzle sizes that have never been attempted before, and solving the newly introduced double-sided puzzle types automatically and effectively. This paper also presents, among other results, the most extensive set of experimental results compiled as of yet, on Type 2 puzzles. genetic algorithms
jigsaw puzzle
computer vision
recombination operators",general genet algorithmbas solver larg jigsaw puzzl complex type paper introduc new type squarepiec jigsaw puzzl addit unknown locat orient piec piec might also need flip puzzl associ number real world problem consider harder comput standpoint specif present novel general genet algorithm gabas solver handl puzzl piec unknown locat orient type 2 puzzl puzzl piec unknown locat orient attitud type 4 puzzl best knowledg solver provid new stateoftheart solv previous attempt puzzl faster far accur solv puzzl size never attempt solv newli introduc doublesid puzzl type automat effect paper also present among result extens set experiment result compil yet type 2 puzzl genet algorithm jigsaw puzzl comput vision recombin oper,0,0.1897873,0.12973164
Solving Zero-Sum Security Games in Discretized Spatio-Temporal Domains,"Haifeng Xu, Fei Fang, Albert Jiang, Vincent Conitzer, Shaddin Dughmi and Milind Tambe","Game Theory and Economic Paradigms (GTEP)
Multiagent Systems (MAS)","Security Games
Zero-Sum Games
Minimax Equilibrium
Oracle
Equilibria Computation","GTEP: Game Theory
GTEP: Equilibrium
MAS: Multiagent Systems (General/other)","Security games model the problem of allocating multiple resources to defend multiple targets. In such settings, the defender's action space is usually exponential in the input size. Therefore, known general-purpose linear or mixed integer program formulations scale up exponentially. One method that has been widely deployed to address this computational issue is to instead compute the marginal probabilities, of which there are only polynomially many.

In this paper, we address a class of problems that cannot be handled by previous approaches based on marginal probabilities. We consider security games in discretized spatio-temporal domains, in which the schedule set is so large that even the marginal probability formulation has exponential size. We develop novel algorithms under an oracle-based algorithmic framework and show that
this framework allows us to efficiently compute Stackelberg mixed strategy 
when the problem allows a polynomial-time oracle. For the cases in which efficient oracles are difficult to find, we propose new direct algorithms or prove hardness results. All our algorithms are examined in experiments with realistic and artificial data.","Solving Zero-Sum Security Games in Discretized Spatio-Temporal Domains Security games model the problem of allocating multiple resources to defend multiple targets. In such settings, the defender's action space is usually exponential in the input size. Therefore, known general-purpose linear or mixed integer program formulations scale up exponentially. One method that has been widely deployed to address this computational issue is to instead compute the marginal probabilities, of which there are only polynomially many.

In this paper, we address a class of problems that cannot be handled by previous approaches based on marginal probabilities. We consider security games in discretized spatio-temporal domains, in which the schedule set is so large that even the marginal probability formulation has exponential size. We develop novel algorithms under an oracle-based algorithmic framework and show that
this framework allows us to efficiently compute Stackelberg mixed strategy 
when the problem allows a polynomial-time oracle. For the cases in which efficient oracles are difficult to find, we propose new direct algorithms or prove hardness results. All our algorithms are examined in experiments with realistic and artificial data. Security Games
Zero-Sum Games
Minimax Equilibrium
Oracle
Equilibria Computation",solv zerosum secur game discret spatiotempor domain secur game model problem alloc multipl resourc defend multipl target set defend action space usual exponenti input size therefor known generalpurpos linear mix integ program formul scale exponenti one method wide deploy address comput issu instead comput margin probabl polynomi mani paper address class problem cannot handl previous approach base margin probabl consid secur game discret spatiotempor domain schedul set larg even margin probabl formul exponenti size develop novel algorithm oraclebas algorithm framework show framework allow us effici comput stackelberg mix strategi problem allow polynomialtim oracl case effici oracl difficult find propos new direct algorithm prove hard result algorithm examin experi realist artifici data secur game zerosum game minimax equilibrium oracl equilibria comput,2,4.1592693,21.617392
Q-intersection Algorithms for Constraint-Based Robust Parameter Estimation,"Clement Carbonnel, Gilles Trombettoni, Philippe Vismara and Gilles Chabert",Search and Constraint Satisfaction (SCS),"intersection graph
computational complexity
parameter estimation
soft numerical constraints
q-intersection",SCS: Constraint Satisfaction,"Given a set of axis-parallel n-dimensional boxes, the q-intersection is defined as the smallest box encompassing all the points that belong to at least q boxes.  Computing the q-intersection is a combinatorial problem that allows us to handle robust parameter estimation with a numerical constraint programming approach. 
The q-intersection can be viewed as a filtering operator for soft constraints that model measurements subject to outliers. This paper highlights the equivalence of this operator with the search of q-cliques in a graph whose boxicity is bounded by the number of variables in the constraint network. We present a computational study of the q-intersection. We also propose a fast incomplete algorithm and a sophisticated exact q-intersection algorithm. First experimental results show that our exact algorithm outperforms the existing one while our heuristic performs an efficient filtering on hard problems.","Q-intersection Algorithms for Constraint-Based Robust Parameter Estimation Given a set of axis-parallel n-dimensional boxes, the q-intersection is defined as the smallest box encompassing all the points that belong to at least q boxes.  Computing the q-intersection is a combinatorial problem that allows us to handle robust parameter estimation with a numerical constraint programming approach. 
The q-intersection can be viewed as a filtering operator for soft constraints that model measurements subject to outliers. This paper highlights the equivalence of this operator with the search of q-cliques in a graph whose boxicity is bounded by the number of variables in the constraint network. We present a computational study of the q-intersection. We also propose a fast incomplete algorithm and a sophisticated exact q-intersection algorithm. First experimental results show that our exact algorithm outperforms the existing one while our heuristic performs an efficient filtering on hard problems. intersection graph
computational complexity
parameter estimation
soft numerical constraints
q-intersection",qintersect algorithm constraintbas robust paramet estim given set axisparallel ndimension box qintersect defin smallest box encompass point belong least q box comput qintersect combinatori problem allow us handl robust paramet estim numer constraint program approach qintersect view filter oper soft constraint model measur subject outlier paper highlight equival oper search qcliqu graph whose boxic bound number variabl constraint network present comput studi qintersect also propos fast incomplet algorithm sophist exact qintersect algorithm first experiment result show exact algorithm outperform exist one heurist perform effici filter hard problem intersect graph comput complex paramet estim soft numer constraint qintersect,7,-3.6280725,4.204894
Fused Feature Representation Discovery for High-dimensional and Sparse Data,Jun Suzuki and Masaaki Nagata,"NLP and Machine Learning (NLPML)
Novel Machine Learning Algorithms (NMLA)","feature representation discovery
feature selection
dimensionality reduction
semi-supervised learning
feature grouping","NLPML: Natural Language Processing (General/Other)
NMLA: Feature Construction/Reformulation
NMLA: Semisupervised Learning","The automatic discovery of a significant low-dimensional feature representation from given data set is a fundamental problem in machine learning. This paper specifically focuses on to develop feature representation discovery methods appropriate for high-dimensional and sparse data, which are remain a frontier, but are now becoming a highly important tool. We formulate our feature representation discovery problem as a variant of semi-supervised learning problem, namely, an optimization problem over unsupervised data whose objective is evaluating the impact of each feature with respect to modeling a target task according to the initial model constructed by using supervised data. The most notable characteristic of our method is that it offers feasible processing speed even if the numbers of data and features both exceed the billions, and successfully provides significantly small feature sets, i.e., less than 10, that can also offer improved performance comparing with those obtained with using the original feature sets. We demonstrate the effectiveness of our method on experiments of two well-known natural language processing tasks.","Fused Feature Representation Discovery for High-dimensional and Sparse Data The automatic discovery of a significant low-dimensional feature representation from given data set is a fundamental problem in machine learning. This paper specifically focuses on to develop feature representation discovery methods appropriate for high-dimensional and sparse data, which are remain a frontier, but are now becoming a highly important tool. We formulate our feature representation discovery problem as a variant of semi-supervised learning problem, namely, an optimization problem over unsupervised data whose objective is evaluating the impact of each feature with respect to modeling a target task according to the initial model constructed by using supervised data. The most notable characteristic of our method is that it offers feasible processing speed even if the numbers of data and features both exceed the billions, and successfully provides significantly small feature sets, i.e., less than 10, that can also offer improved performance comparing with those obtained with using the original feature sets. We demonstrate the effectiveness of our method on experiments of two well-known natural language processing tasks. feature representation discovery
feature selection
dimensionality reduction
semi-supervised learning
feature grouping",fuse featur represent discoveri highdimension spars data automat discoveri signific lowdimension featur represent given data set fundament problem machin learn paper specif focus develop featur represent discoveri method appropri highdimension spars data remain frontier becom high import tool formul featur represent discoveri problem variant semisupervis learn problem name optim problem unsupervis data whose object evalu impact featur respect model target task accord initi model construct use supervis data notabl characterist method offer feasibl process speed even number data featur exceed billion success provid signific small featur set ie less 10 also offer improv perform compar obtain use origin featur set demonstr effect method experi two wellknown natur languag process task featur represent discoveri featur select dimension reduct semisupervis learn featur group,6,-8.940741,-21.080328
Joule Counting Correction for Electric Vehicles using Artificial Neural Nets,Michael Taylor,"Applications (APP)
Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)
Reasoning under Uncertainty (RU)
Robotics (ROB)","Electric Vehicles
Battery Estimation
State of Charge
Artificial Neural Nets
Lithium Iron Phosphate (LiFePo)","APP: Other Applications
MLA: Applications of Unsupervised Learning
MLA: Machine Learning Applications (General/other)
NMLA: Active Learning
NMLA: Time-Series/Data Streams
NMLA: Unsupervised Learning (Other)
NMLA: Machine Learning (General/other)
RU: Uncertainty Representations
ROB: Human-Robot Interaction
ROB: State Estimation","Estimating the remaining energy in high-capacity electric vehicle batteries is essential to safe and efficient operation. Accurate estimation remains a major challenge, however, because battery state cannot be observed directly.  In this paper, we demonstrate a method for estimating battery remaining energy using real data collected from the Charge Car electric vehicle.  This new method relies on energy integration as an initial estimation step, which is then corrected using a neural net that learns how error accumulates from recent charge/discharge cycles.  In this way, the algorithm is able to adapt to nonlinearities and variations that are difficult to model or characterize.  On the collected dataset, this method is demonstrated to be accurate to within 2.5% to 5% of battery remaining energy, which equates to approximately 1 to 2 miles of residual range for the Charge Car given its 10kWh battery pack.","Joule Counting Correction for Electric Vehicles using Artificial Neural Nets Estimating the remaining energy in high-capacity electric vehicle batteries is essential to safe and efficient operation. Accurate estimation remains a major challenge, however, because battery state cannot be observed directly.  In this paper, we demonstrate a method for estimating battery remaining energy using real data collected from the Charge Car electric vehicle.  This new method relies on energy integration as an initial estimation step, which is then corrected using a neural net that learns how error accumulates from recent charge/discharge cycles.  In this way, the algorithm is able to adapt to nonlinearities and variations that are difficult to model or characterize.  On the collected dataset, this method is demonstrated to be accurate to within 2.5% to 5% of battery remaining energy, which equates to approximately 1 to 2 miles of residual range for the Charge Car given its 10kWh battery pack. Electric Vehicles
Battery Estimation
State of Charge
Artificial Neural Nets
Lithium Iron Phosphate (LiFePo)",joul count correct electr vehicl use artifici neural net estim remain energi highcapac electr vehicl batteri essenti safe effici oper accur estim remain major challeng howev batteri state cannot observ direct paper demonstr method estim batteri remain energi use real data collect charg car electr vehicl new method reli energi integr initi estim step correct use neural net learn error accumul recent chargedischarg cycl way algorithm abl adapt nonlinear variat difficult model character collect dataset method demonstr accur within 25 5 batteri remain energi equat approxim 1 2 mile residu rang charg car given 10kwh batteri pack electr vehicl batteri estim state charg artifici neural net lithium iron phosphat lifepo,5,7.402608,10.452942
Managing Change in Graph-structured Data Using Description Logics,"Shqiponja Ahmetaj, Diego Calvanese, Magdalena Ortiz and Mantas Simkus",Knowledge Representation and Reasoning (KRR),"Graph structured data
Description Logics
Static analysis of transactions
Planning","KRR: Computational Complexity of Reasoning
KRR: Description Logics","In this paper we consider the setting of graph-structured data that evolves as a result of operations carried out by users or applications.  We study different reasoning problems, which range from ensuring the satisfaction of a given set of integrity constraints after a given sequence of updates, to deciding the (non-)existence of a sequence of actions that would take the data to an (un)desirable state, starting either from a specific data instance or from an incomplete description of it.  We consider a simple action language in which actions are finite sequences of insertions and deletions of nodes and labels, and use Description Logics for describing integrity constraints and (partial) states of the data.  We then formalize the data management problems mentioned above as a static verification problem and several planning problems.  We provide algorithms and tight complexity bounds for the formalized problems, both for an expressive DL and for a variant of DL-Lite.","Managing Change in Graph-structured Data Using Description Logics In this paper we consider the setting of graph-structured data that evolves as a result of operations carried out by users or applications.  We study different reasoning problems, which range from ensuring the satisfaction of a given set of integrity constraints after a given sequence of updates, to deciding the (non-)existence of a sequence of actions that would take the data to an (un)desirable state, starting either from a specific data instance or from an incomplete description of it.  We consider a simple action language in which actions are finite sequences of insertions and deletions of nodes and labels, and use Description Logics for describing integrity constraints and (partial) states of the data.  We then formalize the data management problems mentioned above as a static verification problem and several planning problems.  We provide algorithms and tight complexity bounds for the formalized problems, both for an expressive DL and for a variant of DL-Lite. Graph structured data
Description Logics
Static analysis of transactions
Planning",manag chang graphstructur data use descript logic paper consid set graphstructur data evolv result oper carri user applic studi differ reason problem rang ensur satisfact given set integr constraint given sequenc updat decid nonexist sequenc action would take data undesir state start either specif data instanc incomplet descript consid simpl action languag action finit sequenc insert delet node label use descript logic describ integr constraint partial state data formal data manag problem mention static verif problem sever plan problem provid algorithm tight complex bound formal problem express dl variant dllite graph structur data descript logic static analysi transact plan,3,-10.303369,-1.715462
BCI Based Control Without Explicit Calibration,"Jonathan Grizou, Iñaki Iturrate, Luis Montesano and Manuel Lopes","Cognitive Modeling (CM)
Humans and AI (HAI)","Brain-Computer Interfaces
Calibration
Human-Robot Interaction","CM: Adaptive Behavior
HAI: Brain-Sensing and Analysis
HAI: Communication Protocols
HAI: Human-Computer Interaction
HAI: User Experience and Usability","Recent work has shown that it is possible to extract feedback information from
EEG measurements of brain activity, such as error potentials, and use it to solve
sequential tasks. As most Brain-Computer Interfaces, a calibration phase is required
to build a decoder that translates raw EEG signals to understandable feedback
signals. This paper proposes a method to solve sequential tasks based on
feedback extracted from the brain without any calibration. The proposed method
uses optimal policies to hallucinate the meaning of the EEG signals and select the
target with the lowest expected error. Also, we use the task and symbol uncertainty
as an exploration bonus for an active strategy to speed up the learning. We
report online experiments where four users directly controlled an agent on a 2D
grid world to reach a target without any previous calibration process.","BCI Based Control Without Explicit Calibration Recent work has shown that it is possible to extract feedback information from
EEG measurements of brain activity, such as error potentials, and use it to solve
sequential tasks. As most Brain-Computer Interfaces, a calibration phase is required
to build a decoder that translates raw EEG signals to understandable feedback
signals. This paper proposes a method to solve sequential tasks based on
feedback extracted from the brain without any calibration. The proposed method
uses optimal policies to hallucinate the meaning of the EEG signals and select the
target with the lowest expected error. Also, we use the task and symbol uncertainty
as an exploration bonus for an active strategy to speed up the learning. We
report online experiments where four users directly controlled an agent on a 2D
grid world to reach a target without any previous calibration process. Brain-Computer Interfaces
Calibration
Human-Robot Interaction",bci base control without explicit calibr recent work shown possibl extract feedback inform eeg measur brain activ error potenti use solv sequenti task braincomput interfac calibr phase requir build decod translat raw eeg signal understand feedback signal paper propos method solv sequenti task base feedback extract brain without calibr propos method use optim polici hallucin mean eeg signal select target lowest expect error also use task symbol uncertainti explor bonus activ strategi speed learn report onlin experi four user direct control agent 2d grid world reach target without previous calibr process braincomput interfac calibr humanrobot interact,3,-2.6732247,-10.40755
Towards Scalable Exploration of Diagnoses in an Ontology Stream,Freddy Lecue,AI and the Web (AIW),"Semantic Web
Ontology Stream
Semantic Reasoning
Knowledge Evolution","AIW: Languages, tools, and methodologies for representing, managing, and visualizing semantic web data
AIW: Ontologies and the web: creation, extraction, evolution, mapping, merging, and alignment; tags and folksonomies","Diagnosis, or the process of identifying the nature and cause of an anomaly in an ontology, has been largely studied by the Semantic Web community. In the context of ontology stream, diagnosis results are not captured by a unique fixed ontology but numerous time-evolving ontologies. Thus any anomaly can be diagnosed by a large number of different explanations depending on the version and evolution of the ontology. We address the problems of identifying, representing, exploiting and exploring the evolution of diagnoses representations. Our approach consists in a graph-based representation, which aims at (i) efficiently organizing and linking time-evolving diagnoses and (ii) being used for scalable exploration. The experiments have shown scalable diagnoses exploration in the context of real and live data from Dublin City.","Towards Scalable Exploration of Diagnoses in an Ontology Stream Diagnosis, or the process of identifying the nature and cause of an anomaly in an ontology, has been largely studied by the Semantic Web community. In the context of ontology stream, diagnosis results are not captured by a unique fixed ontology but numerous time-evolving ontologies. Thus any anomaly can be diagnosed by a large number of different explanations depending on the version and evolution of the ontology. We address the problems of identifying, representing, exploiting and exploring the evolution of diagnoses representations. Our approach consists in a graph-based representation, which aims at (i) efficiently organizing and linking time-evolving diagnoses and (ii) being used for scalable exploration. The experiments have shown scalable diagnoses exploration in the context of real and live data from Dublin City. Semantic Web
Ontology Stream
Semantic Reasoning
Knowledge Evolution",toward scalabl explor diagnos ontolog stream diagnosi process identifi natur caus anomali ontolog larg studi semant web communiti context ontolog stream diagnosi result captur uniqu fix ontolog numer timeevolv ontolog thus anomali diagnos larg number differ explan depend version evolut ontolog address problem identifi repres exploit explor evolut diagnos represent approach consist graphbas represent aim effici organ link timeevolv diagnos ii use scalabl explor experi shown scalabl diagnos explor context real live data dublin citi semant web ontolog stream semant reason knowledg evolut,8,23.700079,7.6675787
Flexible and Scalable Partially Observable Planning with Linear Translations,Blai Bonet and Hector Geffner,"Knowledge Representation and Reasoning (KRR)
Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","Planning with sensing and partial information
Planning with beliefs
On-line planning
Replanning","KRR: Reasoning with Beliefs
PS: Deterministic Planning
PS: Replanning and Plan Repair
PS: Planning (General/Other)
RU: Sequential Decision Making","The problem of on-line planning in partially observable settings involves two problems: keeping track of beliefs about the environment, and selecting actions for achieving goals. While the two problems are computationally intractable in the worst case, significant progress has been achieved in recent years through the use of suitable reductions. In particular, the state-of-the-art CLG planner is based on a translation that maps deterministic partially observable problems into fully observable nondeterministic ones. The translation, which is quadratic in the number of problem fluents and gets rid of the belief tracking problem, is adequate for most benchmarks; it is in fact complete for problems that have width 1. The more recent K-replanner uses two translations that are linear, one for keeping track of beliefs and the other for selecting actions using off-the-shelf classical planners.
As a result, the K-replanner scales up better but is not as general as CLG. In this work, we combine the benefits of these two approaches, the scope of the CLG planner and the efficiency of the K-replanner, by introducing a new planner, called LW1, that is based on a translation that is linear but which is complete for width-1 problems. The scope and performance of the new planner is evaluated by considering the existing benchmarks and new problems.","Flexible and Scalable Partially Observable Planning with Linear Translations The problem of on-line planning in partially observable settings involves two problems: keeping track of beliefs about the environment, and selecting actions for achieving goals. While the two problems are computationally intractable in the worst case, significant progress has been achieved in recent years through the use of suitable reductions. In particular, the state-of-the-art CLG planner is based on a translation that maps deterministic partially observable problems into fully observable nondeterministic ones. The translation, which is quadratic in the number of problem fluents and gets rid of the belief tracking problem, is adequate for most benchmarks; it is in fact complete for problems that have width 1. The more recent K-replanner uses two translations that are linear, one for keeping track of beliefs and the other for selecting actions using off-the-shelf classical planners.
As a result, the K-replanner scales up better but is not as general as CLG. In this work, we combine the benefits of these two approaches, the scope of the CLG planner and the efficiency of the K-replanner, by introducing a new planner, called LW1, that is based on a translation that is linear but which is complete for width-1 problems. The scope and performance of the new planner is evaluated by considering the existing benchmarks and new problems. Planning with sensing and partial information
Planning with beliefs
On-line planning
Replanning",flexibl scalabl partial observ plan linear translat problem onlin plan partial observ set involv two problem keep track belief environ select action achiev goal two problem comput intract worst case signific progress achiev recent year use suitabl reduct particular stateoftheart clg planner base translat map determinist partial observ problem fulli observ nondeterminist one translat quadrat number problem fluent get rid belief track problem adequ benchmark fact complet problem width 1 recent kreplann use two translat linear one keep track belief select action use offtheshelf classic planner result kreplann scale better general clg work combin benefit two approach scope clg planner effici kreplann introduc new planner call lw1 base translat linear complet width1 problem scope perform new planner evalu consid exist benchmark new problem plan sens partial inform plan belief onlin plan replan,5,-4.5652614,17.716774
Online Portfolio Selection with Group Sparsity,"Puja Das, Nicholas Johnson and Arindam Banerjee","Applications (APP)
Game Theory and Economic Paradigms (GTEP)
Machine Learning Applications (MLA)
Novel Machine Learning Algorithms (NMLA)","online learning
portfolio selection
group lasso
non-smooth convex optimization
alternating direction method of multipliers","APP: Other Applications
GTEP: Adversarial Learning
MLA: Machine Learning Applications (General/other)
NMLA: Big Data / Scalability
NMLA: Data Mining and Knowledge Discovery
NMLA: Online Learning
NMLA: Time-Series/Data Streams
NMLA: Machine Learning (General/other)","In portfolio selection, it often might be preferable to focus on a few top
performing industries/sectors to beat the market. These top performing sectors
however might change over time. In this paper, we propose an online portfolio
selection algorithm that can take advantage of sector information through the use of a group sparsity inducing regularizer while making lazy updates to the portfolio. The lazy updates prevent changing ones portfolio too often which otherwise might incur huge transaction costs.
The proposed formulation is not straightforward to solve due to the presence of
non-smooth functions along with the constraint that the portfolios have to lie
within a probability simplex. We propose an efficient primal-dual based alternating direction method of multipliers algorithm and demonstrate its effectiveness for the problem of online portfolio selection
with sector information. We show that our algorithm O-LUGS has sub-linear
regret $w.r.t.$ the best \textit{fixed} and best \textit{shifting} solution in
hindsight. We successfully establish the robustness and scalability of O-LUGS
by performing extensive experiments on two real-world datasets.","Online Portfolio Selection with Group Sparsity In portfolio selection, it often might be preferable to focus on a few top
performing industries/sectors to beat the market. These top performing sectors
however might change over time. In this paper, we propose an online portfolio
selection algorithm that can take advantage of sector information through the use of a group sparsity inducing regularizer while making lazy updates to the portfolio. The lazy updates prevent changing ones portfolio too often which otherwise might incur huge transaction costs.
The proposed formulation is not straightforward to solve due to the presence of
non-smooth functions along with the constraint that the portfolios have to lie
within a probability simplex. We propose an efficient primal-dual based alternating direction method of multipliers algorithm and demonstrate its effectiveness for the problem of online portfolio selection
with sector information. We show that our algorithm O-LUGS has sub-linear
regret $w.r.t.$ the best \textit{fixed} and best \textit{shifting} solution in
hindsight. We successfully establish the robustness and scalability of O-LUGS
by performing extensive experiments on two real-world datasets. online learning
portfolio selection
group lasso
non-smooth convex optimization
alternating direction method of multipliers",onlin portfolio select group sparsiti portfolio select often might prefer focus top perform industriessector beat market top perform sector howev might chang time paper propos onlin portfolio select algorithm take advantag sector inform use group sparsiti induc regular make lazi updat portfolio lazi updat prevent chang one portfolio often otherwis might incur huge transact cost propos formul straightforward solv due presenc nonsmooth function along constraint portfolio lie within probabl simplex propos effici primaldu base altern direct method multipli algorithm demonstr effect problem onlin portfolio select sector inform show algorithm olug sublinear regret wrt best textitfix best textitshift solut hindsight success establish robust scalabl olug perform extens experi two realworld dataset onlin learn portfolio select group lasso nonsmooth convex optim altern direct method multipli,4,12.380091,-0.5959955
Prediction of Helpful Reviews using Emotions Extraction,Lionel Martin and Pearl Pu,"Machine Learning Applications (MLA)
NLP and Machine Learning (NLPML)","helpfulness prediction
product review analysis
emotions extraction","MLA: Applications of Supervised Learning
NLPML: Evaluation and Analysis","Reviews keep playing an increasingly important role in the decision process of buying products and booking hotels. However, the large amount of available information can be confusing to users. A more succinct interface, gathering only the most helpful reviews, can reduce information processing time and save effort. To create such an interface in real time, we need reliable prediction algorithms to classify and predict new reviews which have not been voted but are potentially helpful. So far such helpfulness prediction algorithms have benefited from structural aspects, such as the length and readability score. Since emotional words are at the heart of our written communication and are powerful to trigger listeners’ attention, we believe that emotional words can serve as important parameters for predicting helpfulness of review text. 

Using GALC, a general lexicon of emotional words associated with a model representing 20 different categories, we extracted the emotionality from the review text and applied supervised classification method to derive the emotion-based helpful review prediction. As the second contribution, we propose an evaluation framework comparing three different real-world datasets extracted from the most well-known product review websites. This framework shows that emotion-based methods are outperforming the structure-based approach, by up to 9%.","Prediction of Helpful Reviews using Emotions Extraction Reviews keep playing an increasingly important role in the decision process of buying products and booking hotels. However, the large amount of available information can be confusing to users. A more succinct interface, gathering only the most helpful reviews, can reduce information processing time and save effort. To create such an interface in real time, we need reliable prediction algorithms to classify and predict new reviews which have not been voted but are potentially helpful. So far such helpfulness prediction algorithms have benefited from structural aspects, such as the length and readability score. Since emotional words are at the heart of our written communication and are powerful to trigger listeners’ attention, we believe that emotional words can serve as important parameters for predicting helpfulness of review text. 

Using GALC, a general lexicon of emotional words associated with a model representing 20 different categories, we extracted the emotionality from the review text and applied supervised classification method to derive the emotion-based helpful review prediction. As the second contribution, we propose an evaluation framework comparing three different real-world datasets extracted from the most well-known product review websites. This framework shows that emotion-based methods are outperforming the structure-based approach, by up to 9%. helpfulness prediction
product review analysis
emotions extraction",predict help review use emot extract review keep play increas import role decis process buy product book hotel howev larg amount avail inform confus user succinct interfac gather help review reduc inform process time save effort creat interfac real time need reliabl predict algorithm classifi predict new review vote potenti help far help predict algorithm benefit structur aspect length readabl score sinc emot word heart written communic power trigger listen attent believ emot word serv import paramet predict help review text use galc general lexicon emot word associ model repres 20 differ categori extract emot review text appli supervis classif method deriv emotionbas help review predict second contribut propos evalu framework compar three differ realworld dataset extract wellknown product review websit framework show emotionbas method outperform structurebas approach 9 help predict product review analysi emot extract,6,-7.783789,-2.4944718
An Adversarial Interpretation of Information-Theoretic Bounded Rationality,Pedro A. Ortega and Daniel D. Lee,"Game Theory and Economic Paradigms (GTEP)
Planning and Scheduling (PS)
Reasoning under Uncertainty (RU)","bounded rationality
free energy
game theory
legendre transform","PS: Probabilistic Planning
PS: Planning (General/Other)
RU: Decision/Utility Theory","Recently, there has been a growing interest in modelling planning with information constraints. Accordingly, an agent maximizes a regularized expected utility known as the free energy, where the regularizer is given by the information divergence from a prior to a posterior policy. While this approach can be justified in various ways, most importantly from statistical mechanics and information theory, it is still unclear how it relates to game theory. This connection has been suggested previously in work relating the free energy to risk-sensitive control and to extensive form games. In this work, we present an adversarial interpretation that is equivalent to the free energy optimization problem. The adversary can, by paying an exponential
penalty, generate costs that diminish the decision maker's payoffs. It turns out
that the optimal strategy of the adversary consists in choosing costs so as to
render the decision maker indifferent among its choices, which is a definining
property of a Nash equilibrium, thus tightening the connection between free
energy optimization and game theory.","An Adversarial Interpretation of Information-Theoretic Bounded Rationality Recently, there has been a growing interest in modelling planning with information constraints. Accordingly, an agent maximizes a regularized expected utility known as the free energy, where the regularizer is given by the information divergence from a prior to a posterior policy. While this approach can be justified in various ways, most importantly from statistical mechanics and information theory, it is still unclear how it relates to game theory. This connection has been suggested previously in work relating the free energy to risk-sensitive control and to extensive form games. In this work, we present an adversarial interpretation that is equivalent to the free energy optimization problem. The adversary can, by paying an exponential
penalty, generate costs that diminish the decision maker's payoffs. It turns out
that the optimal strategy of the adversary consists in choosing costs so as to
render the decision maker indifferent among its choices, which is a definining
property of a Nash equilibrium, thus tightening the connection between free
energy optimization and game theory. bounded rationality
free energy
game theory
legendre transform",adversari interpret informationtheoret bound ration recent grow interest model plan inform constraint accord agent maxim regular expect util known free energi regular given inform diverg prior posterior polici approach justifi various way import statist mechan inform theori still unclear relat game theori connect suggest previous work relat free energi risksensit control extens form game work present adversari interpret equival free energi optim problem adversari pay exponenti penalti generat cost diminish decis maker payoff turn optim strategi adversari consist choos cost render decis maker indiffer among choic definin properti nash equilibrium thus tighten connect free energi optim game theori bound ration free energi game theori legendr transform,2,5.94382,17.435823
Learning to Recognize Novel Objects in One Shot through Human-Robot Interactions in Natural Language Dialogues,"Evan Krause, Michael Zillich, Thomas Williams and Matthias Scheutz","Cognitive Systems (CS)
Robotics (ROB)
Vision (VIS)","one-shot learning
object recognition
natural language dialogues","CS: Natural language understanding and dialogue
ROB: Human-Robot Interaction
VIS: Language and Vision
VIS: Object Recognition","Being able to quickly and naturally teach robots new knowledge is critical for many future open-world human-robot interaction scenarios.  In this paper we present a novel approach to using natural language context for one-shot learning of visual objects, where the robot is immediately able to recognize the described object. We describe the architectural components and demonstrate the proposed approach on a robotic platform in a proof-of-concept evaluation.","Learning to Recognize Novel Objects in One Shot through Human-Robot Interactions in Natural Language Dialogues Being able to quickly and naturally teach robots new knowledge is critical for many future open-world human-robot interaction scenarios.  In this paper we present a novel approach to using natural language context for one-shot learning of visual objects, where the robot is immediately able to recognize the described object. We describe the architectural components and demonstrate the proposed approach on a robotic platform in a proof-of-concept evaluation. one-shot learning
object recognition
natural language dialogues",learn recogn novel object one shot humanrobot interact natur languag dialogu abl quick natur teach robot new knowledg critic mani futur openworld humanrobot interact scenario paper present novel approach use natur languag context oneshot learn visual object robot immedi abl recogn describ object describ architectur compon demonstr propos approach robot platform proofofconcept evalu oneshot learn object recognit natur languag dialogu,1,-1.8374004,9.439518
Coactive Learning for Locally Optimal Problem Solving,"Robby Goetschalckx, Alan Fern and Prasad Tadepalli","Humans and AI (HAI)
Knowledge Representation and Reasoning (KRR)","Coactive Learning
Local Optimization
Preference Learning","HCC: Active learning from imperfect human labelers
HAI: Interaction Techniques and Devices
KRR: Preferences
NMLA: Preferences/Ranking Learning","Coactive learning is an online problem solving setting where the solutions
provided by a solver are interactively improved by a domain expert, which
in turn drives learning. 
In this paper we extend the
study of coactive learning to problems where obtaining a
global or near-optimal solution may be intractable or where an expert
can only be expected to make small, local improvements to a candidate solution.
The goal of learning in this new setting is to minimize the cost
as measured by the expert effort
over time. We first establish theoretical bounds
on the average cost of the existing coactive Perceptron
algorithm. In addition, we consider new online algorithms that use
cost-sensitive and Passive-Aggressive (PA) updates, showing similar
or improved theoretical bounds. We provide an empirical evaluation
of the learners in 5 domains, which show that the Perceptron based
algorithms are quite effective and that unlike the case for online
classification, the PA algorithms do not yield significant performance
gains.","Coactive Learning for Locally Optimal Problem Solving Coactive learning is an online problem solving setting where the solutions
provided by a solver are interactively improved by a domain expert, which
in turn drives learning. 
In this paper we extend the
study of coactive learning to problems where obtaining a
global or near-optimal solution may be intractable or where an expert
can only be expected to make small, local improvements to a candidate solution.
The goal of learning in this new setting is to minimize the cost
as measured by the expert effort
over time. We first establish theoretical bounds
on the average cost of the existing coactive Perceptron
algorithm. In addition, we consider new online algorithms that use
cost-sensitive and Passive-Aggressive (PA) updates, showing similar
or improved theoretical bounds. We provide an empirical evaluation
of the learners in 5 domains, which show that the Perceptron based
algorithms are quite effective and that unlike the case for online
classification, the PA algorithms do not yield significant performance
gains. Coactive Learning
Local Optimization
Preference Learning",coactiv learn local optim problem solv coactiv learn onlin problem solv set solut provid solver interact improv domain expert turn drive learn paper extend studi coactiv learn problem obtain global nearoptim solut may intract expert expect make small local improv candid solut goal learn new set minim cost measur expert effort time first establish theoret bound averag cost exist coactiv perceptron algorithm addit consid new onlin algorithm use costsensit passiveaggress pa updat show similar improv theoret bound provid empir evalu learner 5 domain show perceptron base algorithm quit effect unlik case onlin classif pa algorithm yield signific perform gain coactiv learn local optim prefer learn,4,1.0577201,-12.505478
Large Scale Analogical Reasoning,"Vinay Chaudhri, Stijn Heymans, Adam Overholtzer, Aaron Spaulding and Michael Wessel","Cognitive Systems (CS)
Knowledge Representation and Reasoning (KRR)","analogical reasoning
case-based reasoning
question answering
knowledge base systems","CS: Conceptual inference and reasoning
CS: Structural learning and knowledge capture
KRR: Qualitative Reasoning","It has been argued that one can use cognitive simulation of analogical
processing to answer comparison questions.  In the context of a
knowledge base (KB) system, a comparison question takes the form: What
are the similarities and/or differences between A and B?, where
\concept{A} and \concept{B} are concepts in the KB.  Previous attempts
to use a general purpose analogical reasoner to answer this question
revealed three major problems: (a) the system presented too much
information in the answer and the salient similarity or difference was
not highlighted (b) analogical inference found some incorrect
differences (c) some expected similarities were not found. The primary
cause of these problems was the lack of availability of a well-curated
KB, and secondarily, there were also some algorithmic deficiencies.
In this paper, we present an of comparison questions that is inspired
by the general model of analogical reasoning, but is specific to the
questions at hand. We also rely on a well-curated biology KB.  We
present numerous examples of answers produced by the system and
empirical data on the quality of the answers to claim that we have
addressed many of the problems faced in the previous system.","Large Scale Analogical Reasoning It has been argued that one can use cognitive simulation of analogical
processing to answer comparison questions.  In the context of a
knowledge base (KB) system, a comparison question takes the form: What
are the similarities and/or differences between A and B?, where
\concept{A} and \concept{B} are concepts in the KB.  Previous attempts
to use a general purpose analogical reasoner to answer this question
revealed three major problems: (a) the system presented too much
information in the answer and the salient similarity or difference was
not highlighted (b) analogical inference found some incorrect
differences (c) some expected similarities were not found. The primary
cause of these problems was the lack of availability of a well-curated
KB, and secondarily, there were also some algorithmic deficiencies.
In this paper, we present an of comparison questions that is inspired
by the general model of analogical reasoning, but is specific to the
questions at hand. We also rely on a well-curated biology KB.  We
present numerous examples of answers produced by the system and
empirical data on the quality of the answers to claim that we have
addressed many of the problems faced in the previous system. analogical reasoning
case-based reasoning
question answering
knowledge base systems",larg scale analog reason argu one use cognit simul analog process answer comparison question context knowledg base kb system comparison question take form similar andor differ b concepta conceptb concept kb previous attempt use general purpos analog reason answer question reveal three major problem system present much inform answer salient similar differ highlight b analog infer found incorrect differ c expect similar found primari caus problem lack avail wellcur kb secondarili also algorithm defici paper present comparison question inspir general model analog reason specif question hand also reli wellcur biolog kb present numer exampl answer produc system empir data qualiti answer claim address mani problem face previous system analog reason casebas reason question answer knowledg base system,8,6.1609783,-3.0925426
Learning Models of Unknown Events,Matthew Molineaux and David Aha,Cognitive Systems (CS),"learning environment models
explanation generation
execution monitoring
goal-driven autonomy","CM: Symbolic AI
CS: Problem solving and decision making
CS: Introspection and meta-cognition
CS: Structural learning and knowledge capture
PS: Learning Models for Planning and Diagnosis
PS: Plan Execution and Monitoring","Agents with incomplete models of their environment are likely to be surprised. For agents in immense environments that defy complete modeling, this represents an opportunity to learn. We investigate approaches for situated agents to detect surprises, discriminate among different forms of surprise, and hypothesize new models for the unknown events that surprised them. We instantiate these approaches in a new goal reasoning agent (named FOOLMETWICE), investigate its performance in simulation studies, and show that it produces plans with significantly reduced execution cost when compared to not learning models for surprising events.","Learning Models of Unknown Events Agents with incomplete models of their environment are likely to be surprised. For agents in immense environments that defy complete modeling, this represents an opportunity to learn. We investigate approaches for situated agents to detect surprises, discriminate among different forms of surprise, and hypothesize new models for the unknown events that surprised them. We instantiate these approaches in a new goal reasoning agent (named FOOLMETWICE), investigate its performance in simulation studies, and show that it produces plans with significantly reduced execution cost when compared to not learning models for surprising events. learning environment models
explanation generation
execution monitoring
goal-driven autonomy",learn model unknown event agent incomplet model environ like surpris agent immens environ defi complet model repres opportun learn investig approach situat agent detect surpris discrimin among differ form surpris hypothes new model unknown event surpris instanti approach new goal reason agent name foolmetwic investig perform simul studi show produc plan signific reduc execut cost compar learn model surpris event learn environ model explan generat execut monitor goaldriven autonomi,0,4.0679283,11.474822
Learning Concept Embeddings for Query Expansion by Quantum Entropy Minimization,"Alessandro Sordoni, Yoshua Bengio and Jian-Yun Nie","NLP and Knowledge Representation (NLPKR)
NLP and Machine Learning (NLPML)","Embedding
Density Matrix
Query Expansion","NLPKR: Natural Language Processing (General/Other)
NLPML: Natural Language Processing (General/Other)
NMLA: Neural Networks/Deep Learning","In web search, users queries are formulated using only few terms and term-matching retrieval functions could fail at retrieving relevant documents. Given a user query, the technique of query expansion (QE) consists in selecting related terms that could enhance the likelihood of retrieving relevant documents. Selecting such expansion terms is challenging and requires a computational framework capable of encoding complex semantic relationships. In this paper, we propose a novel method for learning, in a supervised way, semantic representations for words and phrases. By embedding queries and documents in special matrices, our model disposes of an increased representational power with respect to existing approaches adopting a vector representation. We show that our model produces high-quality query expansion terms. Our expansion increase IR mesures beyond expansion from current word-embeddings models and well-established traditional QE methods.","Learning Concept Embeddings for Query Expansion by Quantum Entropy Minimization In web search, users queries are formulated using only few terms and term-matching retrieval functions could fail at retrieving relevant documents. Given a user query, the technique of query expansion (QE) consists in selecting related terms that could enhance the likelihood of retrieving relevant documents. Selecting such expansion terms is challenging and requires a computational framework capable of encoding complex semantic relationships. In this paper, we propose a novel method for learning, in a supervised way, semantic representations for words and phrases. By embedding queries and documents in special matrices, our model disposes of an increased representational power with respect to existing approaches adopting a vector representation. We show that our model produces high-quality query expansion terms. Our expansion increase IR mesures beyond expansion from current word-embeddings models and well-established traditional QE methods. Embedding
Density Matrix
Query Expansion",learn concept embed queri expans quantum entropi minim web search user queri formul use term termmatch retriev function could fail retriev relev document given user queri techniqu queri expans qe consist select relat term could enhanc likelihood retriev relev document select expans term challeng requir comput framework capabl encod complex semant relationship paper propos novel method learn supervis way semant represent word phrase embed queri document special matric model dispos increas represent power respect exist approach adopt vector represent show model produc highqual queri expans term expans increas ir mesur beyond expans current wordembed model wellestablish tradit qe method embed densiti matrix queri expans,8,24.04144,1.2998712
Non-Restarting SAT Solvers With Simple Preprocessing Efficiently Simulate Resolution,Paul Beame and Ashish Sabharwal,Search and Constraint Satisfaction (SCS),"clause learning
satisfiability
proof complexity
p-simulation
1-UIP clauses
asserting clauses
resolution","SCS: Constraint Satisfaction
SCS: Constraint Learning and Acquisition
SCS: SAT and CSP: Evaluation and Analysis
SCS: Satisfiability (General/Other)","Propositional satisfiability (SAT) solvers based on conflict directed
clause learning (CDCL) implicitly produce resolution refutations of
unsatisfiable formulas. The precise class of formulas for which they
can produce polynomial size refutations has been the subject of
several studies, with special focus on the clause learning aspect of
these solvers. The results, however, either assume the use of
non-standard and non-asserting learning schemes such as FirstNewCut,
or rely on polynomially many restarts for simulating individual steps
of a resolution refutation, or work with a theoretical model that
significantly deviates from certain key aspects of all modern CDCL
solvers such as learning only one asserting clause from each conflict
and other techniques such as conflict guided backjumping and clause
minimization. We study non-restarting CDCL solvers that learn only one
asserting clause per conflict and show that, with simple preprocessing
that depends only on the number of variables of the input formula,
such solvers can polynomially simulate resolution.","Non-Restarting SAT Solvers With Simple Preprocessing Efficiently Simulate Resolution Propositional satisfiability (SAT) solvers based on conflict directed
clause learning (CDCL) implicitly produce resolution refutations of
unsatisfiable formulas. The precise class of formulas for which they
can produce polynomial size refutations has been the subject of
several studies, with special focus on the clause learning aspect of
these solvers. The results, however, either assume the use of
non-standard and non-asserting learning schemes such as FirstNewCut,
or rely on polynomially many restarts for simulating individual steps
of a resolution refutation, or work with a theoretical model that
significantly deviates from certain key aspects of all modern CDCL
solvers such as learning only one asserting clause from each conflict
and other techniques such as conflict guided backjumping and clause
minimization. We study non-restarting CDCL solvers that learn only one
asserting clause per conflict and show that, with simple preprocessing
that depends only on the number of variables of the input formula,
such solvers can polynomially simulate resolution. clause learning
satisfiability
proof complexity
p-simulation
1-UIP clauses
asserting clauses
resolution",nonrestart sat solver simpl preprocess effici simul resolut proposit satisfi sat solver base conflict direct claus learn cdcl implicit produc resolut refut unsatisfi formula precis class formula produc polynomi size refut subject sever studi special focus claus learn aspect solver result howev either assum use nonstandard nonassert learn scheme firstnewcut reli polynomi mani restart simul individu step resolut refut work theoret model signific deviat certain key aspect modern cdcl solver learn one assert claus conflict techniqu conflict guid backjump claus minim studi nonrestart cdcl solver learn one assert claus per conflict show simpl preprocess depend number variabl input formula solver polynomi simul resolut claus learn satisfi proof complex psimul 1uip claus assert claus resolut,4,-11.970766,17.472887
Worst-Case Solution Quality Analysis When Not Re-Expanding Nodes in Best-First Search,"Richard Valenzano, Nathan Sturtevant and Jonathan Schaeffer",Heuristic Search and Optimization (HSO),"best-first search
re-expansions
heuristics
inconsistency
inadmissibility
solution quality
suboptimality
suboptimal heuristic search
worst-case analysis","HSO: Heuristic Search
HSO: Evaluation and Analysis (Search and Optimization)","The use of inconsistent heuristics with A* can result in increased runtime due to the need to re-expand nodes. Poor performance can also be seen with Weighted A* if nodes are re-expanded. While the negative impact of re-expansions can often be minimized by setting these algorithms to never expand nodes more than once, the result can be a lower solution quality. In this paper, we formally show that the loss in solution quality can be bounded based on the amount of inconsistency along optimal solution paths. This bound holds regardless of whether the heuristic is admissible or inadmissible, though if the heuristic is admissible the bound can be used to show that not re-expanding nodes can have at most a quadratic impact on the quality of solutions found when using A*. We then show that the bound is tight by describing a process for the construction of graphs for which a best-first search that does not re-expand nodes will find solutions whose quality is arbitrarily close to that given by the bound. Finally, we will use the bound to extend a known result regarding the solution quality of WA* when weighting a consistent heuristic, so that it applies to other types of heuristic weighting.","Worst-Case Solution Quality Analysis When Not Re-Expanding Nodes in Best-First Search The use of inconsistent heuristics with A* can result in increased runtime due to the need to re-expand nodes. Poor performance can also be seen with Weighted A* if nodes are re-expanded. While the negative impact of re-expansions can often be minimized by setting these algorithms to never expand nodes more than once, the result can be a lower solution quality. In this paper, we formally show that the loss in solution quality can be bounded based on the amount of inconsistency along optimal solution paths. This bound holds regardless of whether the heuristic is admissible or inadmissible, though if the heuristic is admissible the bound can be used to show that not re-expanding nodes can have at most a quadratic impact on the quality of solutions found when using A*. We then show that the bound is tight by describing a process for the construction of graphs for which a best-first search that does not re-expand nodes will find solutions whose quality is arbitrarily close to that given by the bound. Finally, we will use the bound to extend a known result regarding the solution quality of WA* when weighting a consistent heuristic, so that it applies to other types of heuristic weighting. best-first search
re-expansions
heuristics
inconsistency
inadmissibility
solution quality
suboptimality
suboptimal heuristic search
worst-case analysis",worstcas solut qualiti analysi reexpand node bestfirst search use inconsist heurist result increas runtim due need reexpand node poor perform also seen weight node reexpand negat impact reexpans often minim set algorithm never expand node result lower solut qualiti paper formal show loss solut qualiti bound base amount inconsist along optim solut path bound hold regardless whether heurist admiss inadmiss though heurist admiss bound use show reexpand node quadrat impact qualiti solut found use show bound tight describ process construct graph bestfirst search reexpand node find solut whose qualiti arbitrarili close given bound final use bound extend known result regard solut qualiti wa weight consist heurist appli type heurist weight bestfirst search reexpans heurist inconsist inadmiss solut qualiti suboptim suboptim heurist search worstcas analysi,5,-13.934011,9.580266
Natural Temporal Difference Learning,William Dabney and Philip Thomas,Novel Machine Learning Algorithms (NMLA),"natural gradient
temporal difference learning
reinforcement learning",,"In this paper we investigate the application of natural gradient descent to Bellman error based reinforcement learning algorithms. This combination is interesting because natural gradient descent is invariant to the parameterization of the value function. This invariance property means that natural gradient descent adapts its update directions to correct for poorly conditioned representations. We present and analyze quadratic and linear time natural temporal difference learning algorithms, and prove that they are covariant. We conclude with experiments which suggest that the natural algorithms can match or outperform their non-natural counterparts using linear function approximation, and drastically improve upon their non-natural counterparts when using non-linear function approximation.","Natural Temporal Difference Learning In this paper we investigate the application of natural gradient descent to Bellman error based reinforcement learning algorithms. This combination is interesting because natural gradient descent is invariant to the parameterization of the value function. This invariance property means that natural gradient descent adapts its update directions to correct for poorly conditioned representations. We present and analyze quadratic and linear time natural temporal difference learning algorithms, and prove that they are covariant. We conclude with experiments which suggest that the natural algorithms can match or outperform their non-natural counterparts using linear function approximation, and drastically improve upon their non-natural counterparts when using non-linear function approximation. natural gradient
temporal difference learning
reinforcement learning",natur tempor differ learn paper investig applic natur gradient descent bellman error base reinforc learn algorithm combin interest natur gradient descent invari parameter valu function invari properti mean natur gradient descent adapt updat direct correct poor condit represent present analyz quadrat linear time natur tempor differ learn algorithm prove covari conclud experi suggest natur algorithm match outperform nonnatur counterpart use linear function approxim drastic improv upon nonnatur counterpart use nonlinear function approxim natur gradient tempor differ learn reinforc learn,4,0.79494154,-7.225031
Relaxation Search: a Simple Way of Managing Optional Clauses,"Maria Tsimpoukelli, Fahiem Bacchus, Jessica Davies and George Katsirelos",Search and Constraint Satisfaction (SCS),"Constraint Optimization
Satisfiability
Maximum Satisfiability
Minimal Correction Sets","SCS: Constraint Optimization
SCS: SAT and CSP: Solvers and Tools
SCS: Satisfiability (General/Other)","A number of problems involve managing a set of optional clauses. For example, the soft clauses in a MaxSat formula are optional---they can be falsified for a cost. Similarly when computing a Minimum Correction Set for an unsatisfiable formula all clauses are optional---some can be falsified in order to make the
  remaining satisfiable. In both of these cases the task is to find a subset of the optional clauses that achieves some optimization criteria and is satisfiable. Relaxation search is a simple method of using a standard SAT solver to solve this task. Relaxation search is very easy to implement, sometimes requiring only a simple   modification of the variable selection heuristic in the SAT solver. Furthermore, considerable flexibility and control can be achieved over the order in which subsets of optional clauses examined. We demonstrate how relaxation search can be used to solve MaxSat and to compute Minimum Correction Sets. In both cases  relaxation search is able to achieve state-of-the-art performance and solve some instances other solvers are not able to solve.","Relaxation Search: a Simple Way of Managing Optional Clauses A number of problems involve managing a set of optional clauses. For example, the soft clauses in a MaxSat formula are optional---they can be falsified for a cost. Similarly when computing a Minimum Correction Set for an unsatisfiable formula all clauses are optional---some can be falsified in order to make the
  remaining satisfiable. In both of these cases the task is to find a subset of the optional clauses that achieves some optimization criteria and is satisfiable. Relaxation search is a simple method of using a standard SAT solver to solve this task. Relaxation search is very easy to implement, sometimes requiring only a simple   modification of the variable selection heuristic in the SAT solver. Furthermore, considerable flexibility and control can be achieved over the order in which subsets of optional clauses examined. We demonstrate how relaxation search can be used to solve MaxSat and to compute Minimum Correction Sets. In both cases  relaxation search is able to achieve state-of-the-art performance and solve some instances other solvers are not able to solve. Constraint Optimization
Satisfiability
Maximum Satisfiability
Minimal Correction Sets",relax search simpl way manag option claus number problem involv manag set option claus exampl soft claus maxsat formula optionalthey falsifi cost similar comput minimum correct set unsatisfi formula claus optionalsom falsifi order make remain satisfi case task find subset option claus achiev optim criteria satisfi relax search simpl method use standard sat solver solv task relax search easi implement sometim requir simpl modif variabl select heurist sat solver furthermor consider flexibl control achiev order subset option claus examin demonstr relax search use solv maxsat comput minimum correct set case relax search abl achiev stateoftheart perform solv instanc solver abl solv constraint optim satisfi maximum satisfi minim correct set,3,-11.697466,16.958565
Using Response Functions to Measure Strategy Strength,"Trevor Davis, Neil Burch and Michael Bowling",Game Theory and Economic Paradigms (GTEP),"strategy evaluation
extensive-form games
adaptive opponents","GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information","Extensive-form games are a powerful tool for representing complex multi-agent interactions. Nash equilibrium strategies are commonly used as a solution concept for extensive-form games, but many games are too large for the computation of Nash equilibria to be tractable. In these large games, exploitability has traditionally been used to measure deviation from Nash equilibrium, and thus strategies are aimed to achieve minimal exploitability. However, while exploitability measures a strategy's worst-case performance, it fails to capture how likely that worst-case is to be observed in practice. In fact, empirical evidence has shown that a less exploitable strategy can perform worse than a more exploitable strategy in one-on-one play against a variety of opponents. In this work, we propose a class of response functions that can be used to measure the strength of a strategy. We prove that standard no-regret algorithms can be used to learn optimal strategies for a scenario where the opponent uses one of these response functions. We demonstrate the effectiveness of this technique in Leduc poker against opponents that use the UCT Monte Carlo tree search algorithm.","Using Response Functions to Measure Strategy Strength Extensive-form games are a powerful tool for representing complex multi-agent interactions. Nash equilibrium strategies are commonly used as a solution concept for extensive-form games, but many games are too large for the computation of Nash equilibria to be tractable. In these large games, exploitability has traditionally been used to measure deviation from Nash equilibrium, and thus strategies are aimed to achieve minimal exploitability. However, while exploitability measures a strategy's worst-case performance, it fails to capture how likely that worst-case is to be observed in practice. In fact, empirical evidence has shown that a less exploitable strategy can perform worse than a more exploitable strategy in one-on-one play against a variety of opponents. In this work, we propose a class of response functions that can be used to measure the strength of a strategy. We prove that standard no-regret algorithms can be used to learn optimal strategies for a scenario where the opponent uses one of these response functions. We demonstrate the effectiveness of this technique in Leduc poker against opponents that use the UCT Monte Carlo tree search algorithm. strategy evaluation
extensive-form games
adaptive opponents",use respons function measur strategi strength extensiveform game power tool repres complex multiag interact nash equilibrium strategi common use solut concept extensiveform game mani game larg comput nash equilibria tractabl larg game exploit tradit use measur deviat nash equilibrium thus strategi aim achiev minim exploit howev exploit measur strategi worstcas perform fail captur like worstcas observ practic fact empir evid shown less exploit strategi perform wors exploit strategi oneonon play varieti oppon work propos class respons function use measur strength strategi prove standard noregret algorithm use learn optim strategi scenario oppon use one respons function demonstr effect techniqu leduc poker oppon use uct mont carlo tree search algorithm strategi evalu extensiveform game adapt oppon,2,5.6255345,19.447662
Optimal and Efficient Stochastic Motion Planning in Partially-Known Environments,"Ryan Luna, Morteza Lahijanian, Mark Moll and Lydia Kavraki","Reasoning under Uncertainty (RU)
Robotics (ROB)","Planning under uncertainty
Motion planning with action and environment uncertainty
Optimal stochastic motion planning
Computing policies under uncertainty","PS: Mixed Discrete/Continuous Planning
RU: Uncertainty in AI (General/Other)
ROB: Motion and Path Planning
ROB: Robotics (General/Other)","A framework capable of computing optimal control policies for a continuous system in the presence of both action and environment uncertainty is presented in this work.  The framework decomposes the planning problem into two stages: an offline phase that reasons only over action uncertainty and an online phase that quickly reacts to the uncertain environment.  Offline, a bounded-parameter Markov decision process (BMDP) is employed to model the evolution of the stochastic system over a discretization of the environment.  Online, an optimal control policy over the BMDP is computed.  Upon the discovery of an unknown environment feature during policy execution, the BMDP is updated and the optimal control policy is efficiently recomputed.  Depending on the desired quality of the control policy, a suite of methods is presented to incorporate new information into the BMDP with varying degrees of detail online.  Experiments confirm that the framework recomputes high-quality policies in seconds and is orders of magnitude faster than existing methods.","Optimal and Efficient Stochastic Motion Planning in Partially-Known Environments A framework capable of computing optimal control policies for a continuous system in the presence of both action and environment uncertainty is presented in this work.  The framework decomposes the planning problem into two stages: an offline phase that reasons only over action uncertainty and an online phase that quickly reacts to the uncertain environment.  Offline, a bounded-parameter Markov decision process (BMDP) is employed to model the evolution of the stochastic system over a discretization of the environment.  Online, an optimal control policy over the BMDP is computed.  Upon the discovery of an unknown environment feature during policy execution, the BMDP is updated and the optimal control policy is efficiently recomputed.  Depending on the desired quality of the control policy, a suite of methods is presented to incorporate new information into the BMDP with varying degrees of detail online.  Experiments confirm that the framework recomputes high-quality policies in seconds and is orders of magnitude faster than existing methods. Planning under uncertainty
Motion planning with action and environment uncertainty
Optimal stochastic motion planning
Computing policies under uncertainty",optim effici stochast motion plan partiallyknown environ framework capabl comput optim control polici continu system presenc action environ uncertainti present work framework decompos plan problem two stage offlin phase reason action uncertainti onlin phase quick react uncertain environ offlin boundedparamet markov decis process bmdp employ model evolut stochast system discret environ onlin optim control polici bmdp comput upon discoveri unknown environ featur polici execut bmdp updat optim control polici effici recomput depend desir qualiti control polici suit method present incorpor new inform bmdp vari degre detail onlin experi confirm framework recomput highqual polici second order magnitud faster exist method plan uncertainti motion plan action environ uncertainti optim stochast motion plan comput polici uncertainti,3,-2.739104,-8.067868
Learning Scripts as Hidden Markov Models,"Walker Orr, Prasad Tadepalli, Thomas Dietterich, Xiaoli Fern and Janardhan Rao Doppa","NLP and Machine Learning (NLPML)
Novel Machine Learning Algorithms (NMLA)","HMM
Scripts
NLP
Structural EM
Structure Learning","NLPML: Natural Language Processing (General/Other)
NMLA: Graphical Model Learning","Scripts have been proposed to model the stereotypical event sequences found in narratives. They can be applied to make a variety of inferences including filling gaps in the narratives and resolving ambiguous references. This paper proposes the first formal framework for scripts based on Hidden Markov Models (HMMs). Our framework supports robust inference and learning algorithms, which are lacking in previous clustering models. We develop an algorithm for structure and parameter learning based on Expectation Maximization and evaluate it on a number of natural and synthetic datasets. The results show that our algorithm is superior to several informed baselines for predicting future events given some past history.","Learning Scripts as Hidden Markov Models Scripts have been proposed to model the stereotypical event sequences found in narratives. They can be applied to make a variety of inferences including filling gaps in the narratives and resolving ambiguous references. This paper proposes the first formal framework for scripts based on Hidden Markov Models (HMMs). Our framework supports robust inference and learning algorithms, which are lacking in previous clustering models. We develop an algorithm for structure and parameter learning based on Expectation Maximization and evaluate it on a number of natural and synthetic datasets. The results show that our algorithm is superior to several informed baselines for predicting future events given some past history. HMM
Scripts
NLP
Structural EM
Structure Learning",learn script hidden markov model script propos model stereotyp event sequenc found narrat appli make varieti infer includ fill gap narrat resolv ambigu refer paper propos first formal framework script base hidden markov model hmms framework support robust infer learn algorithm lack previous cluster model develop algorithm structur paramet learn base expect maxim evalu number natur synthet dataset result show algorithm superior sever inform baselin predict futur event given past histori hmm script nlp structur em structur learn,7,3.2154667,-2.8623881
Mapping Users Across Networks by Manifold Alignment on Hypergraph,"Shulong Tan, Ziyu Guan, Deng Cai, Xuzhen Qin, Jiajun Bu and Chun Chen",AI and the Web (AIW),"Social Networks
Manifold Alignment
Hypergraph
De-anonymization
User Mapping","AIW: Machine learning and the web
AIW: Ontologies and the web: creation, extraction, evolution, mapping, merging, and alignment; tags and folksonomies
AIW: Social networking and community identification","Nowadays many people are members of multiple online social networks simultaneously, such as Facebook, Twitter and some other instant messaging circles. But these networks are usually isolated from each other. Mapping common users cross these social networks will be beneficial for cross network recommendation or expanding one’s social circle. Methods based on username comparison perform well on parts of users, however they can not work in the following situations: (a) users choose completely different usernames in different networks; (b) a unique username corresponds to different individuals. In this paper, we propose to utilize social structures to improve the mapping performance. Specifically, a novel subspace learning algorithm, Manifold Alignment on Hypergraph (MAH), is proposed. Different from traditional semi-supervised manifold alignment methods, we use hypergraph to model high-order relations here. For a target user in one network, the proposed algorithm ranks all users in the other network by their probabilities of being the corresponding user. Moreover, methods based on username comparison can be incorporated with our algorithm easily to further boost the mapping accuracy. In experiments, we use both simulation data and real world data to test the proposed method. Experiment results have demonstrated the effectiveness of our proposed algorithm in mapping users cross networks.","Mapping Users Across Networks by Manifold Alignment on Hypergraph Nowadays many people are members of multiple online social networks simultaneously, such as Facebook, Twitter and some other instant messaging circles. But these networks are usually isolated from each other. Mapping common users cross these social networks will be beneficial for cross network recommendation or expanding one’s social circle. Methods based on username comparison perform well on parts of users, however they can not work in the following situations: (a) users choose completely different usernames in different networks; (b) a unique username corresponds to different individuals. In this paper, we propose to utilize social structures to improve the mapping performance. Specifically, a novel subspace learning algorithm, Manifold Alignment on Hypergraph (MAH), is proposed. Different from traditional semi-supervised manifold alignment methods, we use hypergraph to model high-order relations here. For a target user in one network, the proposed algorithm ranks all users in the other network by their probabilities of being the corresponding user. Moreover, methods based on username comparison can be incorporated with our algorithm easily to further boost the mapping accuracy. In experiments, we use both simulation data and real world data to test the proposed method. Experiment results have demonstrated the effectiveness of our proposed algorithm in mapping users cross networks. Social Networks
Manifold Alignment
Hypergraph
De-anonymization
User Mapping",map user across network manifold align hypergraph nowaday mani peopl member multipl onlin social network simultan facebook twitter instant messag circl network usual isol map common user cross social network benefici cross network recommend expand one social circl method base usernam comparison perform well part user howev work follow situat user choos complet differ usernam differ network b uniqu usernam correspond differ individu paper propos util social structur improv map perform specif novel subspac learn algorithm manifold align hypergraph mah propos differ tradit semisupervis manifold align method use hypergraph model highord relat target user one network propos algorithm rank user network probabl correspond user moreov method base usernam comparison incorpor algorithm easili boost map accuraci experi use simul data real world data test propos method experi result demonstr effect propos algorithm map user cross network social network manifold align hypergraph deanonym user map,0,13.603589,4.9686832
Compact Aspect Embedding For Diversified Query Expansion,"Xiaohua Liu, Arbi Bouchoucha, Jian-Yun Nie and Alessandro Sordoni",AI and the Web (AIW),"query expansion
search result diversification
Trace Norm Regularization",AIW: Enhancing web search and information retrieval,"Diversified query expansion (DQE) based approaches aim to select a set of expansion terms with less redundancy among them while covering  as many query aspects as possible. Recently they have experimentally demonstrate their effectiveness for the task of search result diversification. One challenge faced by existing DQE approaches is how to  ensure the aspect coverage. In this paper, we propose a novel method for DQE, called compact aspect embedding, which exploits trace norm regularization to  learn a  low rank vector space  for the query, with each eigenvector of the learnt vector space representing an aspect, and the absolute value of its corresponding eigenvalue representing the association strength of that aspect to the query.  Meanwhile, each expansion term is mapped into the vector space as well. Based on this novel representation of the query aspects and expansion terms, we design a greedy selection strategy to choose a set of expansion terms to explicitly cover all possible aspects of the query. We test our method  on several TREC diversification data sets, and show our method significantly outperforms the state-of-the-art approaches.","Compact Aspect Embedding For Diversified Query Expansion Diversified query expansion (DQE) based approaches aim to select a set of expansion terms with less redundancy among them while covering  as many query aspects as possible. Recently they have experimentally demonstrate their effectiveness for the task of search result diversification. One challenge faced by existing DQE approaches is how to  ensure the aspect coverage. In this paper, we propose a novel method for DQE, called compact aspect embedding, which exploits trace norm regularization to  learn a  low rank vector space  for the query, with each eigenvector of the learnt vector space representing an aspect, and the absolute value of its corresponding eigenvalue representing the association strength of that aspect to the query.  Meanwhile, each expansion term is mapped into the vector space as well. Based on this novel representation of the query aspects and expansion terms, we design a greedy selection strategy to choose a set of expansion terms to explicitly cover all possible aspects of the query. We test our method  on several TREC diversification data sets, and show our method significantly outperforms the state-of-the-art approaches. query expansion
search result diversification
Trace Norm Regularization",compact aspect embed diversifi queri expans diversifi queri expans dqe base approach aim select set expans term less redund among cover mani queri aspect possibl recent experiment demonstr effect task search result diversif one challeng face exist dqe approach ensur aspect coverag paper propos novel method dqe call compact aspect embed exploit trace norm regular learn low rank vector space queri eigenvector learnt vector space repres aspect absolut valu correspond eigenvalu repres associ strength aspect queri meanwhil expans term map vector space well base novel represent queri aspect expans term design greedi select strategi choos set expans term explicit cover possibl aspect queri test method sever trec diversif data set show method signific outperform stateoftheart approach queri expans search result diversif trace norm regular,8,24.037416,1.3055658
Contraction and Revision over DL-Lite TBoxes,"Zhiqiang Zhuang, Zhe Wang, Kewen Wang and Guilin Qi",Knowledge Representation and Reasoning (KRR),"Belief Change
Description Logic
Non-monotonic reasoning","KRR: Belief Change
KRR: Description Logics
KRR: Nonmonotonic Reasoning","An essential task in managing DL ontologies is to deal with changes over the ontologies. 
In particular, outdated axioms have to be removed from the ontology 
and newly formed axioms have to be incorporated into the ontology.
Such changes are formalised as the operations of contraction and revision in the literatures. 
The operations can be defined in various ways.
To investigate properties of a defined operation, it is best to identify some postulates that completely 
characterise the operation such that on the one hand the operation satisfies the postulates 
and on the other hand it is the only operation that satisfies all the postulates.
Such characterisation results have never been shown for contractions under DLs.
In this paper, we define model-based contraction and revision for DL-Lite$_{core}$ TBoxes
and provide characterisation results for both operations.
As a first step for applying the operations in practice, 
we also provide tractable algorithms for both operations.
Since DL semantics incurs infinite numbers of models for DL-Lite TBoxes,
it is not feasible to develop algorithms involving DL models.
The key to our operations and algorithms is the development of an alternative semantics called type semantics. Type semantics closely resembles the semantics underlays propositional logic,
thus it is more succinct than DL semantics. Most importantly, given a finite signature, any DL-Lite$_{core}$ TBox has finite numbers of type models.","Contraction and Revision over DL-Lite TBoxes An essential task in managing DL ontologies is to deal with changes over the ontologies. 
In particular, outdated axioms have to be removed from the ontology 
and newly formed axioms have to be incorporated into the ontology.
Such changes are formalised as the operations of contraction and revision in the literatures. 
The operations can be defined in various ways.
To investigate properties of a defined operation, it is best to identify some postulates that completely 
characterise the operation such that on the one hand the operation satisfies the postulates 
and on the other hand it is the only operation that satisfies all the postulates.
Such characterisation results have never been shown for contractions under DLs.
In this paper, we define model-based contraction and revision for DL-Lite$_{core}$ TBoxes
and provide characterisation results for both operations.
As a first step for applying the operations in practice, 
we also provide tractable algorithms for both operations.
Since DL semantics incurs infinite numbers of models for DL-Lite TBoxes,
it is not feasible to develop algorithms involving DL models.
The key to our operations and algorithms is the development of an alternative semantics called type semantics. Type semantics closely resembles the semantics underlays propositional logic,
thus it is more succinct than DL semantics. Most importantly, given a finite signature, any DL-Lite$_{core}$ TBox has finite numbers of type models. Belief Change
Description Logic
Non-monotonic reasoning",contract revis dllite tbox essenti task manag dl ontolog deal chang ontolog particular outdat axiom remov ontolog newli form axiom incorpor ontolog chang formalis oper contract revis literatur oper defin various way investig properti defin oper best identifi postul complet characteris oper one hand oper satisfi postul hand oper satisfi postul characteris result never shown contract dls paper defin modelbas contract revis dllitecor tbox provid characteris result oper first step appli oper practic also provid tractabl algorithm oper sinc dl semant incur infinit number model dllite tbox feasibl develop algorithm involv dl model key oper algorithm develop altern semant call type semant type semant close resembl semant underlay proposit logic thus succinct dl semant import given finit signatur dllitecor tbox finit number type model belief chang descript logic nonmonoton reason,8,22.909773,6.3532863
Zero Pronoun Resolution as Ranking,Chen Chen and Vincent Ng,NLP and Text Mining (NLPTM),"Zero Pronouns
Text Mining
Natural Language Processing",NLPTM: Evaluation and Analysis,"Compared to overt pronoun resolution, there is less work on the more challenging task of zero pronoun resolution. State-of-the-art approaches to zero pronoun resolution are supervised, requiring the availability of documents containing manually resolved zero pronouns. In contrast, we propose in this paper an unsupervised approach to this task. Underlying our approach is the novel idea of employing a model trained on manually resolved overt pronouns to resolve zero pronouns. Experimental results on the OntoNotes corpus are encouraging: our unsupervised model rivals its supervised counterparts in performance.","Zero Pronoun Resolution as Ranking Compared to overt pronoun resolution, there is less work on the more challenging task of zero pronoun resolution. State-of-the-art approaches to zero pronoun resolution are supervised, requiring the availability of documents containing manually resolved zero pronouns. In contrast, we propose in this paper an unsupervised approach to this task. Underlying our approach is the novel idea of employing a model trained on manually resolved overt pronouns to resolve zero pronouns. Experimental results on the OntoNotes corpus are encouraging: our unsupervised model rivals its supervised counterparts in performance. Zero Pronouns
Text Mining
Natural Language Processing",zero pronoun resolut rank compar overt pronoun resolut less work challeng task zero pronoun resolut stateoftheart approach zero pronoun resolut supervis requir avail document contain manual resolv zero pronoun contrast propos paper unsupervis approach task under approach novel idea employ model train manual resolv overt pronoun resolv zero pronoun experiment result ontonot corpus encourag unsupervis model rival supervis counterpart perform zero pronoun text mine natur languag process,6,21.855635,-10.998265
Supervised Transfer Sparse Coding,"Maruan Al-Shedivat, Jim Jing-Yan Wang, Majed Alzahrani, Jianhua Z. Huang and Xin Gao",Novel Machine Learning Algorithms (NMLA),"Sparse coding
Transfer learning
Supervised learning
Classification
Support Vector Machine","NMLA: Classification
NMLA: Transfer, Adaptation, Multitask Learning
NMLA: Supervised Learning (Other)","A combination of sparse coding and transfer learning techniques was shown to be accurate and robust in classification tasks where training and testing objects have a shared feature space but are sampled from different underlying distributions, i.e., belong to different domains. The key assumption in such case is that in spite of the domain disparity, samples from different domains share some common hidden factors. Previous methods often assumed that all the objects in the target domain are not labeled, and thus the training set solely comprised objects from the source domain. However, in real world applications, the target domain often has some labeled objects, or one can always manually label a small number of them. In this paper, we explore such possibility and show how a little amount of labeled data in the target domain can significantly leverage classification accuracy of the state-of-the-art transfer sparse coding methods. We further propose a unified framework named Supervised Transfer Sparse Coding (STSC) which simultaneously optimizes sparse representation, domain transfer and supervised classification. Experimental results on three applications demonstrate that little manual labeling and then learning the model in a supervised fashion can significantly improve classification accuracy.","Supervised Transfer Sparse Coding A combination of sparse coding and transfer learning techniques was shown to be accurate and robust in classification tasks where training and testing objects have a shared feature space but are sampled from different underlying distributions, i.e., belong to different domains. The key assumption in such case is that in spite of the domain disparity, samples from different domains share some common hidden factors. Previous methods often assumed that all the objects in the target domain are not labeled, and thus the training set solely comprised objects from the source domain. However, in real world applications, the target domain often has some labeled objects, or one can always manually label a small number of them. In this paper, we explore such possibility and show how a little amount of labeled data in the target domain can significantly leverage classification accuracy of the state-of-the-art transfer sparse coding methods. We further propose a unified framework named Supervised Transfer Sparse Coding (STSC) which simultaneously optimizes sparse representation, domain transfer and supervised classification. Experimental results on three applications demonstrate that little manual labeling and then learning the model in a supervised fashion can significantly improve classification accuracy. Sparse coding
Transfer learning
Supervised learning
Classification
Support Vector Machine",supervis transfer spars code combin spars code transfer learn techniqu shown accur robust classif task train test object share featur space sampl differ under distribut ie belong differ domain key assumpt case spite domain dispar sampl differ domain share common hidden factor previous method often assum object target domain label thus train set sole compris object sourc domain howev real world applic target domain often label object one alway manual label small number paper explor possibl show littl amount label data target domain signific leverag classif accuraci stateoftheart transfer spars code method propos unifi framework name supervis transfer spars code stsc simultan optim spars represent domain transfer supervis classif experiment result three applic demonstr littl manual label learn model supervis fashion signific improv classif accuraci spars code transfer learn supervis learn classif support vector machin,6,-15.8120775,-10.826512
